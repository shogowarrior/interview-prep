{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Interview Concepts","text":""},{"location":"#project-overview","title":"Project Overview","text":"<p>This repository serves as a comprehensive resource for interview preparation, focusing on core technical concepts essential for data engineering, system design, and software development roles. It provides structured learning paths, practical examples, and real-world applications to help candidates master key areas through progressive skill development.</p>"},{"location":"#key-areas","title":"Key Areas","text":""},{"location":"#concepts","title":"Concepts","text":"<p>The concepts directory offers structured guidance across three main technical domains:</p> <ul> <li>SQL: Database querying, analytics, and performance optimization</li> <li>Data Modeling: Database design patterns and architectural principles</li> <li>System Design: Scalable system architecture and design patterns</li> </ul> <p>Each area includes progressive learning paths from fundamentals to advanced techniques, with practical exercises and real-world case studies.</p>"},{"location":"#interviews","title":"Interviews","text":"<p>The interviews directory contains company-specific interview problems and case studies, featuring challenges from organizations such as Netflix, Anthropic, and others. These materials provide targeted preparation for industry-specific technical interviews.</p> <p>Navigate to the respective directories to explore detailed content and begin your interview preparation journey.</p>"},{"location":"concepts/","title":"Concepts Overview","text":"<p>Welcome to the comprehensive concepts directory! This section provides structured guidance across multiple technical domains essential for data engineering, system design, and software development interviews. Each area offers progressive learning paths with practical examples and real-world applications.</p>"},{"location":"concepts/#overview","title":"\ud83c\udfaf Overview","text":"<p>The concepts directory serves as your central hub for mastering foundational technical concepts across three main areas:</p> <ul> <li>SQL: Database querying, analytics, and performance optimization</li> <li>Data Modeling: Database design patterns and architectural principles</li> <li>System Design: Scalable system architecture and design patterns</li> </ul>"},{"location":"concepts/#concept-areas","title":"\ud83d\udcda Concept Areas","text":""},{"location":"concepts/#sql-concepts","title":"\ud83d\uddc3\ufe0f SQL Concepts","text":"<p>Master database querying and analytics with comprehensive coverage from basic aggregation to advanced optimization techniques.</p>"},{"location":"concepts/#sql-learning-hub","title":"\ud83d\udcca SQL Learning Hub","text":"<ul> <li>Complete SQL Learning Path - Structured progression from fundamentals to expert-level patterns</li> <li>Practical Exercises - Hands-on practice with Wikibooks and TopTal challenges</li> <li>Interview Problems - Netflix-specific SQL challenges with detailed solutions</li> <li>Advanced Techniques - Window functions, CTEs, and complex analytics</li> </ul> <p>Key Topics: Aggregation, JOINs, window functions, CTEs, optimization, real-world analytics</p>"},{"location":"concepts/#data-modeling-concepts","title":"\ud83c\udfd7\ufe0f Data Modeling Concepts","text":"<p>Learn database design principles and architectural patterns for building scalable data systems.</p>"},{"location":"concepts/#data-modeling-hub","title":"\ud83d\udccb Data Modeling Hub","text":"<ul> <li>Core Entities - User accounts, content catalogs, subscriptions, and billing systems</li> <li>Relationship Patterns - Many-to-many relationships, user behavior tracking, and recommendations</li> <li>Scalability Patterns - Partitioning, sharding, and large-scale data architectures</li> <li>Event Streaming - Real-time data processing and quality metrics</li> </ul> <p>Key Topics: Entity design, relationship modeling, scalability patterns, event-driven architectures</p>"},{"location":"concepts/#system-design-concepts","title":"\ud83c\udfe2 System Design Concepts","text":"<p>Master the principles of designing large-scale, distributed systems and architectures.</p>"},{"location":"concepts/#system-design-hub","title":"\ud83c\udfd7\ufe0f System Design Hub","text":"<ul> <li>System Architecture - Fundamental design patterns and architectural principles</li> <li>Performance Optimization - Scaling strategies and performance tuning</li> <li>Case Studies - Real-world system design examples and analysis</li> <li>Experimentation - A/B testing frameworks and experimental design</li> </ul> <p>Key Topics: Distributed systems, scalability, performance, architectural patterns</p>"},{"location":"concepts/#learning-approach","title":"\ud83c\udfaf Learning Approach","text":"<p>Each concept area follows a structured learning progression:</p> Area Entry Level Advanced Level Focus SQL Basic aggregation &amp; filtering Window functions &amp; optimization Data analytics &amp; performance Data Modeling Entity relationships Scalability patterns Database design &amp; architecture System Design Basic architecture Distributed systems Scalability &amp; performance"},{"location":"concepts/#cross-concept-integration","title":"\ud83d\udd17 Cross-Concept Integration","text":""},{"location":"concepts/#from-theory-to-practice","title":"From Theory to Practice","text":"<ul> <li>SQL Analytics \u2192 Data Modeling Design \u2192 System Architecture:</li> <li>Query Optimization \u2192 Scalable Data Patterns \u2192 Performance Architecture:</li> <li>Interview Problems \u2192 Real-world Case Studies \u2192 Production Systems:</li> </ul>"},{"location":"concepts/#recommended-learning-sequence","title":"Recommended Learning Sequence","text":"<ol> <li>Start with SQL - Build data querying and analytics foundation</li> <li>Learn Data Modeling - Understand database design principles</li> <li>Master System Design - Apply concepts to large-scale architectures</li> <li>Practice Integration - Solve complex problems combining all areas</li> </ol>"},{"location":"concepts/#resources-by-focus-area","title":"\ud83d\udee0\ufe0f Resources by Focus Area","text":""},{"location":"concepts/#for-data-engineering-roles","title":"For Data Engineering Roles=","text":"<ol> <li>SQL Mastery - Complex analytics and optimization</li> <li>Data Modeling Patterns - Scalable database design</li> <li>System Design Principles - Architectural patterns and scalability</li> </ol>"},{"location":"concepts/#for-backend-engineering-roles","title":"For Backend Engineering Roles","text":"<ol> <li>System Design Fundamentals - Architectural patterns and scalability</li> <li>Data Modeling Core - Database design and relationships</li> <li>SQL Optimization - Performance tuning and analytics</li> </ol>"},{"location":"concepts/#for-interview-preparation","title":"For Interview Preparation","text":"<ol> <li>SQL Interview Problems - Algorithmic thinking with data</li> <li>System Design Cases - Architectural decision-making</li> <li>Data Modeling Scenarios - Design pattern application</li> </ol>"},{"location":"concepts/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Company Interview Preparation - Industry-specific case studies and problems</li> <li>Netflix Data Modeling - Real-world applications</li> <li>SQL Performance Optimization - Query tuning strategies</li> </ul>"},{"location":"concepts/#navigation","title":"\ud83d\udd17 Navigation","text":"<ul> <li>\u2b05\ufe0f Back to Main Repository</li> <li>\ud83d\udcca SQL Concepts Hub</li> <li>\ud83d\udccb Data Modeling Hub</li> <li>\ud83c\udfe2 System Design Hub</li> <li>\ud83c\udfe2 Company Interview Prep</li> </ul>"},{"location":"concepts/Data-Modeling/","title":"Data Modeling Concepts","text":"<p>This comprehensive guide covers data modeling patterns and best practices for streaming platforms, with detailed examples and real-world implementations.</p>"},{"location":"concepts/Data-Modeling/#table-of-contents","title":"\ud83d\udcda Table of Contents","text":"<ul> <li>Data Modeling Overview - Introduction and overview</li> </ul>"},{"location":"concepts/Data-Modeling/#streaming-services","title":"Streaming-Services","text":"<p>Comprehensive data modeling patterns for streaming platforms, including content catalog, user engagement metrics, subscription billing, A/B testing, and event streaming at massive scale.</p>"},{"location":"concepts/Data-Modeling/#01-core-entities","title":"01-core-entities","text":"<p>Fundamental entity design patterns for streaming platforms.</p> <ul> <li>Content Catalog Modeling - Movies, shows, seasons, episodes, genres, and localization</li> <li>Accounts &amp; Profiles - User accounts, profiles, and device relationships</li> <li>Subscriptions &amp; Billing - Subscription plans, billing cycles, and payment processing</li> </ul>"},{"location":"concepts/Data-Modeling/#02-relationships-patterns","title":"02-relationships-patterns","text":"<p>Common relationship patterns and data modeling techniques.</p> <ul> <li>Many-to-Many Relationships - Junction tables, bridge tables, and temporal relationships</li> <li>User Viewing History - Playback sessions, granular events, and resume functionality</li> <li>Recommendations Storage - Similarity graphs, personalized recommendations, and feature stores</li> </ul>"},{"location":"concepts/Data-Modeling/#03-event-streaming","title":"03-event-streaming","text":"<p>Event-driven data models for real-time analytics and experimentation.</p> <ul> <li>A/B Testing Data Models - Experiment metadata, user assignments, and outcome measurement</li> <li>Engagement Metrics - Daily Active Users (DAU), event tracking, and user engagement analysis</li> <li>Streaming Quality Metrics - Playback quality monitoring, buffering analysis, and CDN performance</li> </ul>"},{"location":"concepts/Data-Modeling/#04-scalability-patterns","title":"04-scalability-patterns","text":"<p>Advanced patterns for handling massive scale and global distribution.</p> <ul> <li>Event Streaming at Scale - Large-scale event processing, Bronze/Silver/Gold layers, and real-time processing</li> <li>Partitioning &amp; Sharding - Data distribution strategies, hot key management, and query optimization</li> </ul>"},{"location":"concepts/Data-Modeling/#key-design-principles","title":"\ud83c\udfaf Key Design Principles","text":""},{"location":"concepts/Data-Modeling/#data-architecture-patterns","title":"Data Architecture Patterns","text":"<ul> <li>Bronze/Silver/Gold Layers: Raw \u2192 Cleaned \u2192 Business-ready data</li> <li>Event-Driven Design: Immutable events with derived state</li> <li>Multi-Tenant Awareness: Account-level isolation and resource management</li> <li>Global Scale: Cross-region distribution and disaster recovery</li> </ul>"},{"location":"concepts/Data-Modeling/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Partitioning Strategies: Date-based, hash-based, and composite partitioning</li> <li>Indexing Patterns: Composite indexes and clustering for query optimization</li> <li>Storage Tiers: Hot/warm/cold storage with lifecycle management</li> <li>Caching Layers: Real-time serving with Redis/Memcached integration</li> </ul>"},{"location":"concepts/Data-Modeling/#data-quality-governance","title":"Data Quality &amp; Governance","text":"<ul> <li>Schema Evolution: Backward/forward compatibility with Avro/Protobuf</li> <li>Data Validation: Constraints, business rules, and quality monitoring</li> <li>Audit Trails: Immutable event logs and change data capture</li> <li>Compliance: PII separation, retention policies, and data sovereignty</li> </ul>"},{"location":"concepts/Data-Modeling/#common-interview-questions","title":"\ud83d\udccb Common Interview Questions","text":""},{"location":"concepts/Data-Modeling/#core-data-modeling","title":"Core Data Modeling","text":"<ul> <li>How would you model a content catalog that supports multiple languages and regions?</li> <li>What are the trade-offs between normalized and denormalized approaches?</li> <li>How do you handle many-to-many relationships in a distributed system?</li> <li>How would you design for efficient 'continue watching' functionality?</li> </ul>"},{"location":"concepts/Data-Modeling/#scalability-performance","title":"Scalability &amp; Performance","text":"<ul> <li>How do you choose partitioning keys for time-series data?</li> <li>What strategies do you use to handle hot partitions and data skew?</li> <li>How would you design a real-time DAU dashboard at massive scale?</li> <li>What are the trade-offs between different storage formats (Delta, Parquet, etc.)?</li> </ul>"},{"location":"concepts/Data-Modeling/#real-time-event-streaming","title":"Real-Time &amp; Event Streaming","text":"<ul> <li>How do you ensure consistent user assignment in A/B testing?</li> <li>What are the challenges of event schema evolution at scale?</li> <li>How do you handle late-arriving data in streaming pipelines?</li> <li>How would you design QoS monitoring across global CDNs?</li> </ul>"},{"location":"concepts/Data-Modeling/#advanced-patterns","title":"Advanced Patterns","text":"<ul> <li>How do you implement recommendation systems at scale?</li> <li>What are the patterns for handling subscription state changes?</li> <li>How do you design for multi-device content sync?</li> <li>How would you migrate a large dataset without downtime?</li> </ul>"},{"location":"concepts/Data-Modeling/#architecture-overview","title":"\ud83c\udfd7\ufe0f Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client Apps   \u2502\u2500\u2500\u2500\u25b6\u2502  Event Stream   \u2502\u2500\u2500\u2500\u25b6\u2502  Real-time DB   \u2502\n\u2502                 \u2502    \u2502   (Kafka)       \u2502    \u2502  (Cassandra)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                        \u2502                        \u2502\n         \u25bc                        \u25bc                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Data Lake     \u2502    \u2502   Batch Jobs    \u2502    \u2502   Analytics     \u2502\n\u2502   (Delta Lake)  \u2502    \u2502   (Spark)       \u2502    \u2502   (Redshift)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/Data-Modeling/#key-metrics-kpis","title":"\ud83d\udcca Key Metrics &amp; KPIs","text":"<ul> <li>Engagement: DAU/MAU, session duration, content completion rates</li> <li>Quality: Rebuffer ratio, startup time, bitrate distribution</li> <li>Business: Revenue per user, subscription churn, content popularity</li> <li>Technical: Throughput, latency, error rates, data freshness</li> </ul>"},{"location":"concepts/Data-Modeling/#technology-stack-examples","title":"\ud83d\udd27 Technology Stack Examples","text":"<ul> <li>Event Streaming: Apache Kafka, Amazon Kinesis</li> <li>Data Lake: Delta Lake, Apache Iceberg</li> <li>Real-time DB: Apache Cassandra, Amazon DynamoDB</li> <li>Analytics: Apache Spark, Trino, Amazon Redshift</li> <li>Processing: Structured Streaming, Delta Live Tables</li> <li>Storage: S3, Cloud Storage with lifecycle policies</li> </ul>"},{"location":"concepts/Data-Modeling/#learning-path","title":"\ud83d\udcc8 Learning Path","text":"<ol> <li>Start with Core Entities - Understand fundamental data models</li> <li>Master Relationships - Learn advanced relationship patterns</li> <li>Explore Event Streaming - Understand real-time data patterns</li> <li>Tackle Scalability - Handle massive scale challenges</li> <li>Apply to Real Problems - Practice with interview questions</li> </ol>"},{"location":"concepts/Data-Modeling/#best-practices","title":"\ud83c\udf93 Best Practices","text":"<ul> <li>Start Simple: Build minimal viable models, then optimize</li> <li>Measure Everything: Instrument data quality and performance metrics</li> <li>Plan for Scale: Design with growth in mind from day one</li> <li>Test Thoroughly: Validate assumptions with A/B tests and monitoring</li> <li>Document Decisions: Keep rationale for design choices and trade-offs</li> </ul>"},{"location":"concepts/Data-Modeling/#navigation","title":"\ud83d\udd17 Navigation","text":"<ul> <li>\u2b05\ufe0f Back to Concepts Overview</li> <li>\ud83d\udcca SQL Concepts</li> <li>\ud83c\udfd7\ufe0f System Design</li> <li>\ud83c\udfe2 Company Interview Prep</li> </ul> <p>Note: This unified data modeling guide combines concepts from streaming platform implementations with practical examples suitable for technical interviews and real-world applications.</p>"},{"location":"concepts/Data-Modeling/01-core-entities/","title":"Core Entities","text":"<p>This section covers the fundamental entities in a streaming platform data model, including content catalog, user accounts/profiles, and subscription/billing systems.</p>"},{"location":"concepts/Data-Modeling/01-core-entities/#files-in-this-section","title":"Files in this section","text":"<ul> <li>Content Catalog Modeling - Designing data models for movies, shows, seasons, episodes, genres, and localization</li> <li>Accounts &amp; Profiles - User account and profile management with device relationships</li> <li>Subscriptions &amp; Billing - Subscription plans, billing cycles, promotions, and payment processing</li> </ul>"},{"location":"concepts/Data-Modeling/01-core-entities/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"concepts/Data-Modeling/01-core-entities/#content-catalog","title":"Content Catalog","text":"<ul> <li>Unified <code>title</code> table for movies and shows</li> <li>Hierarchical season/episode structure</li> <li>Many-to-many relationships with genres</li> <li>Localized metadata for global content</li> </ul>"},{"location":"concepts/Data-Modeling/01-core-entities/#user-management","title":"User Management","text":"<ul> <li>Account-level device registration</li> <li>Profile-level content access and preferences</li> <li>Many-to-many profile-device relationships</li> <li>Maturity ratings and parental controls</li> </ul>"},{"location":"concepts/Data-Modeling/01-core-entities/#billing-subscriptions","title":"Billing &amp; Subscriptions","text":"<ul> <li>Effective-dated subscription changes (SCD-2)</li> <li>Separate invoice and payment tracking</li> <li>Complex promotion and discount systems</li> <li>Audit trails for regulatory compliance</li> </ul>"},{"location":"concepts/Data-Modeling/01-core-entities/#common-interview-questions","title":"Common Interview Questions","text":"<ul> <li>How would you model content localization across multiple regions?</li> <li>How do you handle device limits and concurrent streaming?</li> <li>What are the trade-offs between real-time vs. end-of-cycle billing?</li> <li>How would you track subscription changes and calculate prorated charges?</li> </ul> <p>Navigate back to Data Modeling Index</p>"},{"location":"concepts/Data-Modeling/01-core-entities/accounts-profiles/","title":"Accounts &amp; Profiles Modeling","text":""},{"location":"concepts/Data-Modeling/01-core-entities/accounts-profiles/#overview","title":"Overview","text":"<p>Design a schema to model Netflix accounts, profiles, and devices where one account can have multiple profiles, and each profile can be accessed on multiple devices.</p>"},{"location":"concepts/Data-Modeling/01-core-entities/accounts-profiles/#schema-design","title":"Schema Design","text":""},{"location":"concepts/Data-Modeling/01-core-entities/accounts-profiles/#core-tables","title":"Core Tables","text":"<pre><code>CREATE TABLE account (\n  account_id BIGINT PRIMARY KEY,\n  created_at TIMESTAMP,\n  home_country STRING,\n  billing_currency STRING\n);\n\nCREATE TABLE profile (\n  profile_id BIGINT PRIMARY KEY,\n  account_id BIGINT REFERENCES account(account_id),\n  name STRING,\n  is_kids BOOLEAN DEFAULT FALSE,\n  maturity_setting STRING,\n  language_pref STRING,\n  created_at TIMESTAMP\n);\n\nCREATE TABLE device (\n  device_id STRING PRIMARY KEY,\n  account_id BIGINT REFERENCES account(account_id),\n  device_type STRING,                 -- TV, mobile, web\n  device_name STRING,\n  registered_at TIMESTAMP\n);\n\nCREATE TABLE profile_device_link (\n  profile_id BIGINT REFERENCES profile(profile_id),\n  device_id STRING REFERENCES device(device_id),\n  linked_at TIMESTAMP,\n  PRIMARY KEY (profile_id, device_id)\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/01-core-entities/accounts-profiles/#extended-schema-for-advanced-features","title":"Extended Schema for Advanced Features","text":"<pre><code>-- Device sessions for concurrent stream limits\nCREATE TABLE device_session (\n  session_id STRING PRIMARY KEY,\n  device_id STRING REFERENCES device(device_id),\n  profile_id BIGINT REFERENCES profile(profile_id),\n  started_at TIMESTAMP,\n  ended_at TIMESTAMP NULL,\n  ip_address STRING\n);\n\n-- Profile preferences and viewing history summary\nCREATE TABLE profile_preferences (\n  profile_id BIGINT PRIMARY KEY REFERENCES profile(profile_id),\n  favorite_genres ARRAY&lt;STRING&gt;,\n  preferred_languages ARRAY&lt;STRING&gt;,\n  autoplay_enabled BOOLEAN DEFAULT TRUE,\n  last_updated TIMESTAMP\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/01-core-entities/accounts-profiles/#business-rules-implementation","title":"Business Rules Implementation","text":"<pre><code>-- Enforce device limits (simplified example)\nCREATE VIEW active_device_count AS\nSELECT\n  account_id,\n  COUNT(DISTINCT device_id) AS active_devices\nFROM device_session\nWHERE ended_at IS NULL\nGROUP BY account_id;\n\n-- Kids profile restrictions\nCREATE VIEW kids_content_access AS\nSELECT\n  p.profile_id,\n  CASE\n    WHEN p.is_kids THEN 'Y'\n    ELSE 'N'\n  END AS can_access_mature_content\nFROM profile p;\n</code></pre>"},{"location":"concepts/Data-Modeling/01-core-entities/accounts-profiles/#lakehouse-optimization","title":"Lakehouse Optimization","text":"<ul> <li>Partitioning: <code>account</code> by <code>home_country</code>, <code>profile</code> by <code>account_id</code></li> <li>Clustering: <code>profile</code> by <code>account_id</code> for efficient account-level queries</li> <li>Z-Ordering: <code>device_session</code> by <code>(profile_id, started_at)</code> for time-series analysis</li> </ul>"},{"location":"concepts/Data-Modeling/01-core-entities/accounts-profiles/#common-queries","title":"Common Queries","text":"<pre><code>-- Get all profiles for an account\nSELECT p.*, pd.device_count\nFROM profile p\nLEFT JOIN (\n  SELECT profile_id, COUNT(*) AS device_count\n  FROM profile_device_link\n  GROUP BY profile_id\n) pd ON p.profile_id = pd.profile_id\nWHERE p.account_id = ?;\n\n-- Find active sessions per account (for concurrent stream limits)\nSELECT\n  a.account_id,\n  COUNT(ds.session_id) AS active_sessions\nFROM account a\nLEFT JOIN profile p ON a.account_id = p.account_id\nLEFT JOIN device_session ds ON p.profile_id = ds.profile_id\n  AND ds.ended_at IS NULL\nGROUP BY a.account_id;\n\n-- Device management for account\nSELECT\n  d.*,\n  COUNT(pdl.profile_id) AS linked_profiles\nFROM device d\nLEFT JOIN profile_device_link pdl ON d.device_id = pdl.device_id\nWHERE d.account_id = ?\nGROUP BY d.device_id, d.device_type, d.device_name, d.registered_at;\n</code></pre>"},{"location":"concepts/Data-Modeling/01-core-entities/accounts-profiles/#design-considerations","title":"Design Considerations","text":"<ul> <li>Account-Profile Relationship: One-to-many with clear ownership</li> <li>Device Management: Many-to-many between profiles and devices</li> <li>Session Tracking: Critical for enforcing business rules like concurrent stream limits</li> <li>Kids Content: Metadata-driven restrictions for compliance</li> <li>Scalability: Optimized for account-level operations and device management</li> </ul>"},{"location":"concepts/Data-Modeling/01-core-entities/accounts-profiles/#follow-up-questions","title":"Follow-up Questions","text":"<ul> <li>How would you handle multiple concurrent profiles per account?</li> <li>How would you design for efficient \"continue watching\" queries?</li> <li>How would you enforce device limits and concurrent streaming restrictions?</li> </ul>"},{"location":"concepts/Data-Modeling/01-core-entities/content-catalog-modeling/","title":"Content Catalog Modeling","text":""},{"location":"concepts/Data-Modeling/01-core-entities/content-catalog-modeling/#overview","title":"Overview","text":"<p>Design a data model to represent Netflix's content library where each title can be a movie or a show with multiple seasons and episodes.</p>"},{"location":"concepts/Data-Modeling/01-core-entities/content-catalog-modeling/#schema-design","title":"Schema Design","text":""},{"location":"concepts/Data-Modeling/01-core-entities/content-catalog-modeling/#core-tables","title":"Core Tables","text":"<pre><code>-- Type can be MOVIE or SHOW\nCREATE TABLE title (\n  title_id BIGINT PRIMARY KEY,\n  type STRING NOT NULL,\n  release_date DATE,\n  maturity_rating STRING,\n  production_country STRING,\n  is_active BOOLEAN DEFAULT TRUE\n);\n\nCREATE TABLE season (\n  season_id BIGINT PRIMARY KEY,\n  title_id BIGINT NOT NULL REFERENCES title(title_id),\n  season_number INT NOT NULL,\n  UNIQUE(title_id, season_number)\n);\n\nCREATE TABLE episode (\n  episode_id BIGINT PRIMARY KEY,\n  title_id BIGINT NOT NULL REFERENCES title(title_id),\n  season_id BIGINT REFERENCES season(season_id),\n  episode_number INT,\n  duration_sec INT,\n  UNIQUE(title_id, season_id, episode_number)\n);\n\nCREATE TABLE genre (\n  genre_id INT PRIMARY KEY,\n  name STRING UNIQUE\n);\n\nCREATE TABLE title_genre (\n  title_id BIGINT REFERENCES title(title_id),\n  genre_id INT REFERENCES genre(genre_id),\n  PRIMARY KEY (title_id, genre_id)\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/01-core-entities/content-catalog-modeling/#localization-support","title":"Localization Support","text":"<pre><code>-- Localization (can apply to titles, seasons, or episodes)\nCREATE TABLE localized_metadata (\n  object_type STRING CHECK (object_type IN ('TITLE','SEASON','EPISODE')),\n  object_id BIGINT,\n  locale STRING,                      -- e.g., en-US, es-MX\n  localized_title STRING,\n  localized_description STRING,\n  PRIMARY KEY (object_type, object_id, locale)\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/01-core-entities/content-catalog-modeling/#lakehouse-implementation","title":"Lakehouse Implementation","text":"<ul> <li>Storage: Delta/Iceberg format with Parquet</li> <li>Partitioning: Sparse partitioning (not usually necessary for titles)</li> <li>Clustering: <code>title_id</code> for episodes table for efficient season lookups</li> <li>Optimization: Z-Order on frequently queried columns</li> </ul>"},{"location":"concepts/Data-Modeling/01-core-entities/content-catalog-modeling/#common-queries","title":"Common Queries","text":"<pre><code>-- Get all episodes of S2 for a show\nSELECT e.*, l.localized_title\nFROM episode e\nLEFT JOIN localized_metadata l\n  ON e.episode_id = l.object_id\n  AND l.object_type = 'EPISODE'\n  AND l.locale = 'en-US'\nWHERE e.title_id = ?\n  AND e.season_number = 2\nORDER BY e.episode_number;\n\n-- Find all action movies\nSELECT t.*, l.localized_title\nFROM title t\nJOIN title_genre tg ON t.title_id = tg.title_id\nJOIN genre g ON tg.genre_id = g.genre_id\nLEFT JOIN localized_metadata l\n  ON t.title_id = l.object_id\n  AND l.object_type = 'TITLE'\n  AND l.locale = ?\nWHERE g.name = 'Action'\n  AND t.type = 'MOVIE';\n</code></pre>"},{"location":"concepts/Data-Modeling/01-core-entities/content-catalog-modeling/#design-considerations","title":"Design Considerations","text":"<ul> <li>Unified Content Model: Single <code>title</code> table handles both movies and shows</li> <li>Flexible Localization: Separate table supports multiple languages efficiently</li> <li>Genre Relationships: Many-to-many relationship for flexible categorization</li> <li>Scalability: Partitioning and clustering optimized for common access patterns</li> </ul>"},{"location":"concepts/Data-Modeling/01-core-entities/content-catalog-modeling/#follow-up-questions","title":"Follow-up Questions","text":"<ul> <li>How would you handle localized metadata for multiple languages?</li> <li>How would you model multiple genres per title?</li> <li>How would you optimize for \"continue watching\" functionality?</li> </ul>"},{"location":"concepts/Data-Modeling/01-core-entities/subscriptions-billing/","title":"Subscriptions &amp; Billing Modeling","text":""},{"location":"concepts/Data-Modeling/01-core-entities/subscriptions-billing/#overview","title":"Overview","text":"<p>Design a data model to store subscription plans, billing cycles, promotions, and payment history with support for mid-cycle plan changes.</p>"},{"location":"concepts/Data-Modeling/01-core-entities/subscriptions-billing/#schema-design","title":"Schema Design","text":""},{"location":"concepts/Data-Modeling/01-core-entities/subscriptions-billing/#core-tables","title":"Core Tables","text":"<pre><code>CREATE TABLE plan (\n  plan_id STRING PRIMARY KEY,        -- 'STANDARD_ADS', 'PREMIUM'\n  name STRING,\n  price_cents INT,\n  currency STRING,\n  max_screens INT,\n  hdr BOOLEAN,\n  ads_supported BOOLEAN,\n  is_active BOOLEAN DEFAULT TRUE\n);\n\n-- Effective-dated subscription records (SCD-2 style)\nCREATE TABLE subscription (\n  subscription_id BIGINT PRIMARY KEY,\n  account_id BIGINT,\n  plan_id STRING REFERENCES plan(plan_id),\n  status STRING,                     -- ACTIVE, PAUSED, CANCELED\n  start_date DATE,\n  end_date DATE NULL                 -- null = current\n);\n\nCREATE TABLE promotion (\n  promo_code STRING PRIMARY KEY,\n  discount_type STRING,              -- PCT or FIXED\n  discount_value DOUBLE,\n  start_date DATE,\n  end_date DATE\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/01-core-entities/subscriptions-billing/#billing-and-payment-tables","title":"Billing and Payment Tables","text":"<pre><code>CREATE TABLE subscription_event (\n  event_id BIGINT PRIMARY KEY,\n  subscription_id BIGINT REFERENCES subscription(subscription_id),\n  event_time TIMESTAMP,\n  event_type STRING,                 -- UPGRADE, DOWNGRADE, CANCEL, RESUME, RENEW\n  old_plan_id STRING,\n  new_plan_id STRING,\n  note STRING\n);\n\nCREATE TABLE invoice (\n  invoice_id BIGINT PRIMARY KEY,\n  account_id BIGINT,\n  period_start DATE,\n  period_end DATE,\n  amount_due_cents INT,\n  currency STRING,\n  promo_code STRING NULL REFERENCES promotion(promo_code),\n  created_at TIMESTAMP,\n  status STRING                      -- OPEN, PAID, VOID\n);\n\nCREATE TABLE payment (\n  payment_id BIGINT PRIMARY KEY,\n  invoice_id BIGINT REFERENCES invoice(invoice_id),\n  amount_cents INT,\n  currency STRING,\n  method STRING,                     -- card, paypal, gift\n  status STRING,                     -- AUTH, CAPTURED, REFUNDED, FAILED\n  created_at TIMESTAMP\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/01-core-entities/subscriptions-billing/#business-logic-implementation","title":"Business Logic Implementation","text":"<pre><code>-- Handle mid-cycle plan changes\nCREATE PROCEDURE process_plan_change(\n  p_subscription_id BIGINT,\n  p_new_plan_id STRING,\n  p_effective_date DATE\n) AS\nBEGIN\n  -- Create credit line for current invoice\n  INSERT INTO invoice_line_items (invoice_id, amount_cents, description)\n  SELECT\n    i.invoice_id,\n    -1 * (p.price_cents * days_remaining / days_in_period),\n    'Proration credit for plan change'\n  FROM invoices i\n  JOIN plans p ON p.plan_id = p_new_plan_id\n  WHERE i.subscription_id = p_subscription_id\n    AND i.status = 'OPEN';\n\n  -- Create new pro-rated charge\n  INSERT INTO invoice_line_items (invoice_id, amount_cents, description)\n  SELECT\n    i.invoice_id,\n    p.price_cents * days_remaining / days_in_period,\n    'Prorated charge for new plan'\n  FROM invoices i\n  JOIN plans p ON p.plan_id = p_new_plan_id\n  WHERE i.subscription_id = p_subscription_id;\nEND;\n</code></pre>"},{"location":"concepts/Data-Modeling/01-core-entities/subscriptions-billing/#lakehouse-optimization","title":"Lakehouse Optimization","text":"<ul> <li>Partitioning: <code>subscription</code> by <code>account_id</code>, <code>invoice</code> by <code>period_start</code></li> <li>Clustering: <code>payment</code> by <code>(account_id, created_at)</code> for account history</li> <li>Retention: Keep full history for financial auditing and compliance</li> </ul>"},{"location":"concepts/Data-Modeling/01-core-entities/subscriptions-billing/#common-queries","title":"Common Queries","text":"<pre><code>-- Get current subscription for account\nSELECT s.*, p.name, p.price_cents\nFROM subscription s\nJOIN plan p ON s.plan_id = p.plan_id\nWHERE s.account_id = ?\n  AND s.end_date IS NULL;\n\n-- Calculate monthly recurring revenue (MRR)\nSELECT\n  DATE_TRUNC('month', i.period_start) AS billing_month,\n  SUM(i.amount_due_cents) / 100.0 AS mrr_usd\nFROM invoice i\nWHERE i.status = 'PAID'\n  AND i.period_start &gt;= DATE_TRUNC('month', CURRENT_DATE - INTERVAL '12' MONTH)\nGROUP BY DATE_TRUNC('month', i.period_start)\nORDER BY billing_month;\n\n-- Track plan change history\nSELECT\n  se.*,\n  p_old.name AS old_plan_name,\n  p_new.name AS new_plan_name\nFROM subscription_event se\nLEFT JOIN plan p_old ON se.old_plan_id = p_old.plan_id\nLEFT JOIN plan p_new ON se.new_plan_id = p_new.plan_id\nWHERE se.subscription_id = ?\nORDER BY se.event_time DESC;\n</code></pre>"},{"location":"concepts/Data-Modeling/01-core-entities/subscriptions-billing/#design-considerations","title":"Design Considerations","text":"<ul> <li>Effective Dating: SCD-2 pattern for subscription history</li> <li>Event Sourcing: <code>subscription_event</code> table tracks all state changes</li> <li>Financial Accuracy: Separate invoice and payment for accounting</li> <li>Proration Logic: Handle mid-cycle changes with proper credits/charges</li> <li>Audit Trail: Complete history for financial reporting and compliance</li> </ul>"},{"location":"concepts/Data-Modeling/01-core-entities/subscriptions-billing/#follow-up-questions","title":"Follow-up Questions","text":"<ul> <li>How do you handle users upgrading/downgrading mid-cycle?</li> <li>How would you model gift subscriptions or promotional periods?</li> <li>How would you handle failed payments and dunning processes?</li> <li>How would you design for multi-currency billing?</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/","title":"Relationships &amp; Patterns","text":"<p>This section covers common relationship patterns in streaming platform data models, focusing on many-to-many relationships, viewing history, and recommendation systems.</p>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/#files-in-this-section","title":"Files in this section","text":"<ul> <li>Many-to-Many Relationships - Junction tables, bridge tables, temporal relationships, and optimization patterns</li> <li>User Viewing History - Playback sessions, granular events, resume functionality, and multi-device sync</li> <li>Recommendations Storage - Similarity graphs, personalized recommendations, feature stores, and model lineage</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"concepts/Data-Modeling/02-relationships-patterns/#many-to-many-relationships","title":"Many-to-Many Relationships","text":"<ul> <li>Simple junction tables for basic relationships</li> <li>Bridge tables with additional attributes</li> <li>Temporal relationships for historical tracking</li> <li>Self-referencing relationships for content similarity</li> <li>Performance optimization through clustering and indexing</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/#user-behavior-modeling","title":"User Behavior Modeling","text":"<ul> <li>Session-based event tracking</li> <li>Append-only event storage for audit trails</li> <li>Denormalized resume state for fast queries</li> <li>Device and profile context preservation</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/#recommendation-systems","title":"Recommendation Systems","text":"<ul> <li>Item-item similarity graphs</li> <li>Precomputed personalized recommendations</li> <li>Model versioning and A/B testing support</li> <li>Feature stores for machine learning</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/#common-interview-questions","title":"Common Interview Questions","text":"<ul> <li>How do you optimize many-to-many relationship queries?</li> <li>What are the trade-offs between normalized and denormalized approaches?</li> <li>How would you design for efficient recommendation serving at scale?</li> <li>How do you handle the cold start problem in recommendations?</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Indexing: Composite indexes for common query patterns</li> <li>Partitioning: Time-based partitioning for event data</li> <li>Materialized Views: Pre-aggregated data for common queries</li> <li>Clustering: Data co-location for join optimization</li> </ul> <p>Navigate back to Data Modeling Index</p>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/","title":"Many-to-Many Relationships","text":""},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#overview","title":"Overview","text":"<p>Common patterns for handling many-to-many relationships in a streaming platform's data architecture, including junction tables, bridge tables, and relationship modeling strategies.</p>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#core-patterns","title":"Core Patterns","text":""},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#1-simple-junction-table","title":"1. Simple Junction Table","text":"<p>Use Case: Title-Genre relationships (one title can have multiple genres, one genre can apply to multiple titles)</p> <pre><code>CREATE TABLE title_genre (\n  title_id BIGINT REFERENCES title(title_id),\n  genre_id INT REFERENCES genre(genre_id),\n  assigned_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n  PRIMARY KEY (title_id, genre_id)\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#2-bridge-table-with-attributes","title":"2. Bridge Table with Attributes","text":"<p>Use Case: Profile-Device relationships with usage context</p> <pre><code>CREATE TABLE profile_device (\n  profile_id BIGINT REFERENCES profile(profile_id),\n  device_id STRING REFERENCES device(device_id),\n  first_used_at TIMESTAMP,\n  last_used_at TIMESTAMP,\n  usage_count INT DEFAULT 0,\n  PRIMARY KEY (profile_id, device_id)\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#3-temporal-many-to-many","title":"3. Temporal Many-to-Many","text":"<p>Use Case: Subscription-Plan changes over time</p> <pre><code>CREATE TABLE account_plan_history (\n  account_id BIGINT,\n  plan_id STRING,\n  start_date DATE,\n  end_date DATE NULL,\n  monthly_price_cents INT,\n  PRIMARY KEY (account_id, start_date)\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#4-self-referencing-many-to-many","title":"4. Self-Referencing Many-to-Many","text":"<p>Use Case: Title-Title relationships (sequels, franchises, recommendations)</p> <pre><code>CREATE TABLE title_relationship (\n  src_title_id BIGINT REFERENCES title(title_id),\n  dst_title_id BIGINT REFERENCES title(title_id),\n  relationship_type STRING,  -- 'sequel', 'prequel', 'spin_off', 'similar'\n  strength_score DOUBLE,\n  created_at TIMESTAMP,\n  PRIMARY KEY (src_title_id, dst_title_id, relationship_type)\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#5-hierarchical-many-to-many","title":"5. Hierarchical Many-to-Many","text":"<p>Use Case: Content categorization with multiple taxonomies</p> <pre><code>CREATE TABLE title_category (\n  title_id BIGINT,\n  category_id INT,\n  taxonomy_type STRING,      -- 'genre', 'mood', 'theme', 'demographic'\n  weight DOUBLE,             -- importance score\n  PRIMARY KEY (title_id, category_id, taxonomy_type)\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#query-patterns","title":"Query Patterns","text":""},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#efficient-many-to-many-queries","title":"Efficient Many-to-Many Queries","text":"<pre><code>-- Get all genres for multiple titles\nSELECT t.title_id, g.name\nFROM title t\nJOIN title_genre tg ON t.title_id = tg.title_id\nJOIN genre g ON tg.genre_id = g.genre_id\nWHERE t.title_id IN (?)\n\n-- Find titles with specific genre combinations\nSELECT t.title_id, t.title_name\nFROM title t\nJOIN title_genre tg ON t.title_id = tg.title_id\nWHERE tg.genre_id IN (1, 2, 3)  -- Action, Drama, Comedy\nGROUP BY t.title_id, t.title_name\nHAVING COUNT(DISTINCT tg.genre_id) = 3  -- Must have all three genres\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#lakehouse-optimization","title":"Lakehouse Optimization","text":"<ul> <li>Clustering: For frequent access patterns, cluster junction tables by the most queried dimension</li> <li>Partitioning: Consider partitioning large junction tables by time dimensions when applicable</li> <li>Materialized Views: Pre-aggregate many-to-many relationships for common query patterns</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#trade-offs-discussion","title":"Trade-offs Discussion","text":""},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#normalization-vs-denormalization","title":"Normalization vs. Denormalization","text":"<ul> <li>Normalized: Pure junction tables minimize redundancy but require joins</li> <li>Denormalized: Array/JSON fields reduce joins but complicate updates and analytics</li> <li>Hybrid: Balance based on query patterns and update frequency</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#storage-considerations","title":"Storage Considerations","text":"<ul> <li>Small relationships: Keep normalized (genres, categories)</li> <li>Large relationships: Consider denormalization (user preferences, viewing history)</li> <li>Temporal relationships: Always keep historical records for audit and analytics</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#streaming-platform-context","title":"Streaming Platform Context","text":"<ul> <li>Content tagging: Multiple taxonomies (genre, mood, themes, demographics)</li> <li>Personalization: User preferences across multiple dimensions</li> <li>Global localization: Content availability varies by region and device type</li> <li>A/B testing: Different recommendation algorithms create different relationship graphs</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#performance-patterns","title":"Performance Patterns","text":""},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#indexing-strategies","title":"Indexing Strategies","text":"<pre><code>-- Composite indexes for common query patterns\nCREATE INDEX idx_title_genre_genre ON title_genre(genre_id, title_id);\nCREATE INDEX idx_profile_device_last_used ON profile_device(last_used_at, profile_id);\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#aggregation-tables","title":"Aggregation Tables","text":"<pre><code>-- Pre-computed genre statistics\nCREATE TABLE genre_stats (\n  genre_id INT PRIMARY KEY,\n  title_count INT,\n  total_views BIGINT,\n  last_updated TIMESTAMP\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/many-to-many-relationships/#follow-up-considerations","title":"Follow-up Considerations","text":"<ul> <li>How do you handle referential integrity in distributed systems?</li> <li>What are the trade-offs between different many-to-many modeling approaches?</li> <li>How do you optimize for both OLTP updates and OLAP analytics?</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/recommendations-storage/","title":"Recommendations Storage Modeling","text":""},{"location":"concepts/Data-Modeling/02-relationships-patterns/recommendations-storage/#overview","title":"Overview","text":"<p>Design a data model to store relationships for \"Because you watched X...\" recommendations, including interactions, similarity graphs, and personalized recommendation lists.</p>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/recommendations-storage/#schema-design","title":"Schema Design","text":""},{"location":"concepts/Data-Modeling/02-relationships-patterns/recommendations-storage/#1-interactions-training-data","title":"1. Interactions &amp; Training Data","text":"<pre><code>CREATE TABLE user_content_interaction (\n  profile_id BIGINT,\n  title_id BIGINT,\n  interaction_type STRING,           -- VIEW, LIKE, DISLIKE, RATING, SHARE\n  interaction_value DOUBLE,          -- 1.0 for view, 5.0 for rating\n  event_ts TIMESTAMP,\n  session_id STRING,\n  PRIMARY KEY (profile_id, title_id, event_ts)\n);\n\nCREATE TABLE user_content_rating (\n  profile_id BIGINT,\n  title_id BIGINT,\n  rating_value INT,                 -- 1-5 stars\n  rated_at TIMESTAMP,\n  PRIMARY KEY (profile_id, title_id)\n);\n\nCREATE TABLE viewing_session (\n  session_id STRING PRIMARY KEY,\n  profile_id BIGINT,\n  title_id BIGINT,\n  watch_duration_sec INT,\n  completion_rate DOUBLE,           -- 0.0 to 1.0\n  started_at TIMESTAMP,\n  ended_at TIMESTAMP\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/recommendations-storage/#2-similarity-graph-models","title":"2. Similarity Graph &amp; Models","text":"<pre><code>CREATE TABLE item_item_edge (\n  src_title_id BIGINT,\n  dst_title_id BIGINT,\n  score DOUBLE,                      -- similarity score\n  model VARCHAR,                     -- 'itemCF', 'graphSage', 'contentBased'\n  model_version STRING,\n  computed_at TIMESTAMP,\n  PRIMARY KEY (src_title_id, dst_title_id, model, model_version)\n);\n\nCREATE TABLE user_user_edge (\n  src_profile_id BIGINT,\n  dst_profile_id BIGINT,\n  similarity_score DOUBLE,\n  computed_at TIMESTAMP,\n  PRIMARY KEY (src_profile_id, dst_profile_id)\n);\n\nCREATE TABLE content_embedding (\n  title_id BIGINT PRIMARY KEY,\n  embedding VECTOR(256),             -- vector representation\n  model_version STRING,\n  updated_at TIMESTAMP\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/recommendations-storage/#3-personalized-recommendations","title":"3. Personalized Recommendations","text":"<pre><code>CREATE TABLE recommendation_list (\n  profile_id BIGINT,\n  context STRING,                   -- 'home', 'because_you_watched', 'genre_drama'\n  rank_position INT,\n  title_id BIGINT,\n  score DOUBLE,                     -- recommendation confidence\n  model STRING,\n  model_version STRING,\n  generated_at TIMESTAMP,\n  expires_at TIMESTAMP,\n  PRIMARY KEY (profile_id, context, rank_position, generated_at)\n);\n\nCREATE TABLE recommendation_explanation (\n  profile_id BIGINT,\n  title_id BIGINT,\n  explanation_type STRING,          -- 'similar_to', 'because_you_watched', 'popular'\n  explanation_text STRING,\n  related_title_id BIGINT NULL,\n  generated_at TIMESTAMP,\n  PRIMARY KEY (profile_id, title_id, generated_at)\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/recommendations-storage/#serving-layer-optimization","title":"Serving Layer Optimization","text":"<pre><code>-- Materialized view for fast serving\nCREATE TABLE current_recommendations AS\nSELECT *\nFROM recommendation_list\nWHERE generated_at &gt;= (\n  SELECT MAX(generated_at)\n  FROM recommendation_list\n  WHERE profile_id = recommendation_list.profile_id\n    AND context = recommendation_list.context\n);\n\n-- Cache for frequently accessed recommendations\nCREATE TABLE recommendation_cache (\n  profile_id BIGINT,\n  context STRING,\n  recommendations JSON,             -- serialized list\n  last_accessed TIMESTAMP,\n  ttl_minutes INT,\n  PRIMARY KEY (profile_id, context)\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/recommendations-storage/#common-queries","title":"Common Queries","text":"<pre><code>-- Get personalized recommendations for homepage\nSELECT\n  rl.*,\n  t.title,\n  re.explanation_text\nFROM current_recommendations rl\nJOIN title t ON rl.title_id = t.title_id\nLEFT JOIN recommendation_explanation re\n  ON rl.profile_id = re.profile_id\n  AND rl.title_id = re.title_id\nWHERE rl.profile_id = ?\n  AND rl.context = 'home'\nORDER BY rl.rank_position;\n\n-- Find similar content (for \"More Like This\")\nSELECT\n  iie.*,\n  t.title AS similar_title\nFROM item_item_edge iie\nJOIN title t ON iie.dst_title_id = t.title_id\nWHERE iie.src_title_id = ?\n  AND iie.model = 'contentBased'\nORDER BY iie.score DESC\nLIMIT 10;\n\n-- A/B test different recommendation models\nSELECT\n  model,\n  model_version,\n  COUNT(*) AS impressions,\n  AVG(CASE WHEN interaction_type = 'VIEW' THEN 1 ELSE 0 END) AS ctr\nFROM recommendation_list rl\nLEFT JOIN user_content_interaction uci\n  ON rl.profile_id = uci.profile_id\n  AND rl.title_id = uci.title_id\n  AND uci.event_ts BETWEEN rl.generated_at AND rl.expires_at\nWHERE rl.generated_at &gt;= CURRENT_DATE - INTERVAL '30' DAY\nGROUP BY model, model_version;\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/recommendations-storage/#model-training-updates","title":"Model Training &amp; Updates","text":"<pre><code>-- Aggregate training data for model updates\nCREATE TABLE training_interactions AS\nSELECT\n  profile_id,\n  title_id,\n  AVG(interaction_value) AS avg_interaction,\n  COUNT(*) AS interaction_count,\n  MAX(event_ts) AS last_interaction\nFROM user_content_interaction\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '90' DAY\nGROUP BY profile_id, title_id;\n\n-- Track model performance\nCREATE TABLE model_performance (\n  model_name STRING,\n  model_version STRING,\n  evaluation_date DATE,\n  metric_name STRING,               -- 'precision@10', 'recall@20'\n  metric_value DOUBLE,\n  PRIMARY KEY (model_name, model_version, evaluation_date, metric_name)\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/recommendations-storage/#design-considerations","title":"Design Considerations","text":"<ul> <li>Multi-Model Support: Store edges from different algorithms</li> <li>Version Control: Track model versions for A/B testing and rollback</li> <li>Real-time Updates: Incremental updates for new user behavior</li> <li>Explainability: Store reasoning for user trust and debugging</li> <li>Performance: Pre-compute recommendations, use caching for serving</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/recommendations-storage/#follow-up-questions","title":"Follow-up Questions","text":"<ul> <li>How would you handle cold-start problem for new users/content?</li> <li>How would you design A/B testing framework for recommendation models?</li> <li>How would you ensure recommendation diversity and avoid filter bubbles?</li> <li>How would you handle real-time feedback and model updates?</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/user-viewing-history/","title":"User Viewing History Modeling","text":""},{"location":"concepts/Data-Modeling/02-relationships-patterns/user-viewing-history/#overview","title":"Overview","text":"<p>Model how you would store user viewing history including partially watched episodes, multiple devices, and resume functionality.</p>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/user-viewing-history/#schema-design","title":"Schema Design","text":""},{"location":"concepts/Data-Modeling/02-relationships-patterns/user-viewing-history/#core-tables","title":"Core Tables","text":"<pre><code>CREATE TABLE account (\n  account_id BIGINT PRIMARY KEY\n);\n\nCREATE TABLE profile (\n  profile_id BIGINT PRIMARY KEY,\n  account_id BIGINT REFERENCES account(account_id),\n  maturity_setting STRING,\n  country_code STRING\n);\n\nCREATE TABLE device (\n  device_id STRING PRIMARY KEY,       -- GUID or device fingerprint\n  device_type STRING,                 -- TV, mobile, web\n  os_version STRING\n);\n\nCREATE TABLE playback_session (\n  session_id STRING PRIMARY KEY,      -- UUID\n  profile_id BIGINT REFERENCES profile(profile_id),\n  device_id STRING REFERENCES device(device_id),\n  title_id BIGINT,\n  episode_id BIGINT NULL,\n  started_at TIMESTAMP,\n  ended_at TIMESTAMP NULL\n);\n\n-- Append-only granular timeline\nCREATE TABLE playback_event (\n  session_id STRING,\n  event_time TIMESTAMP,\n  event_type STRING,              -- PLAY, PAUSE, SEEK, BITRATE_CHANGE, END, ERROR\n  position_sec INT,               -- current playhead\n  bitrate_kbps INT NULL,\n  PRIMARY KEY (session_id, event_time)\n);\n\n-- Denormalized current resume pointer (updated by stream job)\nCREATE TABLE resume_state (\n  profile_id BIGINT,\n  title_id BIGINT,\n  episode_id BIGINT NULL,\n  last_position_sec INT,\n  last_update TIMESTAMP,\n  PRIMARY KEY (profile_id, COALESCE(episode_id, title_id))\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/user-viewing-history/#lakehouse-implementation","title":"Lakehouse Implementation","text":"<ul> <li>Storage: Delta/Iceberg with time-based partitioning</li> <li>Partitioning: <code>playback_event</code> by <code>dt=date(event_time)</code> and <code>country_code</code></li> <li>Clustering: <code>playback_event</code> by <code>session_id</code> for efficient session reconstruction</li> <li>Optimization: <code>resume_state</code> is small and kept unpartitioned for fast reads</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/user-viewing-history/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"concepts/Data-Modeling/02-relationships-patterns/user-viewing-history/#1-session-based-tracking","title":"1. Session-Based Tracking","text":"<ul> <li>Each playback session gets a unique ID</li> <li>Events are tied to sessions, not individual users</li> <li>Enables multi-device resume functionality</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/user-viewing-history/#2-event-sourcing-pattern","title":"2. Event Sourcing Pattern","text":"<ul> <li>Store all playback events in chronological order</li> <li>Reconstruct viewing history from events</li> <li>Support debugging and analytics</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/user-viewing-history/#3-denormalized-resume-state","title":"3. Denormalized Resume State","text":"<ul> <li>Pre-computed latest position per profile/content</li> <li>Updated by streaming jobs processing events</li> <li>Fast lookup for \"Continue Watching\" feature</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/user-viewing-history/#common-queries","title":"Common Queries","text":"<pre><code>-- Get continue watching list\nSELECT\n  rs.*,\n  t.title,\n  e.episode_number\nFROM resume_state rs\nLEFT JOIN title t ON rs.title_id = t.title_id\nLEFT JOIN episode e ON rs.episode_id = e.episode_id\nWHERE rs.profile_id = ?\nORDER BY rs.last_update DESC\nLIMIT 10;\n\n-- Reconstruct viewing session timeline\nSELECT\n  pe.*,\n  ps.title_id,\n  ps.episode_id\nFROM playback_session ps\nJOIN playback_event pe ON ps.session_id = pe.session_id\nWHERE ps.profile_id = ?\n  AND ps.started_at &gt;= CURRENT_DATE - INTERVAL '7' DAY\nORDER BY pe.event_time;\n\n-- Calculate total watch time per title\nSELECT\n  ps.title_id,\n  SUM(\n    CASE\n      WHEN pe.event_type = 'END' THEN pe.position_sec\n      ELSE 0\n    END\n  ) AS total_watch_seconds\nFROM playback_session ps\nJOIN playback_event pe ON ps.session_id = pe.session_id\nWHERE ps.profile_id = ?\n  AND pe.event_type = 'END'\nGROUP BY ps.title_id;\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/user-viewing-history/#data-quality-processing","title":"Data Quality &amp; Processing","text":"<pre><code>-- Deduplication check\nSELECT session_id, event_time, COUNT(*) AS dup_count\nFROM playback_event\nGROUP BY session_id, event_time\nHAVING COUNT(*) &gt; 1;\n\n-- Session completeness check\nSELECT\n  ps.session_id,\n  ps.started_at,\n  ps.ended_at,\n  COUNT(pe.event_time) AS event_count\nFROM playback_session ps\nLEFT JOIN playback_event pe ON ps.session_id = pe.session_id\nWHERE ps.ended_at IS NULL\n  AND ps.started_at &lt; CURRENT_TIMESTAMP - INTERVAL '1' HOUR\nGROUP BY ps.session_id, ps.started_at, ps.ended_at;\n</code></pre>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/user-viewing-history/#design-considerations","title":"Design Considerations","text":"<ul> <li>Event-Driven Architecture: Store events, derive state</li> <li>Multi-Device Support: Sessions per device, unified resume state</li> <li>Partial Watching: Precise position tracking for resume</li> <li>Scale: Partition by time for efficient querying</li> <li>Real-time Processing: Streaming jobs for resume state updates</li> </ul>"},{"location":"concepts/Data-Modeling/02-relationships-patterns/user-viewing-history/#follow-up-questions","title":"Follow-up Questions","text":"<ul> <li>How would you handle multiple concurrent sessions per profile?</li> <li>How would you design for efficient \"continue watching\" queries?</li> <li>How would you handle session cleanup for abandoned sessions?</li> <li>How would you implement cross-device resume synchronization?</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/","title":"Event Streaming","text":"<p>This section covers event-driven data models for streaming platforms, focusing on real-time data collection, experiment tracking, and performance monitoring.</p>"},{"location":"concepts/Data-Modeling/03-event-streaming/#files-in-this-section","title":"Files in this section","text":"<ul> <li>A/B Testing Data Models - Experiment metadata, user assignments, and outcome measurement for recommendation algorithm tests</li> <li>Engagement Metrics - Daily Active Users (DAU), event tracking, and user engagement analysis by region</li> <li>Streaming Quality Metrics - Playback quality monitoring, buffering analysis, and CDN performance tracking</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"concepts/Data-Modeling/03-event-streaming/#event-driven-architecture","title":"Event-Driven Architecture","text":"<ul> <li>Append-only event storage for audit trails</li> <li>Time-based partitioning for efficient querying</li> <li>Clustering for related event grouping</li> <li>Real-time vs. batch processing trade-offs</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/#experimentation-systems","title":"Experimentation Systems","text":"<ul> <li>Consistent user assignment across sessions</li> <li>Statistical rigor with guardrails and CUPED</li> <li>Multi-variant testing with traffic allocation</li> <li>Model versioning and lineage tracking</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/#quality-of-service-qos","title":"Quality of Service (QoS)","text":"<ul> <li>Granular event collection for detailed analysis</li> <li>Rollup aggregations for performance dashboards</li> <li>Real-time alerting for service degradation</li> <li>Device and region-specific optimizations</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/#common-interview-questions","title":"Common Interview Questions","text":"<ul> <li>How would you design real-time DAU tracking at scale?</li> <li>What are the challenges of consistent experiment assignment?</li> <li>How do you balance event granularity with storage costs?</li> <li>How would you monitor streaming quality across global CDNs?</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/#technical-considerations","title":"Technical Considerations","text":"<ul> <li>Event Schema Evolution: Handle changing event structures over time</li> <li>Late Arriving Data: Process events that arrive after their timestamp</li> <li>Event Deduplication: Handle duplicate events from client retries</li> <li>Stream Processing: Real-time aggregation vs. batch processing trade-offs</li> </ul> <p>Navigate back to Data Modeling Index</p>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/","title":"A/B Testing Data Models","text":""},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#overview","title":"Overview","text":"<p>Designing data models to track experiment groups, user assignments, and outcomes for recommendation algorithm experiments and other A/B tests.</p>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#core-concepts","title":"Core Concepts","text":"<ul> <li>Experiment metadata with variants and targeting rules</li> <li>User assignment tracking with consistency guarantees</li> <li>Outcome measurement and statistical analysis support</li> <li>Guardrails and monitoring for experiment integrity</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#core-schema","title":"Core Schema","text":"<pre><code>CREATE TABLE experiment (\n  exp_id STRING PRIMARY KEY,\n  name STRING,\n  start_time TIMESTAMP,\n  end_time TIMESTAMP,\n  unit STRING,                    -- PROFILE or ACCOUNT\n  primary_kpi STRING              -- e.g., '7d hours watched'\n);\n\nCREATE TABLE variant (\n  exp_id STRING REFERENCES experiment(exp_id),\n  variant_id STRING,              -- 'control','treatment_a'\n  traffic_pct DOUBLE,\n  PRIMARY KEY (exp_id, variant_id)\n);\n\nCREATE TABLE assignment (\n  exp_id STRING,\n  unit_id STRING,                 -- profile_id or account_id as text\n  variant_id STRING,\n  assigned_at TIMESTAMP,\n  PRIMARY KEY (exp_id, unit_id)\n) PARTITIONED BY (exp_id);\n\n-- Outcomes joined from facts (watch hours, conversions, etc.)\nCREATE TABLE kpi_outcome (\n  exp_id STRING,\n  unit_id STRING,\n  metric_date DATE,\n  kpi_name STRING,                -- 'watch_hours', 'retention_d30'\n  kpi_value DOUBLE,\n  PRIMARY KEY (exp_id, unit_id, metric_date, kpi_name)\n);\n</code></pre>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#experiment-types","title":"Experiment Types","text":""},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#algorithm-experiments","title":"Algorithm Experiments","text":"<ul> <li>Recommendation algorithms: Collaborative filtering vs. content-based</li> <li>Ranking models: Different ranking functions for content lists</li> <li>Personalization: User profile vs. demographic targeting</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#product-experiments","title":"Product Experiments","text":"<ul> <li>UI changes: Homepage layout, navigation, search interface</li> <li>Content presentation: Artwork, descriptions, metadata display</li> <li>User experience: Onboarding flows, tutorial effectiveness</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#technical-experiments","title":"Technical Experiments","text":"<ul> <li>Performance: Loading speeds, caching strategies, CDN optimization</li> <li>Quality: Bitrate algorithms, adaptive streaming, error handling</li> <li>Infrastructure: Region routing, device optimization</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#assignment-strategies","title":"Assignment Strategies","text":""},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#consistent-hashing","title":"Consistent Hashing","text":"<ul> <li>Ensures users see the same variant across sessions</li> <li>Prevents assignment changes due to cache misses or device switches</li> <li>Critical for recommendation algorithm experiments</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#traffic-allocation","title":"Traffic Allocation","text":"<ul> <li>Gradual rollout: 1% \u2192 5% \u2192 10% \u2192 50% \u2192 100%</li> <li>Regional targeting: Roll out to specific countries first</li> <li>Device targeting: Test on specific device types</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#lakehouse-layout","title":"Lakehouse Layout","text":"<ul> <li><code>experiment</code> and <code>variant</code>: Small dimension tables, unpartitioned</li> <li><code>assignment</code>: Partitioned by <code>exp_id</code> and clustered by <code>unit_id</code> for efficient lookups</li> <li><code>kpi_outcome</code>: Partitioned by <code>metric_date</code> and <code>exp_id</code> for time-series analysis</li> <li>Use Delta Lake for ACID transactions and time travel</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#query-examples","title":"Query Examples","text":"<ul> <li>Experiment results: <code>SELECT variant_id, AVG(kpi_value) FROM kpi_outcome WHERE exp_id = 'rec_algo_v3' GROUP BY variant_id</code></li> <li>Assignment verification: <code>SELECT COUNT(DISTINCT unit_id) FROM assignment WHERE exp_id = 'homepage_redesign'</code></li> <li>Cross-experiment analysis: <code>SELECT exp_id, COUNT(*) FROM assignment GROUP BY exp_id</code></li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#statistical-analysis","title":"Statistical Analysis","text":""},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#guardrails","title":"Guardrails","text":"<ul> <li>Sample ratio mismatch: Ensure assignment ratios match traffic percentages</li> <li>Pre-period bias: Compare pre-experiment behavior between variants</li> <li>CUPED (Controlled-experiment Using Pre-Experiment Data): Reduce variance using pre-experiment metrics</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#power-analysis","title":"Power Analysis","text":"<ul> <li>Minimum detectable effect: Calculate required sample size for statistical significance</li> <li>Duration planning: Time needed to reach statistical significance</li> <li>Multiple testing correction: Bonferroni adjustment for multiple KPIs</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#trade-offs-discussion","title":"Trade-offs Discussion","text":""},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#unit-selection","title":"Unit Selection","text":"<ul> <li>Profile-level: Better for personalization experiments, avoids household contamination</li> <li>Account-level: Easier for billing/pricing experiments, avoids profile gaming</li> <li>Device-level: Best for technical/performance experiments</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#assignment-consistency","title":"Assignment Consistency","text":"<ul> <li>Strict consistency: Users always see same variant (better for recommendations)</li> <li>Session consistency: Assignment can change between sessions (simpler implementation)</li> <li>Hybrid: Consistent within experiment but can change between experiments</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#streaming-platform-context","title":"Streaming Platform Context","text":"<ul> <li>Scale challenges: 200M+ users require careful experiment design</li> <li>Global experiments: Different content libraries and user behaviors by region</li> <li>Algorithm complexity: Recommendation experiments affect long-term user engagement</li> <li>Ethical considerations: Avoid unfair treatment, ensure user welfare</li> <li>Continuous experimentation: Thousands of experiments running simultaneously</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#experiment-health","title":"Experiment Health","text":"<ul> <li>Assignment rate: Percentage of users successfully assigned</li> <li>Variant distribution: Actual vs. expected traffic allocation</li> <li>Data quality: Missing or corrupted experiment data</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#business-impact","title":"Business Impact","text":"<ul> <li>Primary KPIs: Revenue, engagement, retention metrics</li> <li>Secondary KPIs: User satisfaction, content discovery</li> <li>Guardrail metrics: Error rates, performance impact</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/ab-testing-data-models/#follow-up-considerations","title":"Follow-up Considerations","text":"<ul> <li>How would you design experiments for global recommendation systems?</li> <li>What are the trade-offs between different experiment assignment strategies?</li> <li>How do you ensure statistical rigor in A/B testing at scale?</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/","title":"Engagement Metrics","text":""},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#overview","title":"Overview","text":"<p>Designing data models to capture user engagement metrics like Daily Active Users (DAU) by region, enabling efficient aggregation from client logs and app events.</p>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#ingest-architecture","title":"Ingest Architecture","text":"<ul> <li>Client/app pings \u2192 <code>app_event</code> (login, foreground, heartbeat) with <code>profile_id</code>, <code>region</code>, <code>event_time</code>.</li> <li>Event streaming platform (Kafka) \u2192 Bronze layer (raw events) \u2192 Silver layer (cleaned) \u2192 Gold layer (metrics).</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#core-schema","title":"Core Schema","text":"<pre><code>CREATE TABLE app_event (\n  profile_id BIGINT,\n  account_id BIGINT,\n  region STRING,                      -- e.g., US, BR, IN\n  event_name STRING,\n  event_time TIMESTAMP\n) PARTITIONED BY (dt DATE)            -- dt = DATE(event_time)\nCLUSTER BY (region, profile_id);\n</code></pre>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#metric-computations","title":"Metric Computations","text":"<pre><code>-- DAU per region (distinct profiles with any event)\nCREATE OR REPLACE VIEW dau_by_region AS\nSELECT\n  dt,\n  region,\n  COUNT(DISTINCT profile_id) AS dau\nFROM app_event\nGROUP BY dt, region;\n\n-- Engagement depth metrics\nCREATE TABLE engagement_metrics AS\nSELECT\n  dt,\n  region,\n  profile_id,\n  COUNT(CASE WHEN event_name = 'play' THEN 1 END) AS play_events,\n  COUNT(CASE WHEN event_name = 'search' THEN 1 END) AS search_events,\n  COUNT(DISTINCT title_id) AS unique_titles_viewed,\n  SUM(CASE WHEN event_name = 'play' THEN duration_sec ELSE 0 END) AS total_watch_time_sec\nFROM enriched_app_events\nGROUP BY dt, region, profile_id;\n</code></pre>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#lakehouse-layout","title":"Lakehouse Layout","text":"<ul> <li>Bronze layer: Raw <code>app_event</code> partitioned by <code>dt</code>, minimal transformation</li> <li>Silver layer: Enriched events with dimensions (user country, device type, content metadata)</li> <li>Gold layer: Aggregated metrics pre-computed for dashboard and analysis</li> <li>Incremental processing: Use Delta Live Tables or Spark Structured Streaming for continuous updates</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#query-examples","title":"Query Examples","text":"<ul> <li>DAU trend: <code>SELECT dt, SUM(dau) FROM dau_by_region WHERE dt &gt;= '2024-01-01' GROUP BY dt ORDER BY dt</code></li> <li>Regional analysis: <code>SELECT region, AVG(dau) FROM dau_by_region WHERE dt &gt;= '2024-01-01' GROUP BY region</code></li> <li>Engagement segments: <code>SELECT CASE WHEN play_events &gt;= 5 THEN 'high' ELSE 'low' END as segment, COUNT(*) FROM engagement_metrics GROUP BY 1</code></li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#event-types","title":"Event Types","text":""},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#core-engagement-events","title":"Core Engagement Events","text":"<ul> <li><code>app_start</code> - Application launched</li> <li><code>login</code> - User authentication</li> <li><code>browse</code> - Content browsing/scrolling</li> <li><code>search</code> - Search queries</li> <li><code>play</code> - Content playback started</li> <li><code>pause</code> - Playback paused</li> <li><code>resume</code> - Playback resumed</li> <li><code>stop</code> - Playback stopped</li> <li><code>heartbeat</code> - Periodic activity ping</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#business-events","title":"Business Events","text":"<ul> <li><code>subscription_change</code> - Plan upgrades/downgrades</li> <li><code>profile_switch</code> - Switching between profiles</li> <li><code>device_change</code> - Switching devices</li> <li><code>content_rating</code> - Thumbs up/down</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#metric-definitions","title":"Metric Definitions","text":""},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#primary-metrics","title":"Primary Metrics","text":"<ul> <li>DAU (Daily Active Users): Distinct profiles active per day</li> <li>MAU (Monthly Active Users): Distinct profiles active in rolling 30 days</li> <li>Session duration: Time spent in app per session</li> <li>Content engagement: Plays, searches, browsing time</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#derived-metrics","title":"Derived Metrics","text":"<ul> <li>Retention rate: Percentage of users returning after N days</li> <li>Engagement rate: DAU/MAU ratio</li> <li>Churn rate: Percentage of users who become inactive</li> <li>Feature adoption: Usage of specific app features</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#trade-offs-discussion","title":"Trade-offs Discussion","text":""},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#granularity-vs-performance","title":"Granularity vs. Performance","text":"<ul> <li>Detailed events: Enable deep analysis but require significant storage and processing</li> <li>Aggregated metrics: Fast queries but lose ability to drill down</li> <li>Hybrid approach: Keep detailed events for recent data, aggregate older data</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#real-time-vs-accuracy","title":"Real-time vs. Accuracy","text":"<ul> <li>Real-time dashboards: Approximate counts for immediate insights</li> <li>Batch accuracy: Precise metrics computed with full data</li> <li>Lambda architecture: Combine both approaches for comprehensive analytics</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#streaming-platform-context","title":"Streaming Platform Context","text":"<ul> <li>Global scale: 200M+ subscribers across 190+ countries</li> <li>Multiple apps: TV, mobile, web, gaming console platforms</li> <li>Content diversity: Movies, TV shows, documentaries, stand-up comedy</li> <li>Regional preferences: Different content popularity by region</li> <li>Device ecosystem: Smart TVs, streaming devices, mobile, web browsers</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#analytics-use-cases","title":"Analytics Use Cases","text":""},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#product-analytics","title":"Product Analytics","text":"<ul> <li>Feature usage and adoption rates</li> <li>User journey analysis and funnel metrics</li> <li>A/B test results and experiment analysis</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#operational-monitoring","title":"Operational Monitoring","text":"<ul> <li>System performance and reliability metrics</li> <li>Content delivery and playback success rates</li> <li>Regional infrastructure utilization</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#business-intelligence","title":"Business Intelligence","text":"<ul> <li>Revenue per user and subscription analytics</li> <li>Content performance and popularity trends</li> <li>Market expansion and growth metrics</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/engagement-metrics/#follow-up-considerations","title":"Follow-up Considerations","text":"<ul> <li>How would you design a real-time DAU dashboard that handles scale?</li> <li>What are the trade-offs between different event sampling strategies?</li> <li>How do you ensure data quality and consistency across different app platforms?</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/","title":"Streaming Quality Metrics","text":""},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/#overview","title":"Overview","text":"<p>Designing a schema to log playback quality metrics including bitrate, buffering, errors, and streaming session data for monitoring and analytics.</p>"},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/#core-concepts","title":"Core Concepts","text":"<ul> <li>Use <code>playback_event</code> (from viewing history) with QoS fields; or a dedicated QoS topic/table.</li> <li>Granular event logging enables detailed analysis of streaming performance and user experience.</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/#core-schema","title":"Core Schema","text":"<pre><code>CREATE TABLE qos_event (\n  session_id STRING,\n  event_time TIMESTAMP,\n  q_event STRING,                 -- STARTUP, BUFFERING_START, BUFFERING_END, BITRATE_CHANGE, ERROR\n  position_sec INT,\n  bitrate_kbps INT,\n  cdn STRING,\n  error_code STRING NULL,\n  PRIMARY KEY (session_id, event_time)\n) PARTITIONED BY (dt DATE);\n</code></pre>"},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/#rollup-aggregations","title":"Rollup Aggregations","text":"<pre><code>-- Per session QoS summary\nCREATE TABLE qos_session_summary AS\nSELECT\n  session_id,\n  MIN(event_time) AS start_time,\n  MAX(event_time) AS end_time,\n  SUM(CASE WHEN q_event='BUFFERING_START' THEN 1 ELSE 0 END) AS rebuffer_count,\n  SUM(rebuffer_duration_sec) AS rebuffer_time_sec,   -- computed via pairing START/END\n  AVG(bitrate_kbps) AS avg_bitrate\nFROM transform_qos_events(/* window/pairs */)\nGROUP BY session_id;\n</code></pre>"},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/#lakehouse-layout","title":"Lakehouse Layout","text":"<ul> <li><code>qos_event</code> partition by <code>dt = DATE(event_time)</code> for time-based analysis</li> <li>Cluster by <code>session_id</code> to keep related events together</li> <li>Consider additional partitioning by <code>cdn</code> or <code>region</code> for geo-specific analysis</li> <li><code>qos_session_summary</code> can be partitioned by session date for efficient rollup queries</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/#query-examples","title":"Query Examples","text":"<ul> <li>Rebuffer analysis: <code>SELECT AVG(rebuffer_ratio) FROM qos_session_summary WHERE dt &gt;= '2024-01-01'</code></li> <li>CDN performance: <code>SELECT cdn, AVG(avg_bitrate), COUNT(*) FROM qos_session_summary GROUP BY cdn</code></li> <li>Error investigation: <code>SELECT error_code, COUNT(*) FROM qos_event WHERE dt = '2024-01-01' GROUP BY error_code</code></li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/#metric-definitions","title":"Metric Definitions","text":"<ul> <li><code>rebuffer_ratio = rebuffer_time / session_time</code> - Primary QoS metric</li> <li><code>startup_time</code> - Time from play button to first frame</li> <li><code>bitrate_switches</code> - Count of adaptive bitrate changes during session</li> <li><code>error_rate</code> - Percentage of sessions with streaming errors</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/#trade-offs-discussion","title":"Trade-offs Discussion","text":"<ul> <li>Granular vs. Aggregated: Store detailed events for analysis vs. pre-computed metrics for dashboards</li> <li>Real-time vs. Batch: Stream processing for real-time alerts vs. batch processing for historical analysis</li> <li>Storage cost: Detailed events provide flexibility but increase storage costs significantly</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/#streaming-platform-context","title":"Streaming Platform Context","text":"<ul> <li>Global CDN optimization: Multiple CDNs need performance monitoring across all</li> <li>Device diversity: QoS metrics vary significantly by device type (mobile, TV, web, gaming consoles)</li> <li>Content-adaptive streaming: Different content types have different quality requirements</li> <li>Regional performance: Internet infrastructure varies dramatically by region</li> <li>Real-time monitoring: Critical for detecting and responding to streaming issues immediately</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/#performance-monitoring-use-cases","title":"Performance Monitoring Use Cases","text":""},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/#real-time-alerts","title":"Real-time Alerts","text":"<ul> <li>Sudden spike in rebuffer rates</li> <li>Regional streaming degradation</li> <li>CDN performance issues</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/#capacity-planning","title":"Capacity Planning","text":"<ul> <li>Peak concurrent streams by region</li> <li>Bandwidth utilization trends</li> <li>Content popularity vs. quality trade-offs</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/#product-optimization","title":"Product Optimization","text":"<ul> <li>A/B tests of different bitrate algorithms</li> <li>Device-specific quality optimizations</li> <li>Content encoding improvements</li> </ul>"},{"location":"concepts/Data-Modeling/03-event-streaming/streaming-quality-metrics/#follow-up-considerations","title":"Follow-up Considerations","text":"<ul> <li>How would you design real-time QoS monitoring and alerting?</li> <li>What are the trade-offs between different QoS metrics for different content types?</li> <li>How do you handle the volume of QoS events at scale?</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/","title":"Scalability Patterns","text":"<p>This section covers advanced scalability patterns for handling massive data volumes, high throughput, and global distribution in streaming platforms.</p>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/#files-in-this-section","title":"Files in this section","text":"<ul> <li>Event Streaming at Scale - Large-scale event streaming architecture, Bronze/Silver/Gold layers, and real-time processing patterns</li> <li>Partitioning &amp; Sharding - Data partitioning strategies, sharding techniques, and hot key management for distributed systems</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/#large-scale-event-processing","title":"Large-Scale Event Processing","text":"<ul> <li>Bronze/Silver/Gold data lake architecture</li> <li>Schema evolution and backward compatibility</li> <li>Cross-region replication and disaster recovery</li> <li>Real-time vs. batch processing trade-offs</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/#data-distribution-strategies","title":"Data Distribution Strategies","text":"<ul> <li>Date-based and composite partitioning</li> <li>Consistent hashing for sharding</li> <li>Hot key management and load balancing</li> <li>Multi-tier storage (hot/warm/cold)</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>File sizing and compaction strategies</li> <li>Metadata caching and query optimization</li> <li>Partition pruning and clustering</li> <li>Storage format selection (Delta, Parquet, etc.)</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/#common-interview-questions","title":"Common Interview Questions","text":"<ul> <li>How would you design a data lake to handle billions of events daily?</li> <li>What are the trade-offs between different partitioning strategies?</li> <li>How do you handle hot partitions and data skew?</li> <li>How would you migrate a large dataset without downtime?</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/#technical-considerations","title":"Technical Considerations","text":"<ul> <li>Storage Formats: Delta Lake/Iceberg for ACID transactions and time travel</li> <li>Compression: ZSTD/Snappy for balancing compression ratio and speed</li> <li>File Sizing: 128-512 MB optimal files for parallel processing</li> <li>Query Engines: Trino/Spark for interactive analytics, streaming for real-time</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/#monitoring-operations","title":"Monitoring &amp; Operations","text":"<ul> <li>Operational Metrics: Throughput, latency, error rates, data quality</li> <li>Maintenance Tasks: Rebalancing, compaction, retention policies</li> <li>Disaster Recovery: Cross-region replication, event replay capabilities</li> <li>Cost Optimization: Lifecycle management and storage tiering</li> </ul> <p>Navigate back to Data Modeling Index</p>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/","title":"Event Streaming at Scale","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#overview","title":"Overview","text":"<p>Designing scalable event streaming architecture to handle billions of streaming events daily while maintaining both queryability and performance.</p>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#ingestion-design","title":"Ingestion Design","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#event-sources","title":"Event Sources","text":"<ul> <li>Client applications: Mobile, TV, web apps generating user interaction events</li> <li>Content delivery: CDN logs, streaming quality metrics, playback events</li> <li>Backend services: Recommendation requests, search queries, authentication events</li> <li>Business systems: Subscription changes, billing events, marketing interactions</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#streaming-platform","title":"Streaming Platform","text":"<ul> <li>Apache Kafka or Amazon Kinesis for event ingestion</li> <li>Schema Registry (Avro/Protobuf) for evolution and backward compatibility</li> <li>Idempotent producers with exactly-once semantics</li> <li>Multi-region deployment for global resilience</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#partitioning-strategy","title":"Partitioning Strategy","text":"<pre><code>-- Kafka topic partitioning\nCREATE TOPIC playback_events WITH (\n  partitions = 256,  -- Scale with throughput\n  replication = 3     -- Cross-AZ resilience\n);\n\n-- Keys for affinity and ordering\nKey strategies:\n- session_id: Groups events from same playback session\n- profile_id: Enables per-user analytics and recommendations\n- Composite keys: (profile_id, event_type) for specialized processing\n</code></pre>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#schema-evolution","title":"Schema Evolution","text":"<ul> <li>Backward compatible: New fields can be added without breaking existing consumers</li> <li>Forward compatible: Old consumers can read data written by new producers</li> <li>Versioned schemas: Track schema changes over time for debugging</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#bronzesilvergold-architecture","title":"Bronze/Silver/Gold Architecture","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#bronze-layer-raw","title":"Bronze Layer (Raw)","text":"<ul> <li>Purpose: Immutable raw data for compliance and reprocessing</li> <li>Storage: Delta Lake with minimal transformations</li> <li>Partitioning: By date (<code>dt = DATE(event_time)</code>) and ingestion time</li> <li>Retention: 7-90 days depending on data type</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#silver-layer-cleaned","title":"Silver Layer (Cleaned)","text":"<ul> <li>Purpose: Business-ready data with consistent schemas</li> <li>Transformations:</li> <li>Type coercion and validation</li> <li>Deduplication on natural keys</li> <li>Dimension table joins (user, content, device metadata)</li> <li>Data quality checks and quarantine</li> <li>Optimization: Z-Order clustering by common query dimensions</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#gold-layer-aggregated","title":"Gold Layer (Aggregated)","text":"<ul> <li>Purpose: Pre-computed metrics and business intelligence</li> <li>Aggregations:</li> <li>Real-time dashboards (last 30 days)</li> <li>Historical analytics (rolling windows)</li> <li>ML feature stores (user behavior patterns)</li> <li>Experiment results (A/B test outcomes)</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#storage-formats-optimization","title":"Storage Formats &amp; Optimization","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#file-formats","title":"File Formats","text":"<ul> <li>Delta Lake/Iceberg: ACID transactions, time travel, schema evolution</li> <li>Parquet: Columnar storage, compression, predicate pushdown</li> <li>ZSTD/Snappy compression: Balance between compression ratio and speed</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>File sizing: 128-512 MB optimal files for parallel processing</li> <li>Compaction jobs: Merge small files during off-peak hours</li> <li>Metadata caching: Speed up file discovery and planning</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#queryability-patterns","title":"Queryability Patterns","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#real-time-serving","title":"Real-time Serving","text":"<ul> <li>Structured Streaming: Continuous processing with Delta Live Tables</li> <li>Materialized views: Pre-computed aggregations for common queries</li> <li>Caching layers: Redis/Memcached for hot data paths</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#ad-hoc-analytics","title":"Ad-hoc Analytics","text":"<ul> <li>Trino/Spark SQL: Interactive querying across all layers</li> <li>Clustering keys: Optimize for common filter patterns</li> <li>Partition pruning: Date-based partitioning for time-series queries</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#scalability-challenges","title":"Scalability Challenges","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#hot-partition-management","title":"Hot Partition Management","text":"<ul> <li>VIP users: Heavy users causing partition hotspots</li> <li>Popular content: Viral content generating disproportionate events</li> <li>Regional events: Country-specific content launches</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#mitigation-strategies","title":"Mitigation Strategies","text":"<ul> <li>Key salting: Add random prefix to distribute load across partitions</li> <li>Dynamic partitioning: Rebalance partitions based on load patterns</li> <li>Multi-level partitioning: Date + hash-based partitioning</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#monitoring-observability","title":"Monitoring &amp; Observability","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#operational-metrics","title":"Operational Metrics","text":"<ul> <li>Throughput: Events per second, bytes per second</li> <li>Latency: End-to-end processing time, consumer lag</li> <li>Error rates: Failed events, schema validation errors</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#data-quality-metrics","title":"Data Quality Metrics","text":"<ul> <li>Completeness: Missing events, null rates</li> <li>Accuracy: Schema validation, business rule compliance</li> <li>Freshness: Data age, processing delays</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#streaming-platform-context","title":"Streaming Platform Context","text":"<ul> <li>Global scale: 200M+ subscribers generating billions of events daily</li> <li>Multi-CDN architecture: Events from multiple content delivery networks</li> <li>Device diversity: 1000+ device types with different event patterns</li> <li>Content velocity: New content launches generating massive event spikes</li> <li>Real-time personalization: Events must be processed quickly for recommendations</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#cross-region-replication","title":"Cross-Region Replication","text":"<ul> <li>Active-active: Multiple regions serving traffic</li> <li>Event replay: Ability to reprocess historical events</li> <li>Gradual catch-up: New regions can catch up without downtime</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#data-consistency","title":"Data Consistency","text":"<ul> <li>Exactly-once processing: Ensure no duplicate events in analytics</li> <li>Idempotent operations: Safe retry without side effects</li> <li>Transaction boundaries: Group related operations atomically</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/event-streaming-at-scale/#follow-up-considerations","title":"Follow-up Considerations","text":"<ul> <li>How would you design event streaming for global infrastructure?</li> <li>What are the trade-offs between different partitioning strategies?</li> <li>How do you ensure data quality at scale?</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/","title":"Partitioning &amp; Sharding","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#overview","title":"Overview","text":"<p>Designing partitioning keys and sharding strategies for storing user watch history in distributed systems, balancing write load and enabling common queries.</p>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#core-goals","title":"Core Goals","text":"<ul> <li>Balance write load: Avoid hot partitions that cause performance bottlenecks</li> <li>Enable common queries: Optimize for frequently accessed data patterns</li> <li>Support scale: Handle massive user base and content catalog</li> <li>Ensure availability: Maintain service during failures and maintenance</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#partitioning-strategies","title":"Partitioning Strategies","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#lakehouse-partitioning","title":"Lakehouse Partitioning","text":"<p>Date-based partitioning (most common):</p> <pre><code>-- Primary partitioning by event time\nCREATE TABLE playback_event (\n  session_id STRING,\n  profile_id BIGINT,\n  event_time TIMESTAMP,\n  event_type STRING,\n  position_sec INT\n) PARTITIONED BY (dt DATE);  -- dt = DATE(event_time)\n</code></pre> <p>Composite partitioning (for complex queries):</p> <pre><code>-- Multiple levels of partitioning\nPARTITIONED BY (\n  year INT,     -- Year for long-term retention\n  month INT,    -- Month for medium-term analysis\n  dt DATE       -- Day for operational queries\n)\n</code></pre>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#clusteringbucketing","title":"Clustering/Bucketing","text":"<p>Co-locate related data for efficient joins:</p> <pre><code>-- Cluster by session_id for session analysis\nCLUSTER BY (session_id)\n\n-- Bucket by profile_id for user analytics\nCLUSTER BY (profile_id) INTO 256 BUCKETS\n</code></pre>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#sharding-strategies","title":"Sharding Strategies","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#oltp-sharding-real-time-serving","title":"OLTP Sharding (Real-time Serving)","text":"<p>Profile-based sharding:</p> <pre><code># Consistent hashing for profile_id\n\ndef get_shard(profile_id):\n    hash_value = hash(profile_id) % num_shards\n    return f\"shard_{hash_value}\"\n\n# Pros Isolates heavy users, good for per-profile queries\n\n# Cons Complicates account-level queries\n</code></pre> <p>Account-based sharding:</p> <pre><code>def get_shard(account_id):\n    return f\"account_shard_{account_id % num_shards}\"\n\n# Pros Easy household-level enforcement (device limits)\n\n# Cons Uneven load if account sizes vary significantly\n</code></pre>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#hot-key-management","title":"Hot Key Management","text":"<p>Key salting for VIP users:</p> <pre><code>def salt_key(profile_id, num_salt_buckets=10):\n    if is_vip_user(profile_id):\n        salt = random.randint(0, num_salt_buckets)\n        return f\"{salt}_{profile_id}\"\n    return profile_id\n</code></pre> <p>Composite keys for write spreading:</p> <pre><code>def composite_key(profile_id):\n    # Add modulo-based suffix for heavy users\n    if is_heavy_user(profile_id):\n        suffix = profile_id % 100  # Spread across 100 virtual shards\n        return f\"{profile_id}_{suffix}\"\n    return profile_id\n</code></pre>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#storage-tiers","title":"Storage Tiers","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#hot-storage-real-time","title":"Hot Storage (Real-time)","text":"<ul> <li>Cassandra/DynamoDB: For serving layer with strict SLAs</li> <li>Sharding: By user ID for personalization queries</li> <li>Replication: Multi-region for global availability</li> <li>TTL: Automatic expiration of old data</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#warm-storage-analytics","title":"Warm Storage (Analytics)","text":"<ul> <li>Delta Lake: For batch analytics and ML feature engineering</li> <li>Partitioning: Date-based for time-series analysis</li> <li>Clustering: By user dimensions for efficient queries</li> <li>Compression: ZSTD for storage efficiency</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#cold-storage-archive","title":"Cold Storage (Archive)","text":"<ul> <li>S3/Cloud Storage: For long-term retention and compliance</li> <li>Partitioning: Year/month for cost-effective lifecycle management</li> <li>Compression: High compression ratios for archival</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#query-optimization","title":"Query Optimization","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#serving-layer-queries","title":"Serving Layer Queries","text":"<pre><code>-- Fast user queries (OLTP)\nSELECT * FROM user_sessions\nWHERE profile_id = ? AND session_id = ?\nORDER BY event_time DESC\nLIMIT 10\n\n-- Optimized with composite index on (profile_id, event_time)\n</code></pre>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#analytics-queries","title":"Analytics Queries","text":"<pre><code>-- Aggregation queries (OLAP)\nSELECT\n  dt,\n  COUNT(DISTINCT profile_id) as dau,\n  AVG(session_duration) as avg_duration\nFROM playback_events\nWHERE dt &gt;= '2024-01-01'\nGROUP BY dt\nORDER BY dt\n\n-- Optimized with date partitioning and clustering\n</code></pre>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#trade-offs-discussion","title":"Trade-offs Discussion","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#partition-size-vs-file-count","title":"Partition Size vs. File Count","text":"<ul> <li>Small partitions: Better parallelism, more files \u2192 higher metadata overhead</li> <li>Large partitions: Fewer files, better compression \u2192 slower queries</li> <li>Optimal: 128-512 MB files, balance between metadata cost and query performance</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#normalization-vs-denormalization","title":"Normalization vs. Denormalization","text":"<ul> <li>Normalized: Consistent updates, smaller storage \u2192 complex joins</li> <li>Denormalized: Fast queries, larger storage \u2192 update complexity</li> <li>Hybrid: Normalized dimensions, denormalized facts</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#consistency-vs-availability","title":"Consistency vs. Availability","text":"<ul> <li>Strong consistency: Guaranteed accuracy \u2192 higher latency</li> <li>Eventual consistency: Better performance \u2192 potential staleness</li> <li>Context-dependent: User-facing features need strong consistency</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#streaming-platform-context","title":"Streaming Platform Context","text":"<ul> <li>User scale: 200M+ profiles requiring massive parallel processing</li> <li>Content velocity: New releases generate immediate hot keys</li> <li>Global distribution: Users across 190+ countries with regional preferences</li> <li>Device diversity: Multiple device types with different usage patterns</li> <li>Real-time requirements: Resume functionality needs immediate consistency</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#monitoring-maintenance","title":"Monitoring &amp; Maintenance","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#operational-metrics","title":"Operational Metrics","text":"<ul> <li>Partition balance: Data distribution across partitions</li> <li>Hot spot detection: Identify and mitigate overloaded partitions</li> <li>Query performance: Monitor slow queries and optimize access patterns</li> <li>Storage utilization: Track growth and plan capacity</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#maintenance-operations","title":"Maintenance Operations","text":"<ul> <li>Rebalancing: Redistribute data during off-peak hours</li> <li>Compaction: Merge small files to optimize storage</li> <li>Retention: Implement data lifecycle policies</li> <li>Backup: Ensure disaster recovery capabilities</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#migration-strategies","title":"Migration Strategies","text":""},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#zero-downtime-migration","title":"Zero-Downtime Migration","text":"<ol> <li>Dual-write: Write to both old and new partitioning schemes</li> <li>Gradual traffic shift: Move users incrementally to new system</li> <li>Backfill: Populate new partitions with historical data</li> <li>Validation: Ensure data consistency before full cutover</li> </ol>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#schema-evolution","title":"Schema Evolution","text":"<ul> <li>Backward compatibility: Support old queries during transition</li> <li>Forward compatibility: New system handles old data formats</li> <li>Gradual rollout: Test with small user segments first</li> </ul>"},{"location":"concepts/Data-Modeling/04-scalability-patterns/partitioning-sharding/#follow-up-considerations","title":"Follow-up Considerations","text":"<ul> <li>How would you choose partitioning keys for user watch history at scale?</li> <li>What are the trade-offs between different sharding strategies?</li> <li>How do you handle hot keys and data skew in a distributed system?</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/","title":"Data Modeling Concepts for Streaming Services","text":"<p>This comprehensive guide covers data modeling patterns and best practices for streaming platforms, with detailed examples and real-world implementations suitable for technical interviews and production systems.</p>"},{"location":"concepts/Data-Modeling/Streaming-Services/#table-of-contents","title":"\ud83d\udcda Table of Contents","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/#core-entities-for-streaming-platforms","title":"Core Entities for Streaming Platforms","text":"<ul> <li>Content Catalog Modeling - Titles, seasons, episodes, genres, localization</li> <li>Accounts, Profiles &amp; Devices - User accounts, profiles, and device relationships</li> <li>Subscriptions &amp; Billing - Subscription plans, billing cycles, and payment processing</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#data-relationships-features","title":"Data Relationships &amp; Features","text":"<ul> <li>User Viewing History - Playback sessions, events, and resume states</li> <li>Recommendations Storage - Similarity graphs, personalized recommendations</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#analytics-event-streaming","title":"Analytics &amp; Event Streaming","text":"<ul> <li>Engagement Metrics - DAU metrics and user engagement analysis</li> <li>A/B Testing Data Models - Experiment metadata and outcome measurement</li> <li>Streaming Quality Metrics - Playback quality monitoring and buffering analysis</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#scalability-advanced-patterns","title":"Scalability &amp; Advanced Patterns","text":"<ul> <li>Event Streaming at Scale - Large-scale event processing and Bronze/Silver/Gold layers</li> <li>Partitioning &amp; Sharding - Data distribution strategies for watch history</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#additional-considerations","title":"Additional Considerations","text":"<ul> <li>Storage &amp; Governance - PII separation, slowly changing dimensions, data quality</li> <li>Interview Practice - Practice scenarios and trade-offs</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#key-design-principles","title":"\ud83c\udfaf Key Design Principles","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/#data-architecture-patterns","title":"Data Architecture Patterns","text":"<ul> <li>Bronze/Silver/Gold Layers: Raw \u2192 Cleaned \u2192 Business-ready data</li> <li>Event-Driven Design: Immutable events with derived state</li> <li>Multi-Tenant Awareness: Account-level isolation and resource management</li> <li>Global Scale: Cross-region distribution and disaster recovery</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Partitioning Strategies: Date-based, hash-based, and composite partitioning</li> <li>Indexing Patterns: Composite indexes and clustering for query optimization</li> <li>Storage Tiers: Hot/warm/cold storage with lifecycle management</li> <li>Caching Layers: Real-time serving with Redis/Memcached integration</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#data-quality-governance","title":"Data Quality &amp; Governance","text":"<ul> <li>Schema Evolution: Backward/forward compatibility with Avro/Protobuf</li> <li>Data Validation: Constraints, business rules, and quality monitoring</li> <li>Audit Trails: Immutable event logs and change data capture</li> <li>Compliance: PII separation, retention policies, and data sovereignty</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#common-interview-questions","title":"\ud83d\udccb Common Interview Questions","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/#core-data-modeling","title":"Core Data Modeling","text":"<ul> <li>How would you model a content catalog that supports multiple languages and regions?</li> <li>What are the trade-offs between normalized and denormalized approaches?</li> <li>How do you handle many-to-many relationships in a distributed system?</li> <li>How would you design for efficient 'continue watching' functionality?</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#scalability-performance","title":"Scalability &amp; Performance","text":"<ul> <li>How do you choose partitioning keys for time-series data?</li> <li>What strategies do you use to handle hot partitions and data skew?</li> <li>How would you design a real-time DAU dashboard at massive scale?</li> <li>What are the trade-offs between different storage formats (Delta, Parquet, etc.)?</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#real-time-event-streaming","title":"Real-Time &amp; Event Streaming","text":"<ul> <li>How do you ensure consistent user assignment in A/B testing?</li> <li>What are the challenges of event schema evolution at scale?</li> <li>How do you handle late-arriving data in streaming pipelines?</li> <li>How would you design QoS monitoring across global CDNs?</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#advanced-patterns","title":"Advanced Patterns","text":"<ul> <li>How do you implement recommendation systems at scale?</li> <li>What are the patterns for handling subscription state changes?</li> <li>How do you design for multi-device content sync?</li> <li>How would you migrate a large dataset without downtime?</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#architecture-overview","title":"\ud83c\udfd7\ufe0f Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client Apps   \u2502\u2500\u2500\u2500\u25b6\u2502  Event Stream   \u2502\u2500\u2500\u2500\u25b6\u2502  Real-time DB   \u2502\n\u2502                 \u2502    \u2502   (Kafka)       \u2502    \u2502  (Cassandra)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                        \u2502                        \u2502\n         \u25bc                        \u25bc                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Data Lake     \u2502    \u2502   Batch Jobs    \u2502    \u2502   Analytics     \u2502\n\u2502   (Delta Lake)  \u2502    \u2502   (Spark)       \u2502    \u2502   (Redshift)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/Data-Modeling/Streaming-Services/#key-metrics-kpis","title":"\ud83d\udcca Key Metrics &amp; KPIs","text":"<ul> <li>Engagement: DAU/MAU, session duration, content completion rates</li> <li>Quality: Rebuffer ratio, startup time, bitrate distribution</li> <li>Business: Revenue per user, subscription churn, content popularity</li> <li>Technical: Throughput, latency, error rates, data freshness</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#technology-stack-examples","title":"\ud83d\udd27 Technology Stack Examples","text":"<ul> <li>Event Streaming: Apache Kafka, Amazon Kinesis</li> <li>Data Lake: Delta Lake, Apache Iceberg</li> <li>Real-time DB: Apache Cassandra, Amazon DynamoDB</li> <li>Analytics: Apache Spark, Trino, Amazon Redshift</li> <li>Processing: Structured Streaming, Delta Live Tables</li> <li>Storage: S3, Cloud Storage with lifecycle policies</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#learning-path","title":"\ud83d\udcc8 Learning Path","text":"<ol> <li>Core Entities - Understand fundamental data models for streaming platforms</li> <li>Relationships &amp; Features - Learn user behavior and recommendation patterns</li> <li>Analytics &amp; Streaming - Master real-time metrics and experimentation</li> <li>Scalability Patterns - Handle massive scale with advanced partitioning</li> <li>Additional Considerations - Apply data governance and practice interviews</li> </ol>"},{"location":"concepts/Data-Modeling/Streaming-Services/#best-practices","title":"\ud83c\udf93 Best Practices","text":"<ul> <li>Start Simple: Build minimal viable models, then optimize</li> <li>Measure Everything: Instrument data quality and performance metrics</li> <li>Plan for Scale: Design with growth in mind from day one</li> <li>Test Thoroughly: Validate assumptions with A/B tests and monitoring</li> <li>Document Decisions: Keep rationale for design choices and trade-offs</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/#navigation","title":"\ud83d\udd17 Navigation","text":"<ul> <li>\u2b05\ufe0f Back to Data-Modeling Overview</li> <li>\ud83d\udcca SQL Concepts</li> <li>\ud83c\udfd7\ufe0f System Design</li> <li>\ud83c\udfe2 Company Interview Prep</li> </ul> <p>Note: This focused data modeling guide for streaming services combines patterns from major platforms with practical examples suitable for technical interviews and real-world applications.</p>"},{"location":"concepts/Data-Modeling/Streaming-Services/ab-testing/","title":"Ab testing","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/ab-testing/#7-ab-testing-experiments-assignments-outcomes","title":"7) A/B Testing (experiments, assignments, outcomes)","text":"<pre><code>CREATE TABLE experiment (\n  exp_id STRING PRIMARY KEY,\n  name STRING,\n  start_time TIMESTAMP,\n  end_time TIMESTAMP,\n  unit STRING,                    -- PROFILE or ACCOUNT\n  primary_kpi STRING              -- e.g., 7d hours watched\n);\n\nCREATE TABLE variant (\n  exp_id STRING REFERENCES experiment(exp_id),\n  variant_id STRING,              -- 'control','treatment_a'\n  traffic_pct DOUBLE,\n  PRIMARY KEY (exp_id, variant_id)\n);\n\nCREATE TABLE assignment (\n  exp_id STRING,\n  unit_id STRING,                 -- profile_id or account_id as text\n  variant_id STRING,\n  assigned_at TIMESTAMP,\n  PRIMARY KEY (exp_id, unit_id)\n) PARTITIONED BY (exp_id);\n\n-- Outcomes joined from facts (watch hours, conversions, etc.)\nCREATE TABLE kpi_outcome (\n  exp_id STRING,\n  unit_id STRING,\n  metric_date DATE,\n  kpi_name STRING,                -- 'watch_hours', 'retention_d30'\n  kpi_value DOUBLE,\n  PRIMARY KEY (exp_id, unit_id, metric_date, kpi_name)\n);\n</code></pre> <p>Analysis:</p> <ul> <li>Guardrails: compare pre-period covariates; use CUPED or stratification.</li> <li>Avoid contamination: unit = <code>profile</code> for recs; <code>account</code> for pricing.</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/accounts-profiles-devices/","title":"Accounts profiles devices","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/accounts-profiles-devices/#4-accounts-profiles-devices","title":"4) Accounts, Profiles, Devices","text":"<pre><code>CREATE TABLE account (\n  account_id BIGINT PRIMARY KEY,\n  created_at TIMESTAMP,\n  home_country STRING,\n  billing_currency STRING\n);\n\nCREATE TABLE profile (\n  profile_id BIGINT PRIMARY KEY,\n  account_id BIGINT REFERENCES account(account_id),\n  name STRING,\n  is_kids BOOLEAN,\n  language_pref STRING,\n  created_at TIMESTAMP\n);\n\nCREATE TABLE device (\n  device_id STRING PRIMARY KEY,\n  account_id BIGINT REFERENCES account(account_id),\n  device_type STRING,\n  registered_at TIMESTAMP\n);\n\nCREATE TABLE profile_device_link (\n  profile_id BIGINT REFERENCES profile(profile_id),\n  device_id STRING REFERENCES device(device_id),\n  PRIMARY KEY (profile_id, device_id)\n);\n</code></pre> <p>Notes:</p> <ul> <li>Device belongs to account; link to profiles many-to-many for usage/permissions.</li> <li>Enforce device limits via a separate <code>active_streams</code> store or cache keyed by <code>account_id</code>.</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/additional-considerations/","title":"Additional considerations","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/additional-considerations/#11-additional-considerations-storage-and-governance","title":"11. Additional Considerations: Storage and Governance","text":"<p>These guardrails are essential for production data systems and frequently come up in interviews:</p> <ul> <li> <p>PII separation: store PII (email, payment tokens) in a restricted table/key vault; reference by surrogate IDs.</p> </li> <li> <p>Slowly Changing Dimensions: use SCD2 for plans/regions to support point-in-time analytics.</p> </li> <li> <p>Data quality: contracts + expectations (e.g., Great Expectations/Deequ) with fail-open/closed strategy by table.</p> </li> <li> <p>Backfills: design idempotent jobs and partition filters; track backfill lineage.</p> </li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/content-catalog/","title":"Content catalog","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/content-catalog/#1-content-catalog-titles-seasons-episodes-genres-localization","title":"1) Content Catalog (titles, seasons, episodes, genres, localization)","text":"<p>Core entities:</p> <ul> <li><code>title</code> (movie or show), <code>season</code>, <code>episode</code>, <code>genre</code>, many-to-many <code>title_genre</code></li> <li>Localization stored separately per locale.</li> </ul> <p>Relational-ish DDL:</p> <pre><code>-- Type can be MOVIE or SHOW\nCREATE TABLE title (\n  title_id BIGINT PRIMARY KEY,\n  type STRING NOT NULL,\n  release_date DATE,\n  maturity_rating STRING,\n  production_country STRING,\n  is_active BOOLEAN DEFAULT TRUE\n);\n\nCREATE TABLE season (\n  season_id BIGINT PRIMARY KEY,\n  title_id BIGINT NOT NULL REFERENCES title(title_id),\n  season_number INT NOT NULL,\n  UNIQUE(title_id, season_number)\n);\n\nCREATE TABLE episode (\n  episode_id BIGINT PRIMARY KEY,\n  title_id BIGINT NOT NULL REFERENCES title(title_id),\n  season_id BIGINT REFERENCES season(season_id),\n  episode_number INT,\n  duration_sec INT,\n  UNIQUE(title_id, season_id, episode_number)\n);\n\nCREATE TABLE genre (\n  genre_id INT PRIMARY KEY,\n  name STRING UNIQUE\n);\n\nCREATE TABLE title_genre (\n  title_id BIGINT REFERENCES title(title_id),\n  genre_id INT REFERENCES genre(genre_id),\n  PRIMARY KEY (title_id, genre_id)\n);\n\n-- Localization (can apply to titles, seasons, or episodes)\nCREATE TABLE localized_metadata (\n  object_type STRING CHECK (object_type IN ('TITLE','SEASON','EPISODE')),\n  object_id BIGINT,\n  locale STRING,                      -- e.g., en-US, es-MX\n  localized_title STRING,\n  localized_description STRING,\n  PRIMARY KEY (object_type, object_id, locale)\n);\n</code></pre> <p>Lakehouse layout:</p> <ul> <li>Tables in Delta/Iceberg; partition <code>title</code> sparsely (usually not necessary).</li> <li>For big read throughput on <code>episode</code>, consider clustering/bucketing by <code>title_id</code>.</li> <li><code>localized_metadata</code> clustered by <code>(object_type, object_id)</code>; filter by <code>locale</code>.</li> </ul> <p>Queries:</p> <ul> <li>Get all episodes of S2 for a show: filter <code>title_id</code> + <code>season_number=2</code>, join <code>episode</code>.</li> <li>Localized listing: join <code>localized_metadata</code> on requested <code>locale</code>, fallback to <code>en-US</code>.</li> </ul> <p>Trade-offs:</p> <ul> <li>Keep movies and shows unified in <code>title</code> to simplify cross-type queries.</li> <li>Localization separated avoids column bloat and supports sparse locales.</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/engagement-metrics/","title":"Engagement metrics","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/engagement-metrics/#6-engagement-metrics-dau-by-region","title":"6) Engagement Metrics (DAU by region)","text":"<p>Ingest:</p> <ul> <li>Client/app pings \u2192 <code>app_event</code> (login, foreground, heartbeat) with <code>profile_id</code>, <code>region</code>, <code>event_time</code>.</li> </ul> <p>Lakehouse table:</p> <pre><code>CREATE TABLE app_event (\n  profile_id BIGINT,\n  account_id BIGINT,\n  region STRING,                      -- e.g., US, BR, IN\n  event_name STRING,\n  event_time TIMESTAMP\n) PARTITIONED BY (dt DATE)            -- dt = DATE(event_time)\nCLUSTER BY (region, profile_id);\n</code></pre> <p>Metric build:</p> <pre><code>-- DAU per region (distinct profiles with any event)\nCREATE OR REPLACE VIEW dau_by_region AS\nSELECT\n  dt,\n  region,\n  COUNT(DISTINCT profile_id) AS dau\nFROM app_event\nGROUP BY dt, region;\n</code></pre> <p>Notes:</p> <ul> <li>Use incremental processing per <code>dt</code>; compact small files; Z-Order/cluster by <code>region</code> if supported.</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/event-streaming-at-scale/","title":"Event streaming at scale","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/event-streaming-at-scale/#9-event-streaming-scale-schema-processing-queryability","title":"9) Event Streaming @ scale (schema, processing, queryability)","text":"<p>Ingestion design:</p> <ul> <li>Kafka topics: <code>playback_events</code>, <code>app_events</code>, <code>billing_events</code>.</li> <li>Schema Registry with Avro/Protobuf for evolution (backward compatible).</li> <li>Keys:</li> <li><code>session_id</code> for <code>playback_events</code> (affinity ordering).</li> <li><code>profile_id</code> for <code>app_events</code>.</li> <li>Partitions sized to peak throughput; enable idempotent producers + exactly-once sinks (e.g., Spark Structured Streaming + Delta).</li> </ul> <p>Bronze/Silver/Gold:</p> <ul> <li>Bronze: raw JSON/Avro \u2192 <code>dt</code> partition, no transforms.</li> <li>Silver: cleaned &amp; typed; dedup on <code>(session_id, event_time)</code>; join dims (country, device).</li> <li>Gold: business aggregates (<code>resume_state</code>, <code>dau_by_region</code>, <code>qos_session_summary</code>).</li> </ul> <p>Storage formats:</p> <ul> <li>Delta/Iceberg + Parquet; ZSTD or Snappy; optimize file sizes 128\u2013512 MB.</li> </ul> <p>Queryability:</p> <ul> <li>Precompute serving tables (resume, top-K recs).</li> <li>Enable ad-hoc via Trino/Spark; use clustering for high-selectivity keys.</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/interview-practice-note/","title":"Interview practice note","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/interview-practice-note/#interview-practice-note","title":"Interview Practice Note","text":"<p>These data modeling patterns are perfect for whiteboard interviews. For additional practice, consider expanding any section into a 10\u201315 minute walkthrough covering diagrams, trade-offs, and \"what would you do if...\" scenarios.</p>"},{"location":"concepts/Data-Modeling/Streaming-Services/partitioning-sharding/","title":"Partitioning sharding","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/partitioning-sharding/#10-partitioning-sharding-user-watch-history","title":"10) Partitioning &amp; Sharding user watch history","text":"<p>Goal:</p> <ul> <li>Balance write load, avoid hot partitions, enable common queries.</li> </ul> <p>Choices:</p> <ul> <li>Partition column (lake): <code>dt = DATE(event_time)</code>; maybe <code>region</code>.</li> <li>Cluster/Bucket (lake/warehouse): <code>profile_id</code> or <code>session_id</code> to collocate events.</li> <li>OLTP sharding (if needed):</li> <li>Shard key = <code>account_id</code> or <code>profile_id</code> using consistent hashing.</li> <li>Pros of <code>profile_id</code>: isolates heavy users; good affinity for per-profile queries.</li> <li>Pros of <code>account_id</code>: easy household-level enforcement (device limits).</li> <li>Hot keys: VIPs or major launches \u2192 add composite key (<code>profile_id</code> + modulo/random salt) for write spreading; reassemble with map-side combine.</li> <li>Skew management: periodic rebalancing for shards with disproportionate size; use auto-splitting if supported.</li> </ul> <p>Examples:</p> <ul> <li>Lake: <code>playback_event</code> \u2192 <code>PARTITION BY dt</code> and CLUSTER BY (session_id).</li> <li>Warehouse: bucketing <code>playback_event</code> by <code>profile_id</code> into, say, 256 buckets to speed joins with <code>profile</code> and rollups.</li> </ul> <p>Trade-offs:</p> <ul> <li>Over-partitioning (<code>dt</code>, <code>hour</code>, <code>region</code>) \u2192 too many small files. Prefer <code>dt</code> + clustering; run optimize/compaction jobs.</li> <li>Using <code>profile_id</code> as shard key complicates account-level queries; mitigate via secondary indexes/materialized views.</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/recommendations-storage/","title":"Recommendations storage","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/recommendations-storage/#3-recommendations-storage-relationships-features","title":"3) Recommendations storage (relationships &amp; features)","text":"<p>Layers:</p> <ol> <li>Interactions: views, ratings, thumbs, dwell time (from events).</li> <li>Similarity graph: <code>item_item_edge</code> with score &amp; model metadata.</li> <li>Personalized recs: per-profile top-K lists per context (home row, genre row).</li> <li>Model lineage: which model/version produced which list.</li> </ol> <p>DDL (analytics):</p> <pre><code>CREATE TABLE item_item_edge (\n  src_title_id BIGINT,\n  dst_title_id BIGINT,\n  score DOUBLE,\n  model VARCHAR,            -- e.g., 'itemCF', 'graphSage'\n  model_version STRING,\n  computed_at TIMESTAMP,\n  PRIMARY KEY (src_title_id, dst_title_id, model, model_version)\n);\n\nCREATE TABLE personalized_recs (\n  profile_id BIGINT,\n  context STRING,           -- 'home', 'because_you_watched', 'kids'\n  rank INT,\n  title_id BIGINT,\n  score DOUBLE,\n  model STRING,\n  model_version STRING,\n  generated_at TIMESTAMP,\n  PRIMARY KEY (profile_id, context, rank, generated_at)\n);\n\nCREATE TABLE feature_store_items (\n  title_id BIGINT PRIMARY KEY,\n  embedding VECTOR(256)     -- or store in a vector DB; here as opaque blob/bytes\n);\n</code></pre> <p>Serving:</p> <ul> <li>Latest <code>personalized_recs</code> for <code>(profile_id, context)</code>; TTL and regeneration SLAs.</li> <li>For candidate gen on the fly: pull <code>item_item_edge</code> for last-watched titles.</li> </ul> <p>Trade-offs:</p> <ul> <li>Precompute top-K for speed; keep edges for diversity/cold-start logic; store model lineage for A/B/debug.</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/streaming-quality/","title":"Streaming quality","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/streaming-quality/#8-streaming-quality-bitrate-buffering-errors","title":"8) Streaming Quality (bitrate, buffering, errors)","text":"<p>Granular:</p> <ul> <li>Use <code>playback_event</code> (from #2) with QoS fields; or a dedicated QoS topic/table.</li> </ul> <pre><code>CREATE TABLE qos_event (\n  session_id STRING,\n  event_time TIMESTAMP,\n  q_event STRING,                 -- STARTUP, BUFFERING_START, BUFFERING_END, BITRATE_CHANGE, ERROR\n  position_sec INT,\n  bitrate_kbps INT,\n  cdn STRING,\n  error_code STRING NULL,\n  PRIMARY KEY (session_id, event_time)\n) PARTITIONED BY (dt DATE);\n</code></pre> <p>Rollups:</p> <pre><code>-- Per session QoS summary\nCREATE TABLE qos_session_summary AS\nSELECT\n  session_id,\n  MIN(event_time) AS start_time,\n  MAX(event_time) AS end_time,\n  SUM(CASE WHEN q_event='BUFFERING_START' THEN 1 ELSE 0 END) AS rebuffer_count,\n  SUM(rebuffer_duration_sec) AS rebuffer_time_sec,   -- computed via pairing START/END\n  AVG(bitrate_kbps) AS avg_bitrate\nFROM transform_qos_events(/* window/pairs */)\nGROUP BY session_id;\n</code></pre> <p>Use:</p> <ul> <li>Monitor <code>rebuffer_ratio = rebuffer_time / session_time</code> by device/region/CDN.</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/subscriptions-billing/","title":"Subscriptions billing","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/subscriptions-billing/#5-subscriptions-billing-promotions-payments","title":"5) Subscriptions, Billing, Promotions, Payments","text":"<p>Concepts:</p> <ul> <li>Subscription state machine with effective-dated changes (SCD-2 style).</li> <li>Separate invoice and payment for accounting; support partials/refunds.</li> </ul> <pre><code>CREATE TABLE plan (\n  plan_id STRING PRIMARY KEY,        -- 'STANDARD_ADS', 'PREMIUM'\n  price_cents INT,\n  currency STRING,\n  max_screens INT,\n  hdr BOOLEAN,\n  ads_supported BOOLEAN\n);\n\n-- Effective-dated subscription records\nCREATE TABLE subscription (\n  subscription_id BIGINT PRIMARY KEY,\n  account_id BIGINT,\n  plan_id STRING REFERENCES plan(plan_id),\n  status STRING,                     -- ACTIVE, PAUSED, CANCELED\n  start_date DATE,\n  end_date DATE NULL                 -- null = current\n);\n\nCREATE TABLE promotion (\n  promo_code STRING PRIMARY KEY,\n  discount_type STRING,              -- PCT or FIXED\n  discount_value DOUBLE,\n  start_date DATE,\n  end_date DATE\n);\n\nCREATE TABLE subscription_event (\n  event_id BIGINT PRIMARY KEY,\n  subscription_id BIGINT REFERENCES subscription(subscription_id),\n  event_time TIMESTAMP,\n  event_type STRING,                 -- UPGRADE, DOWNGRADE, CANCEL, RESUME, RENEW\n  old_plan_id STRING,\n  new_plan_id STRING,\n  note STRING\n);\n\nCREATE TABLE invoice (\n  invoice_id BIGINT PRIMARY KEY,\n  account_id BIGINT,\n  period_start DATE,\n  period_end DATE,\n  amount_due_cents INT,\n  currency STRING,\n  promo_code STRING NULL REFERENCES promotion(promo_code),\n  created_at TIMESTAMP,\n  status STRING                      -- OPEN, PAID, VOID\n);\n\nCREATE TABLE payment (\n  payment_id BIGINT PRIMARY KEY,\n  invoice_id BIGINT REFERENCES invoice(invoice_id),\n  amount_cents INT,\n  currency STRING,\n  method STRING,                     -- card, paypal, gift\n  status STRING,                     -- AUTH, CAPTURED, REFUNDED, FAILED\n  created_at TIMESTAMP\n);\n</code></pre> <p>Handling mid-cycle plan changes:</p> <ul> <li>Emit <code>subscription_event</code> + proration logic in billing job: create a credit line on current invoice; new pro-rated charge line.</li> </ul>"},{"location":"concepts/Data-Modeling/Streaming-Services/user-viewing-history/","title":"User viewing history","text":""},{"location":"concepts/Data-Modeling/Streaming-Services/user-viewing-history/#2-user-viewing-history-partial-watch-multiple-devices-resume","title":"2) User Viewing History (partial watch, multiple devices, resume)","text":"<p>Concepts:</p> <ul> <li>A playback session per device/profile; fine-grained events; a compact resume state.</li> </ul> <p>DDL:</p> <pre><code>CREATE TABLE account (\n  account_id BIGINT PRIMARY KEY\n);\nCREATE TABLE profile (\n  profile_id BIGINT PRIMARY KEY,\n  account_id BIGINT REFERENCES account(account_id),\n  maturity_setting STRING,\n  country_code STRING\n);\nCREATE TABLE device (\n  device_id STRING PRIMARY KEY,       -- GUID or device fingerprint\n  device_type STRING,                 -- TV, mobile, web\n  os_version STRING\n);\n\nCREATE TABLE playback_session (\n  session_id STRING PRIMARY KEY,      -- UUID\n  profile_id BIGINT REFERENCES profile(profile_id),\n  device_id STRING REFERENCES device(device_id),\n  title_id BIGINT,\n  episode_id BIGINT NULL,\n  started_at TIMESTAMP,\n  ended_at TIMESTAMP NULL\n);\n\n-- Append-only granular timeline\nCREATE TABLE playback_event (\n  session_id STRING,\n  event_time TIMESTAMP,\n  event_type STRING,              -- PLAY, PAUSE, SEEK, BITRATE_CHANGE, END, ERROR\n  position_sec INT,               -- current playhead\n  bitrate_kbps INT NULL,\n  PRIMARY KEY (session_id, event_time)\n);\n\n-- Denormalized current resume pointer (updated by stream job)\nCREATE TABLE resume_state (\n  profile_id BIGINT,\n  title_id BIGINT,\n  episode_id BIGINT NULL,\n  last_position_sec INT,\n  last_update TIMESTAMP,\n  PRIMARY KEY (profile_id, COALESCE(episode_id, title_id))\n);\n</code></pre> <p>Lakehouse:</p> <ul> <li><code>playback_event</code> partition by <code>dt=date(event_time)</code> and optionally <code>country_code</code>; cluster by <code>session_id</code>.</li> <li><code>resume_state</code> small, unpartitioned; updated via incremental job from events.</li> </ul> <p>Continue Watching query:</p> <ul> <li>Select top N from <code>resume_state</code> for a profile ordered by <code>last_update</code> desc.</li> </ul> <p>Trade-offs:</p> <ul> <li>Store events append-only (immutable) \u2192 accurate replays; derive <code>resume_state</code> to serve fast UX.</li> </ul>"},{"location":"concepts/SQL/","title":"SQL Concepts Overview &amp; Learning Hub","text":"<p>Welcome to the comprehensive SQL learning hub! This central overview serves as your main navigation point for mastering SQL concepts across the repository, from basic aggregation to advanced analytics patterns. Whether you're preparing for data engineering interviews or building production SQL skills, this guide provides structured pathways through all SQL concepts with practical Netflix examples.</p>"},{"location":"concepts/SQL/#overview","title":"\ud83c\udfaf Overview","text":"<p>This hub connects general SQL concepts with company-specific interview problems, creating a complete learning ecosystem. Each concept includes hyperlinks to detailed explanations and cross-references to real-world Netflix interview examples where the concepts are applied.</p>"},{"location":"concepts/SQL/#complete-sql-concepts-index","title":"\ud83d\udcda Complete SQL Concepts Index","text":""},{"location":"concepts/SQL/#core-sql-fundamentals","title":"\ud83d\udfe2 Core SQL Fundamentals","text":"<ul> <li>Aggregate Functions - SUM, COUNT, AVG, MIN, MAX with practical examples</li> <li>Monthly Ratings Analysis - Real-world grouping and time-based aggregation</li> </ul>"},{"location":"concepts/SQL/#data-relationships-joins","title":"\ud83d\udfe1 Data Relationships &amp; Joins","text":"<ul> <li>JOINs and Relationships - Multi-table analysis and data relationships</li> <li>Page Impressions Analysis - Business-focused JOIN patterns</li> <li>Subqueries - Correlated and non-correlated subquery patterns</li> </ul>"},{"location":"concepts/SQL/#advanced-sql-techniques","title":"\ud83d\udfe0 Advanced SQL Techniques","text":"<ul> <li>Common Table Expressions (CTEs) - Modular query building and complex analysis</li> <li>CTE vs Window Functions - Choosing the right approach</li> <li>Customer Sales Analysis - Multi-step analytical queries</li> <li>Vacant Days Analysis - Recursive CTEs for date ranges</li> <li>Window Functions - Advanced analytics and ranking operations</li> <li>Window Functions Overview - Complete guide</li> <li>PostgreSQL Interview Examples - Interview-focused problems</li> <li>Ranking Functions - ROW_NUMBER, RANK, DENSE_RANK</li> <li>Navigation Functions - LAG, LEAD, FIRST_VALUE, LAST_VALUE</li> <li>Advanced Concepts - Frames and complex patterns</li> <li>Window Average Comparison - Performance vs accuracy trade-offs</li> </ul>"},{"location":"concepts/SQL/#advanced-patterns-optimization","title":"\ud83d\udd34 Advanced Patterns &amp; Optimization","text":"<ul> <li>Advanced SQL Patterns - Complex query patterns and techniques</li> <li>SQL Optimization - Performance tuning and optimization strategies</li> </ul>"},{"location":"concepts/SQL/#practice-exercises-interview-preparation","title":"\ud83d\udcda Practice Exercises &amp; Interview Preparation","text":""},{"location":"concepts/SQL/#comprehensive-sql-exercise-collections","title":"Comprehensive SQL Exercise Collections","text":"<ul> <li>SQL Exercises from Wikibooks - Complete collection of hands-on SQL exercises organized by difficulty level</li> <li>Beginner: Basic analytics, aggregation, and simple joins</li> <li>Intermediate: Window functions, ranking, and complex patterns</li> <li>Advanced: CTEs, recursive queries, and optimization techniques</li> <li> <p>Includes detailed solutions, explanations, and cross-references to main concepts</p> </li> <li> <p>TopTal SQL Interview Questions - Real-world SQL interview challenges for streaming/media companies</p> </li> <li>Business-focused problems with complete context and solutions</li> <li>Covers user analytics, content performance, and recommendation systems</li> <li>Includes optimization challenges and performance considerations</li> </ul>"},{"location":"concepts/SQL/#structured-exercise-directories","title":"Structured Exercise Directories","text":"<ul> <li>Wikibooks Exercise Directory - Organized practice exercises with schema and sample data</li> <li>TopTal Interview Directory - Advanced interview questions with business context and performance analysis</li> </ul>"},{"location":"concepts/SQL/#sql-concepts-in-netflix-interview-problems","title":"\ud83c\udfe2 SQL Concepts in Netflix Interview Problems","text":""},{"location":"concepts/SQL/#basic-analytics-full-category","title":"\ud83d\udfe2 Basic Analytics (Full Category)","text":"<p>Fundamental SQL analytics focusing on data aggregation and basic metrics.</p> Concept Netflix Example Key Skills COUNT DISTINCT Daily Active Users User metrics, date aggregation Date Arithmetic Day 1 Retention User retention, self-joins Basic Aggregation Most Watched Show Per Day Content analytics, ranking <p>Primary Concepts Used: Aggregation functions, GROUP BY, date functions, basic filtering</p>"},{"location":"concepts/SQL/#window-functions-ranking-full-category","title":"\ud83d\udfe1 Window Functions &amp; Ranking (Full Category)","text":"<p>Advanced problems using window functions and ranking operations.</p> Concept Netflix Example Key Skills DENSE_RANK() Top 3 Shows per Region Regional rankings, content performance LAG/LEAD Functions Consecutive Days Watching User behavior patterns, CTEs PERCENTILE_CONT() Advanced statistical patterns covered in ranking examples Statistical analysis, user segmentation <p>Primary Concepts Used: Window functions, PARTITION BY, ORDER BY, CTEs, statistical functions</p>"},{"location":"concepts/SQL/#advanced-patterns-full-document","title":"\ud83d\udfe0 Advanced Patterns (Full Document)","text":"<p>Complex SQL patterns including recursive queries and advanced analytics.</p> Pattern Application Difficulty Recursive CTEs Employee hierarchy, organizational charts \ud83d\udfe0 Hard Multi-CTE Analysis Customer lifetime value segmentation \ud83d\udfe0 Hard Correlation Analysis Content recommendation analysis \ud83d\udfe0 Hard Statistical Modeling Financial fraud detection \ud83d\udd34 Expert <p>Primary Concepts Used: Recursive CTEs, complex analytics, statistical functions, multi-step problem solving</p>"},{"location":"concepts/SQL/#optimization-performance-full-document","title":"\ud83d\udd34 Optimization &amp; Performance (Full Document)","text":"<p>Query performance optimization and indexing strategies.</p> Focus Area Skills Covered Query Optimization Execution planning, performance tuning Indexing Strategies Index selection, query structure Large-Scale Data Scalability considerations, resource usage <p>Primary Concepts Used: Performance optimization, indexing, query execution planning</p>"},{"location":"concepts/SQL/#learning-progression-difficulty-levels","title":"\ud83c\udfaf Learning Progression &amp; Difficulty Levels","text":"<pre><code>graph TD\n    A[Start: Basic SQL] --&gt; B[\ud83d\udfe2 Aggregate Functions]\n    B --&gt; C[\ud83d\udfe1 JOINs &amp; Subqueries]\n    C --&gt; D[\ud83d\udfe0 CTEs]\n    D --&gt; E[\ud83d\udfe0 Window Functions]\n    E --&gt; F[\ud83d\udd34 Advanced Patterns]\n    F --&gt; G[\ud83d\udd34 Optimization]\n\n    style A fill:#90EE90\n    style B fill:#90EE90\n    style C fill:#FFD700\n    style D fill:#FFA500\n    style E fill:#FFA500\n    style F fill:#FF6347\n    style G fill:#FF6347\n\n</code></pre>"},{"location":"concepts/SQL/#beginner-level-4-6-hours","title":"\ud83d\udfe2 Beginner Level (4-6 hours)","text":"<p>Focus: Build SQL confidence with core operations and hands-on practice</p> <ol> <li>Aggregate Functions</li> <li>Basic Analytics Problems</li> <li> <p>Wikibooks Beginner Exercises - Practice DAU, retention, and content analytics</p> </li> <li> <p>Key Skills: COUNT, SUM, AVG, GROUP BY, basic filtering, user metrics</p> </li> </ol>"},{"location":"concepts/SQL/#intermediate-level-6-10-hours","title":"\ud83d\udfe1 Intermediate Level (6-10 hours)","text":"<p>Focus: Master data relationships, window functions, and complex patterns</p> <ol> <li>JOINs and Relationships</li> <li>Subqueries</li> <li>Wikibooks Window Functions Exercises - Practice ranking and navigation functions</li> <li>TopTal Intermediate Questions - Real-world ranking and analytics problems</li> <li>Key Skills: Multi-table queries, correlated subqueries, window functions, ranking</li> </ol>"},{"location":"concepts/SQL/#advanced-level-10-18-hours","title":"\ud83d\udfe0 Advanced Level (10-18 hours)","text":"<p>Focus: Complex analytical patterns, CTEs, and optimization techniques</p> <ol> <li>CTEs Overview</li> <li>Window Functions Overview</li> <li>Ranking Problems</li> <li> <p>Wikibooks Advanced Patterns - CTEs, recursive queries, and percentile analysis</p> </li> <li> <p>TopTal Advanced Questions - Heavy watchers retention, content hierarchy, statistical functions</p> </li> <li> <p>Key Skills: CTEs, window functions, ranking, recursive queries, statistical analysis</p> </li> </ol>"},{"location":"concepts/SQL/#expert-level-18-hours","title":"\ud83d\udd34 Expert Level (18+ hours)","text":"<p>Focus: Production-ready SQL, performance optimization, and large-scale challenges</p> <ol> <li>Advanced Patterns</li> <li>Optimization Challenges</li> <li> <p>TopTal Optimization Challenges - Efficient DAU calculation, partitioning strategies, Top-N queries</p> </li> <li> <p>Wikibooks Optimization Exercises - Query performance optimization and indexing strategies</p> </li> <li> <p>Key Skills: Recursive queries, performance tuning, large-scale optimization, indexing, partitioning</p> </li> </ol>"},{"location":"concepts/SQL/#concept-to-example-mapping","title":"\ud83d\udd17 Concept-to-Example Mapping","text":""},{"location":"concepts/SQL/#from-sql-concepts-to-netflix-problems","title":"From SQL Concepts to Netflix Problems","text":"SQL Concept Netflix Interview Problems Why It Matters Aggregate Functions Daily Active Users Core business metrics Window Functions Top 3 Shows per Region Content performance analysis CTEs Consecutive Days Watching User behavior patterns Recursive CTEs Advanced Patterns Hierarchical data problems"},{"location":"concepts/SQL/#from-interview-problems-to-sql-concepts","title":"From Interview Problems to SQL Concepts","text":"Netflix Problem SQL Concepts Demonstrated DAU Calculation COUNT DISTINCT, GROUP BY, date functions Top-N Queries DENSE_RANK, PARTITION BY, window functions Retention Analysis Self-joins, date arithmetic, percentage calculations Consecutive Patterns LAG/LEAD, CTEs, window functions"},{"location":"concepts/SQL/#difficulty-assessment-by-category","title":"\ud83d\udcca Difficulty Assessment by Category","text":"Category Difficulty Time to Learn Interview Frequency Business Value Basic Analytics \ud83d\udfe2 Easy 4-6 hours Very High High JOINs &amp; Subqueries \ud83d\udfe1 Medium 6-10 hours High High Window Functions \ud83d\udfe0 Hard 10-18 hours Very High Very High CTEs \ud83d\udfe0 Hard 8-16 hours Medium High Advanced Patterns \ud83d\udd34 Expert 18+ hours Low Very High Optimization \ud83d\udd34 Expert 18+ hours Low Critical Practice Exercises \ud83d\udfe2-\ud83d\udd34 Varies 8-20 hours High Very High"},{"location":"concepts/SQL/#preparation-strategy-by-role","title":"\ud83d\udee0\ufe0f Preparation Strategy by Role","text":""},{"location":"concepts/SQL/#for-data-analyst-roles","title":"For Data Analyst Roles","text":"<ol> <li>Master Basic Analytics (\ud83d\udfe2) - Focus on aggregation and reporting</li> <li>Learn Window Functions (\ud83d\udfe0) - Essential for business intelligence</li> <li>Practice Netflix Examples - Real-world application patterns</li> </ol>"},{"location":"concepts/SQL/#for-data-engineering-roles","title":"For Data Engineering Roles","text":"<ol> <li>All Basic Concepts (\ud83d\udfe2\ud83d\udfe1) - Foundation for complex queries</li> <li>Advanced Patterns (\ud83d\udfe0\ud83d\udd34) - Production-ready SQL</li> <li>Performance Optimization (\ud83d\udd34) - Scale and efficiency</li> </ol>"},{"location":"concepts/SQL/#for-interview-preparation","title":"For Interview Preparation","text":"<ol> <li>Follow Learning Progression - Start with basics, progress to advanced patterns</li> <li>Practice Netflix Problems - Industry-standard challenges with real-world context</li> <li>Complete Wikibooks Exercises - Build foundational skills with structured exercises</li> <li>Master TopTal Challenges - Practice streaming/media interview scenarios</li> <li>Master Window Functions - 70% of SQL interviews focus here, with hands-on practice</li> </ol>"},{"location":"concepts/SQL/#comprehensive-practice-resources","title":"\ud83c\udfaf Comprehensive Practice Resources","text":"<p>This SQL hub provides multiple pathways for skill development, from foundational concepts to interview-ready proficiency:</p>"},{"location":"concepts/SQL/#practice-collections-by-focus-area","title":"Practice Collections by Focus Area","text":"Focus Area Wikibooks Exercises TopTal Challenges Netflix Problems Best For Basic Analytics DAU, Retention, Content Analytics User Metrics, Basic Aggregation Daily Active Users, Most Watched Show Building confidence with core SQL <p>| Window Functions | Ranking, Navigation Functions | Top-N, Consecutive Patterns | Top 3 Shows, Consecutive Days | Mastering advanced analytics |</p> <p>| CTEs &amp; Complex Patterns | Heavy Watchers, Content Hierarchy | Retention Analysis, Recursive CTEs | Advanced Patterns | Complex problem solving |</p> <p>| Optimization | Query Performance, Indexing | Efficient DAU, Partitioning | Optimization Challenges | Production-ready SQL |</p>"},{"location":"concepts/SQL/#recommended-learning-sequence","title":"Recommended Learning Sequence","text":"<ol> <li>Start with Wikibooks - Structured exercises to build foundational skills</li> <li>Apply with TopTal - Real-world interview scenarios and business context</li> <li>Practice with Netflix - Industry-specific problems and advanced patterns</li> <li>Master Optimization - Performance tuning and large-scale considerations</li> </ol>"},{"location":"concepts/SQL/#success-metrics","title":"\ud83c\udfaf Success Metrics","text":"<p>Beginner Level:</p> <ul> <li>\u2705 Write basic aggregation queries</li> <li>\u2705 Understand JOIN relationships</li> <li>\u2705 Solve simple Netflix problems</li> </ul> <p>Intermediate Level:</p> <ul> <li>\u2705 Use subqueries effectively</li> <li>\u2705 Handle date-based analytics</li> <li>\u2705 Optimize basic query performance</li> </ul> <p>Advanced Level:</p> <ul> <li>\u2705 Implement complex window functions</li> <li>\u2705 Build multi-step CTEs</li> <li>\u2705 Solve ranking and analytics problems</li> </ul> <p>Expert Level:</p> <ul> <li>\u2705 Design recursive query solutions</li> <li>\u2705 Optimize complex analytical queries</li> <li>\u2705 Handle large-scale data challenges</li> </ul>"},{"location":"concepts/SQL/#additional-resources-cross-references","title":"\ud83d\udcda Additional Resources &amp; Cross-References","text":""},{"location":"concepts/SQL/#related-learning-areas","title":"Related Learning Areas","text":"<ul> <li>Data Modeling - Understanding database design patterns</li> <li>Netflix Data Modeling - Real-world applications</li> <li>Performance Optimization - Query tuning strategies</li> </ul>"},{"location":"concepts/SQL/#practice-exercise-collections","title":"Practice &amp; Exercise Collections","text":"<ul> <li>SQL Exercises from Wikibooks - Comprehensive hands-on exercises by difficulty level</li> <li>TopTal Interview Questions - Streaming/media-focused SQL interview challenges</li> <li>Wikibooks Exercise Directory - Organized practice exercises with detailed solutions</li> <li>TopTal Interview Directory - Advanced problems with business context and performance analysis</li> </ul>"},{"location":"concepts/SQL/#external-resources","title":"External Resources","text":"<ul> <li>PostgreSQL Documentation - Official reference for advanced SQL features</li> <li>SQL Window Functions Cheat Sheet - Quick reference guide</li> <li>Database Indexing Guide - Performance optimization techniques</li> </ul>"},{"location":"concepts/SQL/#navigation-next-steps","title":"\ud83d\udd17 Navigation &amp; Next Steps","text":"<ul> <li>\u2b05\ufe0f Back to Main Concepts</li> <li>\ud83d\udcca Next: Data Modeling Concepts</li> <li>\ud83c\udfe2 Company Interview Prep</li> <li>\ud83c\udfaf Start Learning: Aggregate Functions</li> <li>\ud83d\udcda Practice Exercises: Wikibooks Collection</li> <li>\ud83c\udfaf Interview Practice: TopTal Challenges</li> </ul> <p>This SQL Concepts Overview serves as your central hub for mastering SQL. Start with the basics, progress systematically through each difficulty level, and apply concepts to real Netflix interview problems. Remember: consistent practice with practical examples is the key to SQL mastery!</p>"},{"location":"concepts/SQL/advanced/","title":"Advanced SQL Patterns","text":"<p>This directory contains advanced SQL concepts and patterns.</p>"},{"location":"concepts/SQL/advanced/#files","title":"Files","text":"<ul> <li><code>advanced-sql-patterns.md</code> - Advanced SQL techniques and patterns</li> </ul>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/","title":"Advanced SQL Patterns","text":"<p>This comprehensive guide covers complex SQL patterns including recursive queries, advanced joins, sophisticated data manipulation techniques, and streaming-specific patterns commonly asked in senior-level interviews. The document combines general advanced SQL patterns with Netflix-style streaming platform examples to provide practical context for real-world applications.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p>General Advanced Patterns</p> </li> <li> <p>Netflix Streaming Patterns</p> </li> <li> <p>Data Modeling for Streaming Platforms</p> </li> <li> <p>Learning Objectives</p> </li> <li> <p>Cross-References</p> </li> </ul>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#general-advanced-patterns","title":"General Advanced Patterns","text":"<p>These patterns demonstrate core SQL concepts that apply across industries and use cases.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#1-employee-management-hierarchy","title":"1. Employee Management Hierarchy","text":"<p>Problem:</p> <p>You have an <code>employees</code> table with employee-manager relationships:</p> employee_id name manager_id department salary 1 Alice NULL Engineering 150000 2 Bob 1 Engineering 120000 3 Charlie 1 Engineering 110000 4 David 2 Engineering 100000 5 Eve 2 Engineering 95000 6 Frank 3 Engineering 90000 <p>Find the complete management chain for each employee, showing the full path from top-level manager to the employee.</p> <p>Solution:</p> <pre><code>WITH RECURSIVE management_chain AS (\n    -- Base case: top-level managers\n    SELECT\n        employee_id,\n        name,\n        manager_id,\n        name as management_path,\n        0 as level\n    FROM employees\n    WHERE manager_id IS NULL\n\n    UNION ALL\n\n    -- Recursive case: employees with managers\n    SELECT\n        e.employee_id,\n        e.name,\n        e.manager_id,\n        CONCAT(mc.management_path, ' -&gt; ', e.name) as management_path,\n        mc.level + 1 as level\n    FROM employees e\n    JOIN management_chain mc ON e.manager_id = mc.employee_id\n)\nSELECT\n    employee_id,\n    name,\n    management_path,\n    level\nFROM management_chain\nORDER BY level, employee_id;\n</code></pre> <p>Explanation:</p> <ul> <li>RECURSIVE CTE: Builds the hierarchy by iteratively joining employees to their managers</li> <li>Base case: Top-level managers (NULL manager_id) start the chains</li> <li>Recursive case: Each iteration adds one level deeper in the hierarchy</li> <li>CONCAT: Builds readable management path strings</li> <li>Level tracking: Shows the depth in the organizational hierarchy</li> </ul> <p>This demonstrates recursive CTEs for hierarchical data, essential for organizational charts and dependency trees.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#2-customer-lifetime-value-segmentation","title":"2. Customer Lifetime Value Segmentation","text":"<p>Problem:</p> <p>You have customer transaction data:</p> customer_id transaction_date amount category 1001 2024-01-15 150.00 Electronics 1001 2024-02-20 75.00 Books 1001 2024-03-10 200.00 Electronics 1002 2024-01-05 50.00 Books 1002 2024-02-15 125.00 Electronics 1003 2024-01-20 300.00 Electronics 1003 2024-02-25 150.00 Books <p>Segment customers by lifetime value and purchase frequency, then rank them within each segment.</p> <p>Solution:</p> <pre><code>WITH customer_metrics AS (\n    -- Calculate basic customer metrics\n    SELECT\n        customer_id,\n        COUNT(*) as transaction_count,\n        SUM(amount) as total_spent,\n        AVG(amount) as avg_transaction,\n        MAX(transaction_date) as last_purchase_date,\n        MIN(transaction_date) as first_purchase_date\n    FROM customer_transactions\n    GROUP BY customer_id\n),\ncustomer_segments AS (\n    -- Segment customers based on lifetime value and frequency\n    SELECT\n        customer_id,\n        total_spent,\n        transaction_count,\n        CASE\n            WHEN total_spent &gt;= 500 THEN 'High Value'\n            WHEN total_spent &gt;= 200 THEN 'Medium Value'\n            ELSE 'Low Value'\n        END as value_segment,\n        CASE\n            WHEN transaction_count &gt;= 10 THEN 'Frequent'\n            WHEN transaction_count &gt;= 5 THEN 'Regular'\n            ELSE 'Occasional'\n        END as frequency_segment\n    FROM customer_metrics\n),\nranked_customers AS (\n    -- Rank customers within each segment combination\n    SELECT\n        cs.*,\n        ROW_NUMBER() OVER (\n            PARTITION BY value_segment, frequency_segment\n            ORDER BY total_spent DESC, transaction_count DESC\n        ) as segment_rank,\n        DENSE_RANK() OVER (ORDER BY total_spent DESC) as overall_rank\n    FROM customer_segments cs\n)\nSELECT\n    customer_id,\n    total_spent,\n    transaction_count,\n    value_segment,\n    frequency_segment,\n    segment_rank,\n    overall_rank\nFROM ranked_customers\nORDER BY total_spent DESC, transaction_count DESC;\n</code></pre> <p>Explanation:</p> <ul> <li>Multiple CTEs: Break down complex analysis into logical steps</li> <li>CASE statements: Create meaningful customer segments</li> <li>ROW_NUMBER: Rank within specific segments</li> <li>DENSE_RANK: Overall ranking without gaps</li> <li>PARTITION BY: Segment-specific rankings</li> </ul> <p>This pattern combines aggregation, conditional logic, and ranking functions for comprehensive customer analysis.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#3-content-recommendation-analysis","title":"3. Content Recommendation Analysis","text":"<p>Problem:</p> <p>You have user viewing history and content metadata:</p> user_id content_id watch_date watch_duration content_type rating 1 100 2024-01-01 45 Movie 4 1 101 2024-01-02 30 Series 5 1 102 2024-01-03 60 Movie 3 2 100 2024-01-01 50 Movie 5 2 103 2024-01-02 25 Series 4 <p>Find users with similar viewing patterns using a collaborative filtering approach.</p> <p>Solution:</p> <pre><code>WITH user_content_matrix AS (\n    -- Create user-content interaction matrix\n    SELECT\n        user_id,\n        content_id,\n        watch_duration,\n        rating,\n        content_type\n    FROM viewing_history\n    WHERE watch_date &gt;= CURRENT_DATE - INTERVAL '30 days'\n),\nuser_similarity AS (\n    -- Calculate similarity between users based on content overlap\n    SELECT\n        a.user_id as user_a,\n        b.user_id as user_b,\n        COUNT(DISTINCT a.content_id) as common_content,\n        CORR(a.rating, b.rating) as rating_correlation,\n        AVG(ABS(a.watch_duration - b.watch_duration)) as avg_duration_diff\n    FROM user_content_matrix a\n    JOIN user_content_matrix b ON a.content_id = b.content_id\n        AND a.user_id &lt; b.user_id  -- Avoid duplicate pairs\n    GROUP BY a.user_id, b.user_id\n    HAVING COUNT(DISTINCT a.content_id) &gt;= 3  -- Minimum overlap\n),\nsimilar_users AS (\n    -- Rank similar users by correlation and overlap\n    SELECT\n        user_a,\n        user_b,\n        common_content,\n        rating_correlation,\n        ROW_NUMBER() OVER (\n            PARTITION BY user_a\n            ORDER BY rating_correlation DESC, common_content DESC\n        ) as similarity_rank\n    FROM user_similarity\n    WHERE rating_correlation &gt; 0.5  -- Strong positive correlation\n)\nSELECT\n    su.user_a,\n    su.user_b,\n    su.common_content,\n    ROUND(su.rating_correlation::numeric, 3) as correlation,\n    su.similarity_rank\nFROM similar_users su\nWHERE similarity_rank &lt;= 5  -- Top 5 similar users\nORDER BY su.user_a, su.similarity_rank;\n</code></pre> <p>Explanation:</p> <ul> <li>Matrix-style CTE: Creates user-content interaction data</li> <li>Self-join: Compares users based on shared content</li> <li>CORR function: Calculates correlation between user ratings</li> <li>HAVING clause: Filters for meaningful overlaps</li> <li>ROW_NUMBER: Ranks similar users for each target user</li> </ul> <p>This demonstrates advanced analytical patterns for recommendation systems.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#4-financial-fraud-detection","title":"4. Financial Fraud Detection","text":"<p>Problem:</p> <p>You have transaction data with potential fraud patterns:</p> transaction_id user_id transaction_date amount merchant_category location 1001 1 2024-01-01 10:00 50.00 Grocery New York 1002 1 2024-01-01 10:30 25.00 Grocery New York 1003 1 2024-01-01 11:00 5000.00 Electronics New York 1004 1 2024-01-01 11:30 50.00 Grocery New York <p>Identify suspicious transaction patterns using statistical analysis.</p> <p>Solution:</p> <pre><code>WITH user_transaction_stats AS (\n    -- Calculate user-specific transaction statistics\n    SELECT\n        user_id,\n        AVG(amount) as avg_transaction,\n        STDDEV(amount) as stddev_transaction,\n        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY amount) as p95_amount,\n        COUNT(*) as transaction_count,\n        AVG(EXTRACT(EPOCH FROM (transaction_date - LAG(transaction_date) OVER\n            (PARTITION BY user_id ORDER BY transaction_date)))) / 60 as avg_time_between_minutes\n    FROM transactions\n    WHERE transaction_date &gt;= CURRENT_DATE - INTERVAL '30 days'\n    GROUP BY user_id\n),\nsuspicious_transactions AS (\n    -- Flag potentially suspicious transactions\n    SELECT\n        t.transaction_id,\n        t.user_id,\n        t.transaction_date,\n        t.amount,\n        t.merchant_category,\n        -- Calculate z-score for amount\n        (t.amount - uts.avg_transaction) / NULLIF(uts.stddev_transaction, 0) as amount_zscore,\n        -- Check if amount is unusually high\n        CASE WHEN t.amount &gt; uts.p95_amount THEN 1 ELSE 0 END as high_amount_flag,\n        -- Check for rapid succession\n        CASE WHEN EXTRACT(EPOCH FROM (t.transaction_date - LAG(t.transaction_date) OVER\n            (PARTITION BY t.user_id ORDER BY t.transaction_date))) / 60 &lt; 5\n             THEN 1 ELSE 0 END as rapid_succession_flag,\n        -- Risk score calculation\n        (CASE WHEN t.amount &gt; uts.p95_amount THEN 2 ELSE 0 END +\n         CASE WHEN EXTRACT(EPOCH FROM (t.transaction_date - LAG(t.transaction_date) OVER\n             (PARTITION BY t.user_id ORDER BY t.transaction_date))) / 60 &lt; 5\n              THEN 1 ELSE 0 END) as risk_score\n    FROM transactions t\n    JOIN user_transaction_stats uts ON t.user_id = uts.user_id\n),\nflagged_transactions AS (\n    -- Final ranking of suspicious transactions\n    SELECT\n        *,\n        ROW_NUMBER() OVER (ORDER BY risk_score DESC, amount_zscore DESC) as suspicion_rank\n    FROM suspicious_transactions\n    WHERE risk_score &gt;= 2 OR amount_zscore &gt; 3\n)\nSELECT\n    transaction_id,\n    user_id,\n    transaction_date,\n    amount,\n    merchant_category,\n    ROUND(amount_zscore::numeric, 2) as amount_zscore,\n    risk_score,\n    suspicion_rank\nFROM flagged_transactions\nWHERE suspicion_rank &lt;= 10  -- Top 10 most suspicious\nORDER BY suspicion_rank;\n</code></pre> <p>Explanation:</p> <ul> <li>Statistical analysis: Uses standard deviation and percentiles for anomaly detection</li> <li>Window functions: LAG for time-based pattern analysis</li> <li>Z-score calculation: Statistical measure for unusual amounts</li> <li>Risk scoring: Combines multiple suspicious indicators</li> <li>PERCENTILE_CONT: Advanced PostgreSQL statistical function</li> </ul> <p>This pattern shows advanced analytical SQL for fraud detection and risk analysis.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#netflix-streaming-patterns","title":"Netflix Streaming Patterns","text":"<p>These patterns are specifically tailored for streaming platforms like Netflix, focusing on user engagement, content performance, and recommendation systems.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#1-daily-active-users-dau","title":"1. Daily Active Users (DAU)","text":"<p>Problem:</p> <p>Table: <code>watch_events(user_id, show_id, event_ts, watch_time)</code></p> <p>Write a query to find the number of unique active users per day.</p> <p>Solution:</p> <pre><code>SELECT\n  DATE(event_ts) AS watch_date,\n  COUNT(DISTINCT user_id) AS daily_active_users\nFROM watch_events\nGROUP BY DATE(event_ts)\nORDER BY watch_date;\n</code></pre> <p>Explanation:</p> <ul> <li>DATE() function: Extracts date from timestamp for daily aggregation</li> <li>COUNT(DISTINCT): Ensures each user is counted only once per day</li> <li>GROUP BY: Groups events by date for per-day calculations</li> </ul> <p>This is fundamental for measuring platform engagement and growth metrics.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#2-most-watched-show-per-day","title":"2. Most Watched Show per Day","text":"<p>Problem:</p> <p>Table: <code>watch_history(user_id, show_id, watch_date, watch_time_minutes)</code></p> <p>Find the most watched show per day based on total minutes.</p> <p>Solution:</p> <pre><code>SELECT watch_date, show_id\nFROM (\n  SELECT\n    watch_date,\n    show_id,\n    SUM(watch_time_minutes) AS total_watch_time,\n    RANK() OVER (PARTITION BY watch_date ORDER BY SUM(watch_time_minutes) DESC) AS rnk\n  FROM watch_history\n  GROUP BY watch_date, show_id\n) t\nWHERE rnk = 1;\n</code></pre> <p>Explanation:</p> <ul> <li>RANK() window function: Ranks shows within each day by total watch time</li> <li>PARTITION BY: Resets ranking for each day</li> <li>Subquery: Allows filtering by rank after window function calculation</li> </ul> <p>Critical for content performance analysis and trending content identification.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#3-day-1-retention","title":"3. Day-1 Retention","text":"<p>Problem:</p> <p>From <code>user_signup(user_id, signup_date)</code> and <code>watch_events(user_id, event_ts)</code>, calculate Day-1 retention (users who signed up on Day X and came back on Day X+1).</p> <p>Solution:</p> <pre><code>SELECT\n  s.signup_date,\n  COUNT(DISTINCT a.user_id) * 100.0 / COUNT(DISTINCT s.user_id) AS day1_retention\nFROM user_signup s\nLEFT JOIN watch_events a\n  ON s.user_id = a.user_id\n  AND DATE(a.event_ts) = DATE_ADD(s.signup_date, INTERVAL 1 DAY)\nGROUP BY s.signup_date;\n</code></pre> <p>Explanation:</p> <ul> <li>LEFT JOIN: Includes all signups, even those without next-day activity</li> <li>DATE_ADD: Calculates the next day after signup</li> <li>Percentage calculation: Numerator (returned users) divided by denominator (total signups)</li> </ul> <p>Essential metric for measuring user onboarding success and engagement.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#4-top-3-shows-per-region","title":"4. Top 3 Shows per Region","text":"<p>Problem:</p> <p>Table: <code>viewership(user_id, show_id, region, watch_time)</code></p> <p>Find the top 3 most watched shows per region by total minutes.</p> <p>Solution:</p> <pre><code>SELECT region, show_id, total_watch_time\nFROM (\n  SELECT\n    region,\n    show_id,\n    SUM(watch_time) AS total_watch_time,\n    DENSE_RANK() OVER (PARTITION BY region ORDER BY SUM(watch_time) DESC) AS rnk\n  FROM viewership\n  GROUP BY region, show_id\n) t\nWHERE rnk &lt;= 3;\n</code></pre> <p>Explanation:</p> <ul> <li>DENSE_RANK(): Provides consecutive ranking without gaps</li> <li>PARTITION BY region: Ranks within each geographic region separately</li> <li>Top-N pattern: Filters for rank &lt;= 3 to get top performers</li> </ul> <p>Critical for regional content strategy and localization decisions.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#5-consecutive-days-watching","title":"5. Consecutive Days Watching","text":"<p>Problem:</p> <p>From <code>user_activity(user_id, activity_date)</code>, find users who have watched content for 3 consecutive days.</p> <p>Solution:</p> <pre><code>SELECT DISTINCT user_id\nFROM (\n  SELECT\n    user_id,\n    activity_date,\n    LAG(activity_date,1) OVER (PARTITION BY user_id ORDER BY activity_date) AS prev_day,\n    LAG(activity_date,2) OVER (PARTITION BY user_id ORDER BY activity_date) AS prev2_day\n  FROM user_activity\n) t\nWHERE DATEDIFF(activity_date, prev_day) = 1\n  AND DATEDIFF(prev_day, prev2_day) = 1;\n</code></pre> <p>Explanation:</p> <ul> <li>LAG() window function: Accesses previous rows' values</li> <li>DATEDIFF: Calculates difference between consecutive dates</li> <li>Multiple LAG calls: Checks for 3-day sequences</li> </ul> <p>Identifies highly engaged users who form watching habits.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#6-percentage-of-users-watching-originals","title":"6. Percentage of Users Watching Originals","text":"<p>Problem:</p> <p><code>shows(show_id, type)</code> where type = 'original' or 'licensed' <code>watch_history(user_id, show_id)</code></p> <p>Find the percentage of users who watched at least one Netflix Original.</p> <p>Solution:</p> <pre><code>SELECT\n  COUNT(DISTINCT CASE WHEN s.type = 'original' THEN w.user_id END) * 100.0 /\n  COUNT(DISTINCT w.user_id) AS pct_original_watchers\nFROM watch_history w\nJOIN shows s ON w.show_id = s.show_id;\n</code></pre> <p>Explanation:</p> <ul> <li>CASE in COUNT DISTINCT: Counts users who watched originals</li> <li>Conditional aggregation: Separates original vs licensed content consumption</li> <li>Percentage calculation: Ratio of original watchers to total watchers</li> </ul> <p>Key metric for measuring success of Netflix's original content strategy.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#7-power-users","title":"7. Power Users","text":"<p>Problem:</p> <p>Find the top 1% of users by total watch time in <code>watch_history</code>.</p> <p>Solution:</p> <pre><code>WITH ranked AS (\n  SELECT\n    user_id,\n    SUM(watch_time_minutes) AS total_time,\n    PERCENT_RANK() OVER (ORDER BY SUM(watch_time_minutes) DESC) AS pct_rank\n  FROM watch_history\n  GROUP BY user_id\n)\nSELECT user_id, total_time\nFROM ranked\nWHERE pct_rank &lt;= 0.01;\n</code></pre> <p>Explanation:</p> <ul> <li>PERCENT_RANK(): Calculates percentile rank (0.01 = top 1%)</li> <li>CTE structure: Breaks down aggregation and ranking</li> <li>HAVING equivalent: Filters for top percentile</li> </ul> <p>Identifies most valuable users for targeted retention and premium feature testing.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#8-consecutive-days-streak-7-day-binge","title":"8. Consecutive Days Streak (7-day binge)","text":"<p>Problem:</p> <p>Table: <code>watch_events(user_id, event_ts)</code></p> <p>Find users who have watched content for 7 consecutive days.</p> <p>Solution:</p> <pre><code>WITH daily_watch AS (\n  SELECT DISTINCT user_id, DATE(event_ts) AS watch_date\n  FROM watch_events\n),\nwith_lags AS (\n  SELECT\n    user_id,\n    watch_date,\n    ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY watch_date) -\n    ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY watch_date) AS grp\n  FROM daily_watch\n)\nSELECT user_id\nFROM (\n  SELECT user_id, grp, COUNT(*) AS streak_length\n  FROM with_lags\n  GROUP BY user_id, grp\n) t\nWHERE streak_length &gt;= 7;\n</code></pre> <p>Explanation:</p> <ul> <li>ROW_NUMBER trick: Creates group identifiers for consecutive sequences</li> <li>Date-ordered ROW_NUMBER: Generates sequence numbers</li> <li>Group counting: Identifies length of consecutive day streaks</li> </ul> <p>Advanced pattern for identifying binge-watching behavior and engagement patterns.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#9-recommendation-acceptance-rate","title":"9. Recommendation Acceptance Rate","text":"<p>Problem:</p> <p>Tables:</p> <ul> <li><code>recommendations(user_id, show_id, recommendation_ts)</code></li> <li><code>watch_events(user_id, show_id, event_ts)</code></li> </ul> <p>Find the percentage of recommended shows that were actually watched.</p> <p>Solution:</p> <pre><code>SELECT\n  COUNT(DISTINCT CASE WHEN w.user_id IS NOT NULL THEN r.user_id END) * 100.0 /\n  COUNT(DISTINCT r.user_id) AS acceptance_rate_pct\nFROM recommendations r\nLEFT JOIN watch_events w\n  ON r.user_id = w.user_id\n  AND r.show_id = w.show_id;\n</code></pre> <p>Explanation:</p> <ul> <li>LEFT JOIN: Includes all recommendations, even unwatched ones</li> <li>Conditional COUNT DISTINCT: Counts accepted recommendations</li> <li>Acceptance rate: Measures recommendation algorithm effectiveness</li> </ul> <p>Critical for evaluating recommendation system performance and personalization.</p>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#data-modeling-for-streaming-platforms","title":"Data Modeling for Streaming Platforms","text":""},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#watch-history-modeling","title":"Watch History Modeling","text":"<p>For efficient analytics at Netflix scale:</p> <p>Fact Table: <code>fact_watch_events(user_id, show_id, region, watch_time, event_ts)</code> Dimension Tables:</p> <ul> <li><code>dim_users(user_id, signup_date, country, device)</code></li> <li><code>dim_shows(show_id, title, genre, is_original, release_date)</code></li> </ul> <p>Partitioning Strategy:</p> <ul> <li>Partition <code>fact_watch_events</code> by <code>region</code> and <code>date(event_ts)</code></li> <li>Enables fast queries for regional analytics and daily reporting</li> </ul>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#content-recommendation-modeling","title":"Content Recommendation Modeling","text":"<p>Tables:</p> <ul> <li><code>fact_recommendations(user_id, show_id, recommendation_ts, rank_position)</code></li> <li><code>fact_watch_events</code> (to track acceptance)</li> </ul> <p>Benefits:</p> <ul> <li>A/B testing of recommendation algorithms</li> <li>Measurement of recommendation acceptance rates</li> <li>Optimization of recommendation ranking algorithms</li> </ul>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#data-quality-considerations","title":"Data Quality Considerations","text":"<p>Validation Rules:</p> <ul> <li>No negative watch_time values</li> <li>User_id must exist in dim_users</li> <li>Show_id must exist in dim_shows</li> </ul> <p>Monitoring:</p> <ul> <li>Data freshness checks (daily partitions arrive before SLA)</li> <li>Deduplication (unique event_id for each watch event)</li> <li>Schema evolution tracking for recommendation algorithm changes</li> </ul>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#learning-objectives","title":"Learning Objectives","text":""},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#general-sql-patterns","title":"General SQL Patterns","text":"<ul> <li>Recursive CTEs: Building hierarchical queries and tree structures</li> <li>Complex multi-CTE queries: Breaking down complex analysis into manageable steps</li> <li>Advanced window functions: Statistical analysis and pattern detection</li> <li>Correlation analysis: Finding relationships between datasets</li> <li>Risk scoring algorithms: Multi-factor analysis and ranking</li> </ul>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#streaming-specific-patterns","title":"Streaming-Specific Patterns","text":"<ul> <li>Retention analysis: Day-N retention and cohort analysis</li> <li>Ranking functions: Top-N queries with regional partitioning</li> <li>Consecutive sequence detection: Binge-watching and streak analysis</li> <li>Recommendation system evaluation: Acceptance rate and performance metrics</li> <li>Percentile analysis: Power user identification and segmentation</li> </ul>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#data-engineering-skills","title":"Data Engineering Skills","text":"<ul> <li>Data modeling: Star schema design for analytics</li> <li>Partitioning strategies: Optimizing for query performance</li> <li>Data quality: Validation, monitoring, and lineage tracking</li> <li>Streaming platform metrics: DAU, engagement, and content performance</li> </ul>"},{"location":"concepts/SQL/advanced/advanced-sql-patterns/#cross-references","title":"Cross-References","text":"<ul> <li>Window Functions Overview</li> <li>CTEs and Advanced Patterns</li> <li>SQL Optimization Challenges</li> <li>Data Modeling</li> <li>SQL Learning Path</li> </ul> <p>Navigate back to SQL Concepts | Concepts Overview</p>"},{"location":"concepts/SQL/aggregation/","title":"SQL Aggregation","text":"<p>This directory contains SQL aggregation concepts and examples.</p>"},{"location":"concepts/SQL/aggregation/#files","title":"Files","text":"<ul> <li><code>aggregate-functions.md</code> - Basic aggregation functions and usage</li> <li><code>monthly-ratings.md</code> - Monthly aggregation and rating calculations</li> </ul>"},{"location":"concepts/SQL/aggregation/aggregate-functions/","title":"SQL Aggregate Functions: Comprehensive Guide","text":""},{"location":"concepts/SQL/aggregation/aggregate-functions/#overview","title":"Overview","text":""},{"location":"concepts/SQL/aggregation/aggregate-functions/#definition","title":"\u2705 Definition","text":"<p>Aggregate functions perform a calculation on a set of values and return a single value. They are typically used with <code>GROUP BY</code> to summarize data.</p> <p>These functions collapse rows into a summary, unlike window functions.</p>"},{"location":"concepts/SQL/aggregation/aggregate-functions/#common-aggregate-functions","title":"\ud83d\udcd8 Common Aggregate Functions","text":"Function Description <code>SUM()</code> Total of numeric values <code>AVG()</code> Average of numeric values <code>COUNT()</code> Number of rows <code>MAX()</code> Maximum value <code>MIN()</code> Minimum value"},{"location":"concepts/SQL/aggregation/aggregate-functions/#syntax","title":"\ud83e\udde9 Syntax","text":"<pre><code>SELECT AGG_FUNCTION(column)\nFROM table\n[WHERE conditions]\n[GROUP BY column];\n</code></pre>"},{"location":"concepts/SQL/aggregation/aggregate-functions/#examples","title":"\ud83d\udcd8 Examples","text":""},{"location":"concepts/SQL/aggregation/aggregate-functions/#1-total-salary-of-all-employees","title":"1. Total salary of all employees","text":"<pre><code>SELECT SUM(salary) AS total_salary\nFROM employees;\n</code></pre>"},{"location":"concepts/SQL/aggregation/aggregate-functions/#2-average-salary-by-department","title":"2. Average salary by department","text":"<pre><code>SELECT department_id, AVG(salary) AS avg_salary\nFROM employees\nGROUP BY department_id;\n</code></pre>"},{"location":"concepts/SQL/aggregation/aggregate-functions/#3-count-of-employees-per-job-title","title":"3. Count of employees per job title","text":"<pre><code>SELECT job_title, COUNT(*) AS num_employees\nFROM employees\nGROUP BY job_title;\n</code></pre>"},{"location":"concepts/SQL/aggregation/aggregate-functions/#4-highest-and-lowest-salary-in-company","title":"4. Highest and lowest salary in company","text":"<pre><code>SELECT \n    MAX(salary) AS highest_salary,\n    MIN(salary) AS lowest_salary\nFROM employees;\n</code></pre>"},{"location":"concepts/SQL/aggregation/aggregate-functions/#comparison-with-window-function","title":"\u2705 Comparison with Window Function","text":"Feature Aggregate Function Window Function Row Output Collapses to fewer rows Maintains all original rows OVER() clause required? \u274c No \u2705 Yes Use Case Summary per group/table Per-row contextual metrics PostgreSQL Support All major aggregates All aggregates + window-specific Performance Fast for grouped data Optimized for analytical queries"},{"location":"concepts/SQL/aggregation/aggregate-functions/#postgresql-windowed-aggregates","title":"\ud83d\ude80 PostgreSQL Windowed Aggregates","text":"<p>Aggregate functions can be used as window functions in PostgreSQL by adding an <code>OVER()</code> clause. This creates powerful analytical capabilities:</p>"},{"location":"concepts/SQL/aggregation/aggregate-functions/#example-employee-analysis-with-windowed-aggregates","title":"Example Employee Analysis with Windowed Aggregates","text":"<pre><code>SELECT\n    employee_id,\n    department,\n    salary,\n    -- Traditional aggregate (summary)\n    AVG(salary) OVER (PARTITION BY department) AS dept_avg_salary,\n\n    -- Running calculations\n    SUM(salary) OVER (ORDER BY salary ROWS UNBOUNDED PRECEDING) AS running_total,\n\n    -- Moving averages\n    AVG(salary) OVER (ORDER BY hire_date ROWS 2 PRECEDING) AS recent_avg,\n\n    -- Statistical functions\n    STDDEV(salary) OVER (PARTITION BY department) AS dept_salary_stddev,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY salary) OVER (PARTITION BY department) AS dept_median_salary\nFROM employees\nORDER BY department, salary DESC;\n</code></pre>"},{"location":"concepts/SQL/aggregation/aggregate-functions/#key-postgresql-windowed-aggregate-features","title":"Key PostgreSQL Windowed Aggregate Features","text":"<ul> <li>All Standard Aggregates: <code>SUM()</code>, <code>AVG()</code>, <code>COUNT()</code>, <code>MIN()</code>, <code>MAX()</code></li> <li>Statistical Functions: <code>STDDEV()</code>, <code>VARIANCE()</code>, <code>CORR()</code></li> <li>Ordered-Set Aggregates: <code>PERCENTILE_CONT()</code>, <code>PERCENTILE_DISC()</code></li> <li>Advanced Frames: <code>ROWS</code>, <code>RANGE</code>, <code>GROUPS</code> frame specifications</li> <li>Named Windows: Reusable window specifications</li> </ul>"},{"location":"concepts/SQL/aggregation/aggregate-functions/#postgresql-window-function-resources","title":"\ud83d\udcda PostgreSQL Window Function Resources","text":"<p>For comprehensive PostgreSQL window function coverage, see:</p> <ul> <li><code>concepts/SQL/window-functions/window-functions-overview.md</code> - Complete PostgreSQL window functions guide</li> <li><code>concepts/SQL/window-functions/postgresql-ranking-functions.md</code> - Ranking functions with examples</li> <li><code>concepts/SQL/window-functions/postgresql-navigation-functions.md</code> - Navigation functions (LAG, LEAD, etc.)</li> <li><code>concepts/SQL/window-functions/postgresql-advanced-concepts.md</code> - Advanced concepts and performance</li> <li><code>concepts/SQL/window-functions/postgresql-interview-examples.md</code> - Interview-ready examples</li> </ul>"},{"location":"concepts/SQL/aggregation/monthly-ratings/","title":"SQL Question 2: Analyzing Monthly Average Ratings of Airbnb Property Listings","text":""},{"location":"concepts/SQL/aggregation/monthly-ratings/#problem-description","title":"Problem Description","text":"<p>Given the <code>reviews</code> table with columns: <code>review_id</code>, <code>user_id</code>, <code>submit_date</code>, <code>listing_id</code>, <code>stars</code>, write a SQL query to get the average rating of each Airbnb property listing per month.</p> <ul> <li>The <code>submit_date</code> column represents when the review was submitted</li> <li>The <code>listing_id</code> column represents the unique ID of the Airbnb property</li> <li>The <code>stars</code> column represents the rating given by the user (1 is the lowest, 5 is the highest rating)</li> </ul>"},{"location":"concepts/SQL/aggregation/monthly-ratings/#input-data","title":"Input Data","text":""},{"location":"concepts/SQL/aggregation/monthly-ratings/#reviews-table","title":"reviews Table","text":"review_id user_id submit_date listing_id stars 6171 123 01/02/2022 00:00:00 50001 4 7802 265 01/15/2022 00:00:00 69852 4 5293 362 01/22/2022 00:00:00 50001 3 6352 192 02/05/2022 00:00:00 69852 3 4517 981 02/10/2022 00:00:00 69852 2"},{"location":"concepts/SQL/aggregation/monthly-ratings/#solution","title":"Solution","text":"<pre><code>-- Extract month and listing_id, calculate average stars per month\nSELECT\n    EXTRACT(MONTH FROM submit_date) AS mth,\n    listing_id,\n    AVG(stars) AS avg_stars\nFROM\n    reviews\nGROUP BY\n    mth,\n    listing_id\nORDER BY\n    listing_id,\n    mth;\n</code></pre>"},{"location":"concepts/SQL/aggregation/monthly-ratings/#explanation","title":"Explanation","text":"<p>This SQL query performs the following operations:</p> <ol> <li>Grouping: Groups the data by month (extracted from <code>submit_date</code>) and <code>listing_id</code></li> <li>Aggregation: For each group, calculates the average <code>stars</code> rating using the <code>AVG()</code> function</li> <li>Month Extraction: Uses <code>EXTRACT(MONTH FROM submit_date)</code> to get the month number from the timestamp</li> <li>Ordering: Orders the results by <code>listing_id</code> first, then by month (<code>mth</code>)</li> </ol> <p>This approach ensures we get one row per listing per month with the average rating for that period.</p>"},{"location":"concepts/SQL/aggregation/monthly-ratings/#output","title":"Output","text":"mth listing_id avg_stars 1 50001 3.50 1 69852 4.00 2 69852 2.50"},{"location":"concepts/SQL/cte/","title":"Common Table Expressions (CTE)","text":"<p>This directory contains CTE concepts, comparisons, and examples.</p>"},{"location":"concepts/SQL/cte/#files","title":"Files","text":"<ul> <li><code>cte-vs-window-comparison.md</code> - Comparison between CTEs and window functions</li> <li><code>customer-sales-analysis.md</code> - Customer sales analysis using CTEs</li> <li><code>vacant-days-detailed.md</code> - Detailed vacant days calculation with CTEs</li> </ul>"},{"location":"concepts/SQL/cte/cte-vs-window-comparison/","title":"CTE vs Window Functions: Performance and Readability Comparison","text":""},{"location":"concepts/SQL/cte/cte-vs-window-comparison/#introduction","title":"Introduction","text":"<p>Both of your CTE-based SQL queries are correct and will return the same result \u2014 customers whose total sales are greater than the average of all customer totals.</p> <p>But they differ in style, performance, and readability.</p>"},{"location":"concepts/SQL/cte/cte-vs-window-comparison/#option-1-using-two-ctes-with-scalar-subquery-logic","title":"\u2705 Option 1 \u2014 Using two CTEs with scalar subquery logic","text":"<pre><code>WITH customer_totals AS (\n    SELECT customer_id, SUM(amount) AS total_sales\n    FROM sales\n    GROUP BY customer_id\n),\naverage_total AS (\n    SELECT AVG(total_sales) AS avg_sales\n    FROM customer_totals\n)\nSELECT ct.*\nFROM customer_totals ct, average_total at\nWHERE ct.total_sales &gt; at.avg_sales;\n</code></pre> <p>\ud83d\udd0d Pros</p> <ul> <li>Very clear and modular: one CTE for totals, one for the average.</li> <li>Easy to understand in steps.</li> <li>Works well on most SQL engines, even older ones.</li> </ul> <p>\ud83e\udde0 How it works</p> <ul> <li>Step 1: Compute <code>customer_id \u2192 total_sales</code>.</li> <li>Step 2: Compute a single row with average.</li> <li>Step 3: Join both (effectively applying the average as a filter).</li> </ul> <p>\u26a0\ufe0f Caveat</p> <ul> <li>The second CTE (<code>average_total</code>) is a scalar subquery \u2014 computed once.</li> <li>In large datasets, the optimizer will usually inline it, but not always efficiently.</li> </ul>"},{"location":"concepts/SQL/cte/cte-vs-window-comparison/#option-2-using-window-function","title":"\u2705 Option 2 \u2014 Using window function","text":"<pre><code>WITH customer_totals AS (\n  SELECT customer_id,\n         SUM(amount) AS total_sales\n  FROM sales\n  GROUP BY customer_id\n),\nwith_avg AS (\n  SELECT *,\n         AVG(total_sales) OVER () AS avg_sales\n  FROM customer_totals\n)\nSELECT customer_id, total_sales\nFROM with_avg\nWHERE total_sales &gt; avg_sales;\n</code></pre> <p>\ud83d\udd0d Pros</p> <ul> <li>Uses window function to calculate the average inline, over all rows.</li> <li>No joins \u2014 simpler physical query plan.</li> <li>Very efficient for large datasets and modern engines (PostgreSQL, SQL Server, etc.).</li> </ul> <p>*\ud83e\udde0 How it works</p> <ul> <li><code>AVG(total_sales) OVER ()</code> computes a rolling (same) average across all rows.</li> <li>Each row gets that value, and we filter with <code>WHERE</code>.</li> </ul> <p>\u26a0\ufe0f Caveat</p> <ul> <li>Slightly harder to read if you're not familiar with window functions.</li> <li>May not be supported in older SQL engines (e.g., MySQL 5.x).</li> </ul>"},{"location":"concepts/SQL/cte/cte-vs-window-comparison/#verdict","title":"\u2705 Verdict","text":"Feature Option 1 (CTE + scalar avg) Option 2 (window avg) Simplicity (conceptual) \u2705 Easier for beginners Slightly more advanced Performance (modern DB) \u26a0\ufe0f Good, but depends on engine \u2705 Usually faster, no join Readability \u2705 Very readable \u2705 Clean, but needs window knowledge Portability \u2705 More widely supported \u26a0\ufe0f Some older DBs lack <code>OVER</code>"},{"location":"concepts/SQL/cte/cte-vs-window-comparison/#recommendation","title":"\ud83d\udc49 Recommendation","text":"<ul> <li>Use Option 2 if your DB engine supports window functions \u2014 it's efficient and clean.</li> <li>Use Option 1 if you're targeting compatibility or working with developers new to SQL.</li> </ul>"},{"location":"concepts/SQL/cte/cte-vs-window-comparison/#postgresql-specific-enhancements","title":"\ud83d\ude80 PostgreSQL-Specific Enhancements","text":"<p>For PostgreSQL users, Option 2 can be further optimized using named windows and advanced frame specifications:</p> <pre><code>WITH customer_totals AS (\n    SELECT\n        customer_id,\n        department,\n        SUM(amount) AS total_sales,\n        COUNT(*) AS order_count\n    FROM sales\n    GROUP BY customer_id, department\n)\nSELECT\n    customer_id,\n    department,\n    total_sales,\n    order_count,\n    -- Multiple window functions with named windows\n    overall_avg_sales,\n    dept_avg_sales,\n    ROUND((total_sales - overall_avg_sales) / overall_avg_sales * 100, 2) AS vs_global_avg_pct,\n    ROUND((total_sales - dept_avg_sales) / dept_avg_sales * 100, 2) AS vs_dept_avg_pct,\n    -- Performance metrics\n    sales_rank,\n    performance_percentile\nFROM (\n    SELECT *,\n        -- Named windows for PostgreSQL optimization\n        AVG(total_sales) OVER global_window AS overall_avg_sales,\n        AVG(total_sales) OVER dept_window AS dept_avg_sales,\n        ROW_NUMBER() OVER global_window AS sales_rank,\n        ROUND(PERCENT_RANK() OVER global_window * 100, 1) AS performance_percentile\n    FROM customer_totals\n    WINDOW\n        global_window AS (),\n        dept_window AS (PARTITION BY department)\n) analysis\nWHERE total_sales &gt; overall_avg_sales\nORDER BY total_sales DESC;\n</code></pre> <p>PostgreSQL Advantages::</p> <ul> <li>Named Windows: Avoid repetition and improve readability</li> <li>Performance: Optimized execution plans for window functions</li> <li>Advanced Functions: Access to PostgreSQL-specific window functions</li> <li>Memory Efficiency: Better memory management for large datasets</li> </ul>"},{"location":"concepts/SQL/cte/cte-vs-window-comparison/#performance-comparison-postgresql","title":"\ud83d\udcca Performance Comparison (PostgreSQL)","text":"Metric CTE + Scalar Subquery Window Function Named Windows Query Complexity Medium Low Low Memory Usage Higher Optimized Optimized Execution Time Variable Usually faster Fastest Readability High Medium-High High PostgreSQL Optimization Good Excellent Excellent"},{"location":"concepts/SQL/cte/cte-vs-window-comparison/#related-postgresql-window-function-resources","title":"\ud83d\udd17 Related PostgreSQL Window Function Resources","text":"<p>For comprehensive PostgreSQL window function coverage, see:</p> <ul> <li><code>concepts/SQL/window-functions/window-functions-overview.md</code> - Complete PostgreSQL window functions guide</li> <li><code>concepts/SQL/window-functions/postgresql-ranking-functions.md</code> - Ranking functions with examples</li> <li><code>concepts/SQL/window-functions/postgresql-navigation-functions.md</code> - Navigation functions (LAG, LEAD, etc.)</li> <li><code>concepts/SQL/window-functions/postgresql-advanced-concepts.md</code> - Advanced concepts and performance</li> <li><code>concepts/SQL/window-functions/postgresql-interview-examples.md</code> - Interview-ready examples</li> </ul>"},{"location":"concepts/SQL/cte/cte-vs-window-comparison/#postgresql-performance-tips","title":"\ud83d\udca1 PostgreSQL Performance Tips","text":"<ol> <li>Index Strategy: Create indexes on <code>PARTITION BY</code> and <code>ORDER BY</code> columns</li> <li>Named Windows: Use for queries with multiple window functions</li> <li>Frame Control: Be explicit about frames to optimize memory usage</li> <li>Parallel Processing: PostgreSQL can parallelize window functions</li> <li>Memory Settings: Adjust <code>work_mem</code> for large result sets</li> </ol> <p>Let me know which DB engine you're using if you'd like a performance comparison or tuning tip!</p>"},{"location":"concepts/SQL/cte/customer-sales-analysis/","title":"Customer Sales Analysis Using CTE","text":""},{"location":"concepts/SQL/cte/customer-sales-analysis/#scenario-use-cte-to-avoid-repeating-complex-logic-non-recursive","title":"\u2705 Scenario Use CTE to avoid repeating complex logic (non-recursive)","text":"<p>Let\u2019s say you have a <code>sales</code> table:</p> sale_id customer_id amount sale_date 1 100 500 2024-01-01 2 101 700 2024-01-03 3 100 200 2024-02-01 4 102 300 2024-02-10 5 100 1000 2024-03-01 <p>You want to:</p> <ol> <li>Calculate total sales per customer,</li> <li>Then get customers with above-average total sales.</li> </ol>"},{"location":"concepts/SQL/cte/customer-sales-analysis/#cte-version-clean-and-readable","title":"\u2705 CTE version (clean and readable)","text":"<pre><code>WITH customer_totals AS (\n    SELECT customer_id, SUM(amount) AS total_sales\n    FROM sales\n    GROUP BY customer_id\n),\naverage_total AS (\n    SELECT AVG(total_sales) AS avg_sales\n    FROM customer_totals\n)\nSELECT ct.*\nFROM customer_totals ct, average_total at\nWHERE ct.total_sales &gt; at.avg_sales;\n</code></pre>"},{"location":"concepts/SQL/cte/customer-sales-analysis/#why-cte-helps","title":"Why CTE helps","text":"<ul> <li>You calculate total sales per customer once, and reuse it.</li> <li>You can't use <code>AVG(SUM(...))</code> directly in one query without nesting it awkwardly.</li> <li>If you tried doing it in one SELECT with subqueries, it gets messy, like this:</li> </ul>"},{"location":"concepts/SQL/cte/customer-sales-analysis/#windowing-version-clean-and-readable","title":"\u2705 Windowing version (clean and readable)","text":"<pre><code>SELECT customer_id, total_sales\nFROM (\n    SELECT customer_id,\n           SUM(amount) AS total_sales,\n           AVG(SUM(amount)) OVER () AS avg_sales\n    FROM sales\n    GROUP BY customer_id\n) AS t\nWHERE total_sales &gt; avg_sales;\n</code></pre>"},{"location":"concepts/SQL/cte/vacant-days-detailed/","title":"SQL Question Average Vacant Days for Active Airbnb Listings in 2021","text":""},{"location":"concepts/SQL/cte/vacant-days-detailed/#problem-statement","title":"Problem Statement","text":"<p>Write a query to calculate the average number of vacant days across active Airbnb properties in 2021. Only include currently active properties and round the result to the nearest whole number.</p>"},{"location":"concepts/SQL/cte/vacant-days-detailed/#schema","title":"Schema","text":""},{"location":"concepts/SQL/cte/vacant-days-detailed/#bookings-table","title":"<code>bookings</code> Table","text":"Column Name Type Description listing_id integer Unique identifier for the property checkin_date timestamp Start date of the booking checkout_date timestamp End date of the booking"},{"location":"concepts/SQL/cte/vacant-days-detailed/#listings-table","title":"<code>listings</code> Table","text":"Column Name Type Description listing_id integer Unique identifier for the property is_active integer 1 if active, 0 otherwise"},{"location":"concepts/SQL/cte/vacant-days-detailed/#assumptions","title":"Assumptions","text":"<ul> <li>A property is considered active if <code>is_active = 1</code></li> <li>For bookings that start before 2021, consider January 1, 2021 as the start date</li> <li>For bookings that end after 2021, consider December 31, 2021 as the end date</li> <li>Active listings with no bookings in 2021 should be considered as having 365 vacant days</li> </ul>"},{"location":"concepts/SQL/cte/vacant-days-detailed/#example-input","title":"Example Input","text":""},{"location":"concepts/SQL/cte/vacant-days-detailed/#bookings","title":"<code>bookings</code>","text":"listing_id checkin_date checkout_date 1 2021-08-17 00:00:00 2021-08-19 00:00:00 1 2021-08-19 00:00:00 2021-08-25 00:00:00 2 2021-08-19 00:00:00 2021-09-22 00:00:00 3 2021-12-23 00:00:00 2022-01-05 00:00:00"},{"location":"concepts/SQL/cte/vacant-days-detailed/#listings","title":"<code>listings</code>","text":"listing_id is_active 1 1 2 0 3 1"},{"location":"concepts/SQL/cte/vacant-days-detailed/#expected-output","title":"Expected Output","text":"avg_vacant_days 357"},{"location":"concepts/SQL/cte/vacant-days-detailed/#solution-explanation","title":"Solution Explanation","text":"<ol> <li>Property 1:</li> <li>Active (is_active = 1)</li> <li>Total rented days = 8 (Aug 17-19 + Aug 19-25)</li> <li> <p>Vacant days = 365 - 8 = 357</p> </li> <li> <p>Property 2:</p> </li> <li>Inactive (is_active = 0)</li> <li> <p>Excluded from calculation</p> </li> <li> <p>Property 3:</p> </li> <li>Active (is_active = 1)</li> <li>Total rented days = 9 (Dec 23-31, 2021)</li> <li> <p>Vacant days = 365 - 9 = 356</p> </li> <li> <p>Calculation:</p> </li> <li>Average vacant days = (357 + 356) / 2 = 356.5 \u2192 357 (rounded)</li> </ol> <p>Note: The actual query should work with any dataset following this schema, not just the example provided.</p>"},{"location":"concepts/SQL/cte/vacant-days-detailed/#answer","title":"Answer","text":"<pre><code>WITH listing_vacancies AS (\nSELECT \n  listings.listing_id,\n  365 - COALESCE(\n    SUM(\n      CASE WHEN checkout_date&gt;'12/31/2021' THEN '12/31/2021' ELSE checkout_date END -\n      CASE WHEN checkin_date&lt;'01/01/2021' THEN '01/01/2021' ELSE checkin_date END \n  ),0) AS vacant_days\nFROM listings \nLEFT JOIN bookings\n  ON listings.listing_id = bookings.listing_id \nWHERE listings.is_active = 1\nGROUP BY listings.listing_id)\n\nSELECT ROUND(AVG(vacant_days)) \nFROM listing_vacancies;\n</code></pre>"},{"location":"concepts/SQL/cte/vacant-days-detailed/#explanation-of-the-sql-query","title":"Explanation of the SQL Query","text":"<ol> <li>CTE (<code>listing_vacancies</code>):</li> <li>For each active listing, calculates the number of vacant days in 2021.</li> <li> <p>Uses a <code>LEFT JOIN</code> to ensure even listings with no bookings are included (they'll have 365 vacant days).</p> </li> <li> <p>Booked Days Calculation:</p> </li> <li>For each booking, calculates how many days it overlaps with 2021.</li> <li>If a booking starts before 2021, it considers January 1, 2021 as the start.</li> <li>If a booking ends after 2021, it considers December 31, 2021 as the end.</li> <li>The difference between these two dates gives the number of days the property was booked during 2021.</li> <li> <p>Sums all such days for each listing. If there are no bookings, the sum is <code>NULL</code>, so <code>COALESCE(..., 0)</code> ensures it's treated as 0.</p> </li> <li> <p>Vacant Days Calculation:</p> </li> <li> <p>Subtracts the total booked days from 365 (total days in 2021) for each listing.</p> </li> <li> <p>Final Output:</p> </li> <li>The outer query takes the average of all <code>vacant_days</code> values across active listings.</li> <li>Uses <code>ROUND</code> to return a whole number as required by the problem statement.</li> </ol> <p>This approach ensures:</p> <ul> <li>Only active listings are included.</li> <li>Listings with no bookings are treated as fully vacant.</li> <li>Bookings that span outside 2021 are properly bounded to the year.</li> <li>The result is the rounded average vacant days for all active listings.</li> </ul> <p>In this query, the COALESCE function ensures that if a listing has no bookings (so the SUM(...) returns NULL), it will be treated as having 0 booked days.</p> <p>Detailed breakdown:</p> <p>SUM(...) calculates the total number of days a listing was booked in 2021. If a listing has no bookings, SUM(...) returns NULL. COALESCE(SUM(...), 0) replaces that NULL with 0. Why is this important?</p> <p>Listings with no bookings should be counted as having all 365 days vacant. Without COALESCE, the subtraction 365 - NULL would result in NULL (unknown), and that listing would not be counted in the average. With COALESCE, such listings get 365 - 0 = 365 vacant days, as intended. Summary: COALESCE(..., 0) guarantees that listings with no bookings are correctly treated as fully vacant for the year.</p>"},{"location":"concepts/SQL/extras/","title":"SQL Extras - External Content Repository","text":"<p>This directory contains curated SQL learning materials from external sources, organized into key resource files to complement the main SQL concepts.</p>"},{"location":"concepts/SQL/extras/#main-files","title":"\ud83d\udcc4 Main Files","text":""},{"location":"concepts/SQL/extras/#toptalmd","title":"toptal.md","text":"<p>SQL interview questions and coding challenges from TopTal's engineering blog and resources. Includes practical problems designed for interview preparation and skill development.</p>"},{"location":"concepts/SQL/extras/#wikibooksmd","title":"wikibooks.md","text":"<p>SQL exercises and examples from Wikibooks SQL tutorials and documentation. Comprehensive learning materials covering fundamental to advanced SQL concepts.</p>"},{"location":"concepts/SQL/extras/#integration-with-main-sql-concepts","title":"\ud83d\udd17 Integration with Main SQL Concepts","text":"<ul> <li>Cross-reference main concepts using relative links: <code>../../aggregation/aggregate-functions.md</code></li> <li>Tagged content by SQL concept for easy discovery</li> <li>Difficulty progression to guide learning paths</li> </ul>"},{"location":"concepts/SQL/extras/#external-sources","title":"\ud83d\udcda External Sources","text":""},{"location":"concepts/SQL/extras/#recommended-sources-for-future-addition","title":"Recommended Sources for Future Addition","text":"<ul> <li>LeetCode SQL problems</li> <li>HackerRank SQL challenges</li> <li>SQLZoo tutorials</li> <li>Mode Analytics SQL tutorial</li> <li>DataLemur interview questions</li> </ul>"},{"location":"concepts/SQL/extras/#learning-path-integration","title":"\ud83c\udfaf Learning Path Integration","text":"<p>This extras directory supplements the main SQL learning progression:</p> <pre><code>Main SQL Concepts \u2192 Practice Problems \u2192 Interview Preparation\n     \u2193                        \u2193              \u2193\n[aggregation] \u2192 [wikibooks.md] \u2192 [toptal.md] \u2192 [company-specific]\n</code></pre> <p>Navigate back to SQL Concepts | SQL Learning Hub</p>"},{"location":"concepts/SQL/extras/toptal/","title":"SQL Interview Questions Streaming &amp; Media Analytics","text":"<p>This comprehensive guide covers SQL interview questions specifically tailored for streaming and media companies like Netflix, focusing on user analytics, content performance, and business metrics.</p>"},{"location":"concepts/SQL/extras/toptal/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p>Beginner Level</p> </li> <li> <p>Intermediate Level</p> </li> <li> <p>Advanced Level</p> </li> <li> <p>Optimization Challenges</p> </li> <li> <p>Cross-References</p> </li> </ul>"},{"location":"concepts/SQL/extras/toptal/#beginner-level","title":"Beginner Level","text":""},{"location":"concepts/SQL/extras/toptal/#1-daily-active-users-dau","title":"1 Daily Active Users (DAU)","text":"<p>Business Context: DAU:</p> <pre><code> User engagement metrics for streaming platforms.\n</code></pre> <p>Question:</p> <pre><code> You have a table `user_activity`:\n\n | user_id | activity_date | activity_type |\n |---------|---------------|---------------|\n | 101     | 2025-08-01    | play          |\n | 101     | 2025-08-01    | pause         |\n | 102     | 2025-08-01    | play          |\n | 103     | 2025-08-02    | play          |\n\n Write a query to find the number of **unique active users per day**.\n</code></pre> <p>Solution:</p> <pre><code>SELECT\n    activity_date,\n    COUNT(DISTINCT user_id) AS daily_active_users\nFROM user_activity\nGROUP BY activity_date\nORDER BY activity_date;\n</code></pre> <p>Learning Objectives:</p> <pre><code>- Basic aggregation with `COUNT(DISTINCT)`\n- Grouping data by date\n- Understanding user activity metrics\n</code></pre> <p>Related Concepts:</p> <p><code>aggregation-functions.md</code></p>"},{"location":"concepts/SQL/extras/toptal/#2-day-1-retention","title":"2 Day-1 Retention","text":"<p>Business Context:</p> <pre><code> User retention analysis for subscription services.\n</code></pre> <p>Question:</p> <pre><code> From `user_signup(user_id, signup_date)` and `user_activity(user_id, activity_date)`, calculate **Day-1 retention** (users who signed up on Day X and came back on Day X+1).\n</code></pre> <p>Solution:</p> <pre><code>SELECT\n    s.signup_date,\n    COUNT(DISTINCT a.user_id) * 1.0 / COUNT(DISTINCT s.user_id) AS day1_retention\nFROM user_signup s\nLEFT JOIN user_activity a\n    ON s.user_id = a.user_id\n    AND a.activity_date = DATE_ADD(s.signup_date, INTERVAL 1 DAY)\nGROUP BY s.signup_date;\n</code></pre> <p>Learning Objectives:</p> <pre><code> - Cohort analysis patterns\n\n - Percentage calculations with joins\n\n - Understanding retention metrics\n</code></pre> <p>Related Concepts:</p> <p><code>joins.md</code>, <code>aggregation-functions.md</code></p>"},{"location":"concepts/SQL/extras/toptal/#intermediate-level","title":"Intermediate Level","text":""},{"location":"concepts/SQL/extras/toptal/#3-most-watched-show-per-day","title":"3 Most Watched Show per Day","text":"<p>Business Context:</p> <pre><code> Content popularity analysis.\n</code></pre> <p>Question:</p> <pre><code> Table `watch_history`:\n\n | user_id | show_id | watch_date | watch_time_minutes |\n |---------|---------|------------|-------------------|\n | 1       | A       | 2025-08-01 | 30                |\n | 2       | A       | 2025-08-01 | 50                |\n | 3       | B       | 2025-08-01 | 80                |\n\n Find the **most watched show per day** (based on total minutes).\n</code></pre> <p>Solution:</p> <pre><code>SELECT watch_date, show_id\nFROM (\n    SELECT\n        watch_date,\n        show_id,\n        SUM(watch_time_minutes) AS total_watch_time,\n        RANK() OVER (PARTITION BY watch_date ORDER BY SUM(watch_time_minutes) DESC) AS rnk\n    FROM watch_history\n    GROUP BY watch_date, show_id\n) t\nWHERE rnk = 1;\n</code></pre> <p>Learning Objectives:</p> <pre><code>- Window functions for ranking\n- Subquery patterns\n- Top-N per category problems\n</code></pre> <p>Related Concepts:</p> <p><code>window-functions-overview.md</code>, <code>ranking-functions.md</code></p>"},{"location":"concepts/SQL/extras/toptal/#4-top-3-shows-per-region","title":"4 Top 3 Shows per Region","text":"<p>Business Context:</p> <pre><code> Regional content performance analysis.\n</code></pre> <p>Question:</p> <pre><code> Table `viewership(user_id, show_id, region, watch_time)`. Find the **top 3 most watched shows per region** by total minutes.\n</code></pre> <p>Solution:</p> <pre><code>SELECT region, show_id, total_watch_time\nFROM (\n    SELECT\n        region,\n        show_id,\n        SUM(watch_time) AS total_watch_time,\n        DENSE_RANK() OVER (PARTITION BY region ORDER BY SUM(watch_time) DESC) AS rnk\n    FROM viewership\n    GROUP BY region, show_id\n) t\nWHERE rnk &lt;= 3;\n</code></pre> <p>Learning Objectives:</p> <pre><code>- Difference between RANK() and DENSE_RANK()\n- Multi-level partitioning\n- Top-N queries per category\n</code></pre> <p>Related Concepts:</p> <p><code>window-functions-overview.md</code>, <code>ranking-functions.md</code></p>"},{"location":"concepts/SQL/extras/toptal/#5-consecutive-days-watching","title":"5 Consecutive Days Watching","text":"<p>Business Context:</p> <pre><code> User engagement streak analysis.\n</code></pre> <p>Question:</p> <pre><code> From `user_activity(user_id, activity_date)`, find users who have watched content for **3 consecutive days**.\n</code></pre> <p>Solution:</p> <pre><code>SELECT DISTINCT user_id\nFROM (\n    SELECT\n        user_id,\n        activity_date,\n        LAG(activity_date,1) OVER (PARTITION BY user_id ORDER BY activity_date) AS prev_day,\n        LAG(activity_date,2) OVER (PARTITION BY user_id ORDER BY activity_date) AS prev2_day\n    FROM user_activity\n) t\nWHERE DATEDIFF(activity_date, prev_day) = 1\nAND DATEDIFF(prev_day, prev2_day) = 1;\n</code></pre> <p>Learning Objectives:</p> <pre><code>- Using LAG() for sequential analysis\n- Date arithmetic in SQL\n- Streak detection patterns\n</code></pre> <p>Related Concepts:</p> <p><code>navigation-functions.md</code>, <code>window-functions-overview.md</code></p>"},{"location":"concepts/SQL/extras/toptal/#advanced-level","title":"Advanced Level","text":""},{"location":"concepts/SQL/extras/toptal/#6-heavy-watchers-retention-cte","title":"6 Heavy Watchers Retention (CTE)","text":"<p>Business Context:</p> <p>Advanced user segmentation and retention.</p> <p>Question:</p> <p>Find users who watched &gt;100 minutes in a day, and then find how many of them watched again the next day.</p> <p>Solution:</p> <pre><code>WITH daily_watch AS (\n    SELECT\n        user_id,\n        DATE(event_ts) AS watch_date,\n        SUM(watch_time) AS total_minutes\n    FROM watch_events\n    GROUP BY user_id, DATE(event_ts)\n),\nheavy_watchers AS (\n    SELECT user_id, watch_date\n    FROM daily_watch\n    WHERE total_minutes &gt; 100\n)\nSELECT COUNT(DISTINCT h1.user_id) AS retained_users\nFROM heavy_watchers h1\nJOIN heavy_watchers h2\n    ON h1.user_id = h2.user_id\n    AND h2.watch_date = DATE_ADD(h1.watch_date, INTERVAL 1 DAY);\n</code></pre> <p>Learning Objectives:</p> <ul> <li>Common Table Expressions (CTEs)</li> <li>Multi-step data transformation</li> <li>Advanced retention analysis</li> </ul> <p>Related Concepts:</p> <p><code>cte-overview.md</code></p>"},{"location":"concepts/SQL/extras/toptal/#7-content-hierarchy-recursive-cte","title":"7 Content Hierarchy (Recursive CTE)","text":"<p>Business Context:</p> <p>Managing hierarchical content structures.</p> <p>Question:</p> <p>Model content hierarchy (seasons, episodes) using recursive CTEs.</p> <p>Solution:</p> <pre><code>WITH RECURSIVE content_hierarchy AS (\n    -- Base case: top-level content\n    SELECT\n        content_id,\n        parent_id,\n        title,\n        0 AS level,\n        CAST(content_id AS VARCHAR) AS path\n    FROM content\n    WHERE parent_id IS NULL\n\n    UNION ALL\n\n    -- Recursive case: child content\n    SELECT\n        c.content_id,\n        c.parent_id,\n        c.title,\n        ch.level + 1,\n        CONCAT(ch.path, '&gt;', c.content_id)\n    FROM content c\n    JOIN content_hierarchy ch ON c.parent_id = ch.content_id\n)\nSELECT * FROM content_hierarchy ORDER BY path;\n</code></pre> <p>Learning Objectives:</p> <ul> <li>Recursive CTE patterns</li> <li>Hierarchical data modeling</li> <li>Tree traversal in SQL</li> </ul> <p>Related Concepts:</p> <p><code>cte-overview.md</code></p>"},{"location":"concepts/SQL/extras/toptal/#8-percentile-analysis","title":"8 Percentile Analysis","text":"<p>Business Context:</p> <p>Statistical analysis of user behavior.</p> <p>Question:</p> <p>Find the top 1% of users by total watch time.</p> <p>Solution:</p> <pre><code>WITH user_totals AS (\n    SELECT\n        user_id,\n        SUM(watch_time) AS total_watch_time\n    FROM watch_events\n    GROUP BY user_id\n),\nranked AS (\n    SELECT\n        user_id,\n        total_watch_time,\n        PERCENT_RANK() OVER (ORDER BY total_watch_time DESC) AS pct_rank\n    FROM user_totals\n)\nSELECT user_id, total_watch_time\nFROM ranked\nWHERE pct_rank &lt;= 0.01;\n</code></pre> <p>Learning Objectives:</p> <ul> <li>Statistical functions in SQL</li> <li>Percentile calculations</li> <li>User segmentation</li> </ul> <p>Related Concepts:</p> <p><code>advanced-window-functions.md</code></p>"},{"location":"concepts/SQL/extras/toptal/#9-recommendation-acceptance-rate","title":"9 Recommendation Acceptance Rate","text":"<p>Business Context:</p> <p>Measuring recommendation system effectiveness.</p> <p>Question:</p> <p>Measure how many recommended shows were actually watched.</p> <p>Solution:</p> <pre><code>SELECT\n    COUNT(DISTINCT CASE WHEN w.user_id IS NOT NULL THEN r.user_id END) * 100.0 /\n    COUNT(DISTINCT r.user_id) AS acceptance_rate_pct\nFROM recommendations r\nLEFT JOIN watch_events w\n    ON r.user_id = w.user_id\n    AND r.show_id = w.show_id\n    AND w.event_ts BETWEEN r.recommendation_ts\n                         AND r.recommendation_ts + INTERVAL '24' HOUR;\n</code></pre> <p>Learning Objectives:</p> <ul> <li> <p>Complex join conditions with time windows</p> </li> <li> <p>Conditional aggregation</p> </li> <li> <p>A/B testing analysis patterns</p> </li> </ul> <p>Related Concepts:</p> <p><code>joins.md</code>, <code>aggregation-functions.md</code></p>"},{"location":"concepts/SQL/extras/toptal/#optimization-challenges","title":"Optimization Challenges","text":""},{"location":"concepts/SQL/extras/toptal/#10-efficient-dau-calculation","title":"10 Efficient DAU Calculation","text":"<p>Business Context:</p> <p>Performance optimization for large-scale analytics.</p> <p>Question:</p> <p>Optimizing daily active users query for large datasets.</p> <p>Solutions:</p> <pre><code>-- Optimized version with date range limiting\nSELECT\n    DATE(event_ts) AS activity_date,\n    COUNT(DISTINCT user_id) AS daily_active_users\nFROM watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '30' DAY\n    AND event_ts &lt; CURRENT_DATE\nGROUP BY DATE(event_ts)\nORDER BY activity_date;\n\n-- Approximate counting for very large datasets\nSELECT\n    DATE(event_ts) AS activity_date,\n    APPROX_COUNT_DISTINCT(user_id) AS approx_dau\nFROM watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '30' DAY\nGROUP BY DATE(event_ts);\n</code></pre>"},{"location":"concepts/SQL/extras/toptal/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Query performance optimization</li> <li>Approximate vs exact calculations</li> <li>Date range filtering</li> </ul> <p>Related Concepts:</p> <p><code>sql-optimization-challenges.md</code></p>"},{"location":"concepts/SQL/extras/toptal/#11-time-based-partitioning","title":"11 Time-based Partitioning","text":"<p>Business Context:</p> <p>Database design for time-series data.</p> <p>Question:</p> <p>Optimizing for time-series queries with partitioning.</p> <p>Solution:</p> <pre><code>-- Partition by day for efficient date range queries\nCREATE TABLE watch_events (\n    event_id BIGINT,\n    user_id BIGINT,\n    show_id BIGINT,\n    event_ts TIMESTAMP,\n    watch_time INT\n)\nPARTITION BY DATE(event_ts);\n\n-- Query benefits from partition pruning\nSELECT COUNT(DISTINCT user_id)\nFROM watch_events\nWHERE event_ts &gt;= '2025-01-01'\n    AND event_ts &lt; '2025-02-01';\n</code></pre> <p>Learning Objectives:</p> <ul> <li>Database partitioning strategies</li> <li>Partition pruning</li> <li>Schema design for analytics</li> </ul> <p>Related Concepts:</p> <p><code>partitioning-sharding.md</code></p>"},{"location":"concepts/SQL/extras/toptal/#12-efficient-top-n-queries","title":"12 Efficient Top-N Queries","text":"<p>Business Context:</p> <p>Performance optimization for ranking queries.</p> <p>Question:</p> <p>Optimizing ranking queries with large datasets.</p> <p>Solution:</p> <pre><code>-- Use LIMIT with proper ordering for Top-N\nSELECT show_id, SUM(watch_time) AS total_watch_time\nFROM watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '7' DAY\nGROUP BY show_id\nORDER BY total_watch_time DESC\nLIMIT 10;\n\n-- Pre-aggregate for frequently accessed metrics\nCREATE MATERIALIZED VIEW daily_show_metrics AS\nSELECT\n    DATE(event_ts) AS watch_date,\n    show_id,\n    COUNT(DISTINCT user_id) AS unique_viewers,\n    SUM(watch_time) AS total_watch_time\nFROM watch_events\nGROUP BY DATE(event_ts), show_id;\n</code></pre> <p>Learning Objectives:</p> <ul> <li>LIMIT clause optimization</li> <li>Materialized views</li> <li>Pre-aggregation strategies</li> </ul> <p>Related Concepts:</p> <p><code>sql-optimization-challenges.md</code></p>"},{"location":"concepts/SQL/extras/toptal/#cross-references","title":"Cross-References","text":""},{"location":"concepts/SQL/extras/toptal/#main-sql-concepts-covered","title":"Main SQL Concepts Covered","text":"<ul> <li>Aggregation Functions: <code>aggregation-functions.md</code></li> <li>Window Functions: <code>window-functions-overview.md</code></li> <li>Common Table Expressions: <code>cte-overview.md</code></li> <li>Joins: <code>joins.md</code></li> <li>Subqueries: <code>subqueries.md</code></li> <li>Optimization: <code>sql-optimization-challenges.md</code></li> </ul>"},{"location":"concepts/SQL/extras/toptal/#business-domains","title":"Business Domains","text":"<ul> <li>Streaming Analytics: User engagement, content performance</li> <li>Subscription Metrics: Retention, churn analysis</li> <li>Recommendation Systems: Acceptance rate, personalization</li> <li>Content Management: Hierarchical data, catalog analytics</li> </ul>"},{"location":"concepts/SQL/extras/toptal/#difficulty-progression","title":"Difficulty Progression","text":"<ul> <li>Beginner: Basic aggregation, simple joins</li> <li>Intermediate: Window functions, ranking, date arithmetic</li> <li>Advanced: CTEs, recursive queries, statistical functions</li> <li>Expert: Complex joins, performance optimization, schema design</li> </ul>"},{"location":"concepts/SQL/extras/toptal/#learning-path","title":"Learning Path","text":"<ol> <li>Start with basic aggregation and joins</li> <li>Learn window functions for advanced analytics</li> <li>Master CTEs and subqueries for complex transformations</li> <li>Study optimization techniques for production environments</li> <li>Explore recursive patterns for hierarchical data</li> </ol>"},{"location":"concepts/SQL/extras/toptal/#this-guide-is-designed-for-sql-interviews-at-streaming-companies-and-covers-the-most-common-patterns-encountered-in-data-analyst-and-data-engineer-roles","title":"This guide is designed for SQL interviews at streaming companies and covers the most common patterns encountered in data analyst and data engineer roles","text":""},{"location":"concepts/SQL/extras/toptal/#appendix-contributor-guidelines-and-organizational-templates","title":"Appendix Contributor Guidelines and Organizational Templates","text":""},{"location":"concepts/SQL/extras/toptal/#toptal-sql-interview-questions","title":"TopTal SQL Interview Questions","text":"<p>This directory contains SQL interview questions and coding challenges adapted from TopTal's engineering blog and SQL interview preparation resources. Focus is on real-world interview scenarios and advanced SQL patterns.</p>"},{"location":"concepts/SQL/extras/toptal/#interview-question-structure","title":"\ud83c\udfaf Interview Question Structure","text":"<p>Each interview question follows this format:</p>"},{"location":"concepts/SQL/extras/toptal/#problem-statement","title":"Problem Statement","text":"<p>Real-world business scenario requiring SQL solution</p> <p>Business Context:</p> <ul> <li>Company: Tech startup, e-commerce, SaaS platform</li> <li>Data Scale: Millions of users, billions of events</li> <li>Performance Requirements: Sub-second response time</li> <li>Business Impact: Revenue optimization, user engagement, fraud detection</li> </ul>"},{"location":"concepts/SQL/extras/toptal/#schema-design","title":"Schema Design","text":"<pre><code>-- Production-ready table schemas\nCREATE TABLE users (\n    user_id BIGINT PRIMARY KEY,\n    created_at TIMESTAMP,\n    country VARCHAR(2),\n    subscription_status VARCHAR(20)\n) PARTITION BY created_at;\n\nCREATE TABLE user_events (\n    event_id BIGINT PRIMARY KEY,\n    user_id BIGINT,\n    event_type VARCHAR(50),\n    event_value DECIMAL(10,2),\n    event_timestamp TIMESTAMP\n) PARTITION BY event_timestamp;\n</code></pre>"},{"location":"concepts/SQL/extras/toptal/#sample-data","title":"Sample Data","text":"<pre><code>-- Representative dataset for testing\nINSERT INTO users VALUES\n(1, '2023-01-01', 'US', 'premium'),\n(2, '2023-01-15', 'CA', 'basic');\n</code></pre>"},{"location":"concepts/SQL/extras/toptal/#solution-requirements","title":"Solution Requirements","text":"<ul> <li>Expected Output: Specific format and columns</li> <li>Performance Constraints: Query must complete within time limits</li> <li>Edge Cases: Handle NULL values, empty results, data quality issues</li> </ul>"},{"location":"concepts/SQL/extras/toptal/#optimal-solution","title":"Optimal Solution","text":"<pre><code>-- Production-ready SQL with performance considerations\nWITH user_metrics AS (\n    SELECT\n        u.user_id,\n        u.country,\n        COUNT(ue.event_id) as event_count,\n        SUM(ue.event_value) as total_value,\n        AVG(ue.event_value) as avg_value\n    FROM users u\n    LEFT JOIN user_events ue ON u.user_id = ue.user_id\n        AND ue.event_timestamp &gt;= u.created_at\n    WHERE u.subscription_status = 'premium'\n    GROUP BY u.user_id, u.country\n)\nSELECT\n    country,\n    COUNT(*) as premium_users,\n    AVG(event_count) as avg_events_per_user,\n    SUM(total_value) as total_revenue\nFROM user_metrics\nGROUP BY country\nORDER BY total_revenue DESC;\n</code></pre>"},{"location":"concepts/SQL/extras/toptal/#performance-analysis","title":"Performance Analysis","text":"<ul> <li>Query Execution Plan: Index usage, join strategies</li> <li>Optimization Opportunities: Partitioning, materialized views</li> <li>Scalability Considerations: Data volume impact, memory usage</li> </ul>"},{"location":"concepts/SQL/extras/toptal/#toptal-source-material","title":"\ud83d\udd17 TopTal Source Material","text":"<p>Based on TopTal's SQL interview preparation articles:</p> <ul> <li>SQL Interview Questions</li> <li>Database Design Patterns</li> <li>Data Engineering Challenges</li> </ul>"},{"location":"concepts/SQL/extras/toptal/#difficulty-distribution","title":"\ud83d\udcca Difficulty Distribution","text":"Difficulty Count Focus Areas \ud83d\udfe1 Intermediate ~12 questions Complex analytics, multi-table queries \ud83d\udfe0 Advanced ~8 questions Window functions, CTEs, optimization \ud83d\udd34 Expert ~5 questions Large-scale data, performance tuning"},{"location":"concepts/SQL/extras/toptal/#interview-preparation-focus","title":"\ud83c\udfaf Interview Preparation Focus","text":""},{"location":"concepts/SQL/extras/toptal/#common-toptal-sql-patterns","title":"Common TopTal SQL Patterns","text":"<ol> <li>Revenue Analytics - Customer lifetime value, cohort analysis</li> <li>User Behavior Analysis - Funnel analysis, retention metrics</li> <li>Performance Optimization - Query tuning, index strategies</li> <li>Data Quality - NULL handling, data validation</li> <li>Scalability - Partitioning, query optimization for large datasets</li> </ol>"},{"location":"concepts/SQL/extras/toptal/#toptal-specific-preparation","title":"TopTal-Specific Preparation","text":"<ul> <li>Focus on business context and real-world applications</li> <li>Emphasize performance and scalability considerations</li> <li>Practice explaining query logic and optimization decisions</li> <li>Understand trade-offs between different solution approaches</li> </ul>"},{"location":"concepts/SQL/extras/toptal/#adding-new-interview-questions","title":"\ud83d\udcdd Adding New Interview Questions","text":"<p>When adding TopTal-style questions:</p> <ol> <li>Include complete business context and requirements</li> <li>Provide production-ready schema with constraints</li> <li>Include performance analysis and optimization notes</li> <li>Tag with relevant SQL concepts and business domains</li> <li>Reference original TopTal article when applicable</li> </ol>"},{"location":"concepts/SQL/extras/wikibooks/","title":"SQL Exercises from Wikibooks","text":"<p>A comprehensive collection of SQL exercises organized by difficulty level, designed to build practical SQL skills from basic concepts to advanced patterns. These exercises are adapted from real-world scenarios and interview problems, particularly focusing on analytics and data manipulation common in streaming platforms and business intelligence.</p>"},{"location":"concepts/SQL/extras/wikibooks/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Beginner Level: Basic Analytics</li> <li>Intermediate Level Window Functions &amp; Ranking</li> <li>Advanced Level: Complex Patterns &amp; Optimization</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#beginner-level-basic-analytics","title":"Beginner Level Basic Analytics","text":"<p>Exercises focusing on fundamental SQL operations including aggregation, grouping, and basic data manipulation.</p>"},{"location":"concepts/SQL/extras/wikibooks/#1-daily-active-users-dau","title":"1. Daily Active Users (DAU)","text":"<p>Learning Objectives::</p> <ul> <li>Understand basic aggregation with <code>COUNT DISTINCT</code></li> <li>Practice <code>GROUP BY</code> with date-based grouping</li> <li>Learn to calculate user metrics per time period</li> </ul> <p>Question: You have a table <code>user_activity</code>:</p> user_id activity_date activity_type 101 2025-08-01 play 101 2025-08-01 pause 102 2025-08-01 play 103 2025-08-02 play <p>Write a query to find the number of unique active users per day.</p> <p>Solution::</p> <pre><code>SELECT\n  activity_date,\n  COUNT(DISTINCT user_id) AS daily_active_users\nFROM user_activity\nGROUP BY activity_date\nORDER BY activity_date;\n</code></pre> <p>Explanation::</p> <ul> <li>Groups by day and counts distinct users</li> <li>Basic aggregation pattern for DAU calculation</li> <li>Common Netflix interview question for analytics basics</li> </ul> <p>Cross-references::</p> <ul> <li>Aggregate Functions</li> <li>SQL Concepts Overview</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#2-day-1-retention","title":"2. Day-1 Retention","text":"<p>Learning Objectives::</p> <ul> <li>Practice self-joins with date arithmetic</li> <li>Calculate retention metrics using percentages</li> <li>Understand cohort analysis patterns</li> </ul> <p>Question: From <code>user_signup(user_id, signup_date)</code> and <code>user_activity(user_id, activity_date)</code>, calculate Day-1 retention (users who signed up on Day X and came back on Day X+1).</p> <p>Solution::</p> <pre><code>SELECT\n  s.signup_date,\n  COUNT(DISTINCT a.user_id) * 1.0 / COUNT(DISTINCT s.user_id) AS day1_retention\nFROM user_signup s\nLEFT JOIN user_activity a\n  ON s.user_id = a.user_id\n  AND a.activity_date = DATE_ADD(s.signup_date, INTERVAL 1 DAY)\nGROUP BY s.signup_date;\n</code></pre> <p>Explanation::</p> <ul> <li>Numerator: users active next day after signup</li> <li>Denominator: total signups for that day</li> <li>Uses <code>LEFT JOIN</code> with date filtering for the retention logic</li> <li>Common cohort analysis pattern in user analytics</li> </ul> <p>Cross-references::</p> <ul> <li>JOINs and Relationships</li> <li>Aggregate Functions</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#3-most-watched-show-per-day","title":"3. Most Watched Show per Day","text":"<p>Learning Objectives::</p> <ul> <li>Implement ranking without window functions</li> <li>Practice subqueries for top-N queries</li> <li>Understand content analytics patterns</li> </ul> <p>Question: Table <code>watch_history</code>:</p> user_id show_id watch_date watch_time_minutes 1 A 2025-08-01 30 2 A 2025-08-01 50 3 B 2025-08-01 80 <p>Find the most watched show per day (based on total minutes).</p> <p>Solution::</p> <pre><code>SELECT watch_date, show_id\nFROM (\n  SELECT\n    watch_date,\n    show_id,\n    SUM(watch_time_minutes) AS total_watch_time,\n    RANK() OVER (PARTITION BY watch_date ORDER BY SUM(watch_time_minutes) DESC) AS rnk\n  FROM watch_history\n  GROUP BY watch_date, show_id\n) t\nWHERE rnk = 1;\n</code></pre> <p>Explanation::</p> <ul> <li>Uses window function <code>RANK()</code> to find the top show per day</li> <li>Groups by date and show first, then ranks within each date</li> <li>Basic ranking pattern for top-N per category problems</li> </ul> <p>Cross-references::</p> <ul> <li>Window Functions</li> <li>Aggregate Functions</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#intermediate-level-window-functions-ranking","title":"Intermediate Level Window Functions &amp; Ranking","text":"<p>Exercises focusing on advanced analytics using window functions, ranking, and navigation functions.</p>"},{"location":"concepts/SQL/extras/wikibooks/#4-top-3-shows-per-region","title":"4. Top 3 Shows per Region","text":"<p>Learning Objectives::</p> <ul> <li>Master <code>DENSE_RANK()</code> vs <code>RANK()</code> functions</li> <li>Practice partitioning for grouped rankings</li> <li>Understand regional analytics patterns</li> </ul> <p>Question: Table <code>viewership(user_id, show_id, region, watch_time)</code>. Find the top 3 most watched shows per region by total minutes.</p> <p>Solution::</p> <pre><code>SELECT region, show_id, total_watch_time\nFROM (\n  SELECT\n    region,\n    show_id,\n    SUM(watch_time) AS total_watch_time,\n    DENSE_RANK() OVER (PARTITION BY region ORDER BY SUM(watch_time) DESC) AS rnk\n  FROM viewership\n  GROUP BY region, show_id\n) t\nWHERE rnk &lt;= 3;\n</code></pre> <p>Explanation::</p> <ul> <li>Uses <code>DENSE_RANK()</code> for ranking within each region</li> <li><code>DENSE_RANK()</code> vs <code>RANK()</code>: no gaps in ranking (1,2,2,3 vs 1,2,2,4)</li> <li>Groups by region and show first, then applies ranking window function</li> <li>Common pattern for \"top N per category\" problems</li> </ul> <p>Cross-references::</p> <ul> <li>Ranking Functions</li> <li>Window Functions Overview</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#5-consecutive-days-watching","title":"5. Consecutive Days Watching","text":"<p>Learning Objectives::</p> <ul> <li>Use navigation functions (<code>LAG</code>/<code>LEAD</code>) for pattern detection</li> <li>Implement streak analysis with window functions</li> <li>Practice date arithmetic for consecutive patterns</li> </ul> <p>Question: From <code>user_activity(user_id, activity_date)</code>, find users who have watched content for 3 consecutive days.</p> <p>Solution::</p> <pre><code>SELECT DISTINCT user_id\nFROM (\n  SELECT\n    user_id,\n    activity_date,\n    LAG(activity_date,1) OVER (PARTITION BY user_id ORDER BY activity_date) AS prev_day,\n    LAG(activity_date,2) OVER (PARTITION BY user_id ORDER BY activity_date) AS prev2_day\n  FROM user_activity\n) t\nWHERE DATEDIFF(activity_date, prev_day) = 1\n  AND DATEDIFF(prev_day, prev2_day) = 1;\n</code></pre> <p>Explanation::</p> <ul> <li>Uses <code>LAG()</code> window function to look back at previous activity dates</li> <li><code>DATEDIFF()</code> checks for consecutive days (difference = 1)</li> <li>Alternative approach: use <code>ROW_NUMBER()</code> and date arithmetic for more complex streak detection</li> <li>Common pattern for \"consecutive days\" or \"streak\" problems</li> </ul> <p>Cross-references::</p> <ul> <li>Navigation Functions</li> <li>Common Table Expressions (CTEs)</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#advanced-level-complex-patterns-optimization","title":"Advanced Level Complex Patterns &amp; Optimization","text":"<p>Exercises covering advanced SQL patterns including CTEs, recursive queries, and performance optimization techniques.</p>"},{"location":"concepts/SQL/extras/wikibooks/#6-heavy-watchers-retention-ctes","title":"6. Heavy Watchers Retention (CTEs)","text":"<p>Learning Objectives::</p> <ul> <li>Build multi-step analysis using CTEs</li> <li>Implement complex retention calculations</li> <li>Practice CTE chaining for readability</li> </ul> <p>Question: Find users who watched &gt;100 minutes in a day, and then find how many of them watched again the next day.</p> <p>Solution::</p> <pre><code>WITH daily_watch AS (\n  SELECT\n    user_id,\n    DATE(event_ts) AS watch_date,\n    SUM(watch_time) AS total_minutes\n  FROM watch_events\n  GROUP BY user_id, DATE(event_ts)\n),\nheavy_watchers AS (\n  SELECT user_id, watch_date\n  FROM daily_watch\n  WHERE total_minutes &gt; 100\n)\nSELECT COUNT(DISTINCT h1.user_id) AS retained_users\nFROM heavy_watchers h1\nJOIN heavy_watchers h2\n  ON h1.user_id = h2.user_id\n  AND h2.watch_date = DATE_ADD(h1.watch_date, INTERVAL 1 DAY);\n</code></pre> <p>Explanation::</p> <ul> <li>Uses CTEs to break down the problem into logical steps</li> <li>First CTE calculates daily watch time per user</li> <li>Second CTE identifies heavy watchers</li> <li>Final query joins to find retention</li> <li>CTEs improve readability and allow step-by-step problem solving</li> </ul> <p>Cross-references::</p> <ul> <li>Common Table Expressions (CTEs)</li> <li>CTE vs Window Functions</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#7-content-hierarchy-recursive-ctes","title":"7. Content Hierarchy (Recursive CTEs)","text":"<p>Learning Objectives::</p> <ul> <li>Implement recursive CTEs for hierarchical data</li> <li>Understand tree-like data structures in SQL</li> <li>Practice path generation and level calculation</li> </ul> <p>Question: Model content hierarchy (seasons, episodes) using recursive CTEs.</p> <p>Solution::</p> <pre><code>WITH RECURSIVE content_hierarchy AS (\n  -- Base case: top-level content\n  SELECT\n    content_id,\n    parent_id,\n    title,\n    0 AS level,\n    CAST(content_id AS VARCHAR) AS path\n  FROM content\n  WHERE parent_id IS NULL\n\n  UNION ALL\n\n  -- Recursive case: child content\n  SELECT\n    c.content_id,\n    c.parent_id,\n    c.title,\n    ch.level + 1,\n    CONCAT(ch.path, '&gt;', c.content_id)\n  FROM content c\n  JOIN content_hierarchy ch ON c.parent_id = ch.content_id\n)\nSELECT * FROM content_hierarchy ORDER BY path;\n</code></pre> <p>Explanation::</p> <ul> <li>Recursive CTE handles hierarchical or tree-like data structures</li> <li>Base case starts with root content (no parent)</li> <li>Recursive case builds the hierarchy level by level</li> <li>Generates path and level information for each item</li> </ul> <p>Cross-references::</p> <ul> <li>Common Table Expressions (CTEs)</li> <li>Vacant Days Analysis</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#8-percentile-analysis-advanced-window-functions","title":"8. Percentile Analysis (Advanced Window Functions)","text":"<p>Learning Objectives::</p> <ul> <li>Use statistical window functions like <code>PERCENT_RANK()</code></li> <li>Implement percentile-based user segmentation</li> <li>Practice advanced ranking techniques</li> </ul> <p>Question: Find the top 1% of users by total watch time.</p> <p>Solution::</p> <pre><code>WITH user_totals AS (\n  SELECT\n    user_id,\n    SUM(watch_time) AS total_watch_time\n  FROM watch_events\n  GROUP BY user_id\n),\nranked AS (\n  SELECT\n    user_id,\n    total_watch_time,\n    PERCENT_RANK() OVER (ORDER BY total_watch_time DESC) AS pct_rank\n  FROM user_totals\n)\nSELECT user_id, total_watch_time\nFROM ranked\nWHERE pct_rank &lt;= 0.01;\n</code></pre> <p>Explanation::</p> <ul> <li><code>PERCENT_RANK()</code> enables statistical analysis</li> <li>Identifies top percentage of users by any metric</li> <li>Useful for user segmentation and targeting</li> </ul> <p>Cross-references::</p> <ul> <li>Advanced Concepts</li> <li>Window Functions Overview</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#9-query-performance-optimization","title":"9. Query Performance Optimization","text":"<p>Learning Objectives::</p> <ul> <li>Optimize queries for large datasets</li> <li>Use approximate functions for performance</li> <li>Implement date range filtering effectively</li> </ul> <p>Question: Optimizing daily active users query for large datasets.</p> <p>Solution::</p> <pre><code>-- Optimized version with proper partitioning\nSELECT\n  DATE(event_ts) AS activity_date,\n  COUNT(DISTINCT user_id) AS daily_active_users\nFROM watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '30' DAY  -- Limit date range\n  AND event_ts &lt; CURRENT_DATE\nGROUP BY DATE(event_ts)\nORDER BY activity_date;\n\n-- Alternative using approximate counting for very large datasets\nSELECT\n  DATE(event_ts) AS activity_date,\n  APPROX_COUNT_DISTINCT(user_id) AS approx_dau\nFROM watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '30' DAY\nGROUP BY DATE(event_ts);\n</code></pre> <p>Explanation::</p> <ul> <li>Date range filtering reduces data scanned</li> <li><code>APPROX_COUNT_DISTINCT()</code> trades accuracy for performance</li> <li>Partitioning by date enables efficient queries</li> </ul> <p>Cross-references::</p> <ul> <li>SQL Optimization</li> <li>Advanced SQL Patterns</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#10-indexing-strategy","title":"10. Indexing Strategy","text":"<p>Learning Objectives::</p> <ul> <li>Design composite indexes for query patterns</li> <li>Understand covering indexes vs regular indexes</li> <li>Implement partial indexes for filtered data</li> </ul> <p>Question:</p> <p>Key indexes for common Netflix query patterns.</p> <p>Solution::</p> <pre><code>-- Composite index for user activity queries\nCREATE INDEX idx_user_date ON watch_events (user_id, event_ts);\n\n-- Covering index for content analytics\nCREATE INDEX idx_content_metrics ON watch_events (show_id, event_ts, watch_time);\n\n-- Partial index for active users only\nCREATE INDEX idx_recent_activity ON watch_events (user_id, event_ts)\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '90' DAY;\n</code></pre> <p>Explanation::</p> <ul> <li>Composite indexes optimize multi-column queries</li> <li>Covering indexes avoid table lookups</li> <li>Partial indexes reduce index size for filtered data</li> </ul> <p>Cross-references::</p> <ul> <li>SQL Optimization</li> <li>Performance Optimization</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#learning-progression","title":"Learning Progression","text":"<p>Start with Beginner Level exercises to build confidence with basic SQL operations. Progress to Intermediate Level to master window functions and ranking. Finally, tackle Advanced Level exercises to handle complex analytical patterns and optimization challenges.</p> <p>Each exercise includes:</p> <ul> <li>Learning Objectives: What you'll master</li> <li>Solution: Complete SQL code with best practices</li> <li>Explanation: Key concepts and patterns demonstrated</li> <li>Cross-references: Links to detailed concept documentation</li> </ul> <p>For additional practice, explore the Netflix SQL Interview Problems which demonstrate these concepts in real-world scenarios.</p> <p>Next Steps::</p> <ul> <li>SQL Concepts Overview</li> <li>Aggregate Functions</li> <li>Window Functions</li> <li>Advanced SQL Patterns</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#wikibooks-sql-exercises","title":"Wikibooks SQL Exercises","text":"<p>This directory contains SQL exercises and examples adapted from Wikibooks SQL tutorials. Content focuses on practical SQL learning with hands-on exercises.</p>"},{"location":"concepts/SQL/extras/wikibooks/#exercise-structure","title":"\ud83c\udfaf Exercise Structure","text":"<p>Each exercise follows this format:</p>"},{"location":"concepts/SQL/extras/wikibooks/#problem-statement","title":"Problem Statement","text":"<p>Clear, concise description of the SQL challenge:</p>"},{"location":"concepts/SQL/extras/wikibooks/#sample-data","title":"Sample Data","text":"<pre><code>-- Table schemas and sample data\nCREATE TABLE example_table (\n    id INT,\n    name VARCHAR(50),\n    value DECIMAL(10,2)\n);\n\nINSERT INTO example_table VALUES\n(1, 'Example 1', 100.50),\n(2, 'Example 2', 200.75);\n</code></pre>"},{"location":"concepts/SQL/extras/wikibooks/#solution","title":"Solution","text":"<pre><code>-- Well-commented SQL query\nSELECT\n    name,\n    SUM(value) as total_value\nFROM example_table\nWHERE value &gt; 100\nGROUP BY name;\n</code></pre>"},{"location":"concepts/SQL/extras/wikibooks/#explanation","title":"Explanation","text":"<ul> <li>Concepts Demonstrated: Aggregation functions, filtering, GROUP BY</li> <li>Key Learning Points: How to filter before aggregation, proper column selection</li> <li>Difficulty: \ud83d\udfe2 Beginner</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#wikibooks-source-material","title":"\ud83d\udd17 Wikibooks Source Material","text":"<p>Based on the following Wikibooks chapters:</p> <ul> <li>SQL Exercises</li> <li>SQL Dialects Reference</li> <li>SQL Queries</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#difficulty-distribution","title":"\ud83d\udcca Difficulty Distribution","text":"Difficulty Count Topics \ud83d\udfe2 Beginner ~15 exercises Basic SELECT, aggregation, simple JOINs \ud83d\udfe1 Intermediate ~10 exercises Complex JOINs, subqueries \ud83d\udfe0 Advanced ~5 exercises Window functions, CTEs"},{"location":"concepts/SQL/extras/wikibooks/#learning-integration","title":"\ud83c\udfaf Learning Integration","text":"<p>These exercises complement the main SQL concepts:</p> <ul> <li>Practice basic skills from aggregation</li> <li>Apply JOIN patterns from joins</li> <li>Build toward window functions</li> </ul>"},{"location":"concepts/SQL/extras/wikibooks/#adding-new-exercises","title":"\ud83d\udcdd Adding New Exercises","text":"<p>When adding new Wikibooks exercises:</p> <ol> <li>Follow the established naming convention: <code>01-basic-aggregation.md</code></li> <li>Include complete schema setup and sample data</li> <li>Provide multiple solution approaches where applicable</li> <li>Tag with relevant SQL concepts and difficulty level</li> <li>Cross-reference related main concepts</li> </ol>"},{"location":"concepts/SQL/joins/","title":"Joins","text":"<ul> <li>Page Impressions and User Actions Analysis</li> </ul>"},{"location":"concepts/SQL/joins/page-impressions-analysis/","title":"Page Impressions and User Actions Analysis","text":"<p>The following SQL questions will be based on these tables schemas:</p>"},{"location":"concepts/SQL/joins/page-impressions-analysis/#database-schema","title":"Database Schema","text":""},{"location":"concepts/SQL/joins/page-impressions-analysis/#tables","title":"Tables","text":"<p>page_impression:</p> <ul> <li><code>id</code> STRING</li> <li><code>visitor_id</code> STRING</li> <li><code>page_name</code> STRING</li> <li><code>referrer_page_name</code> STRING</li> <li><code>ts</code> BIGINT</li> <li><code>ds</code> STRING</li> </ul> <p>user_action:</p> <ul> <li><code>id</code> STRING</li> <li><code>page_impression_id</code> STRING (foreign key to page_impression.id)</li> <li><code>action</code> STRING</li> <li><code>ts</code> BIGINT</li> <li><code>ds</code> STRING</li> </ul> <p>Relationship: page_impression.id \u2190 user_action.page_impression_id (one-to-many)</p>"},{"location":"concepts/SQL/joins/page-impressions-analysis/#example-data","title":"Example Data","text":"id visitor_id page_name referrer_page_name ts ds 1 10 HOME 2021-03-01 12:00:00 2021-03-01 2 20 HOME 2021-03-01 12:01:00 2021-03-01 3 30 HOME 2021-03-01 12:02:00 2021-03-01 4 10 LISTING HOME 2021-03-01 12:04:00 2021-03-01 5 30 LISTING HOME 2021-03-01 12:04:30 2021-03-01 6 30 LISTING LISTING 2021-03-01 12:04:46 2021-03-01 7 20 CONTACT HOME 2021-03-01 12:05:12 2021-03-01 8 10 CONTACT LISTING 2021-03-01 12:06:12 2021-03-01 9 30 BOOKING LISTING 2021-03-01 12:06:30 2021-03-01 id page_impression_id action ts ds 100 1 BUTTON CLICK 2021-03-01 12:00:10 2021-03-01 200 1 BUTTON CLICK 2021-03-01 12:03:59 2021-03-01 300 3 BUTTON CLICK 2021-03-01 12:04:29 2021-03-01 400 5 BUTTON CLICK 2021-03-01 12:04:38 2021-03-01 500 5 OPEN MODAL 2021-03-01 12:04:56 2021-03-01 600 5 CLOSE MODAL 2021-03-01 12:05:30 2021-03-01 700 5 BUTTON CLICK 2021-03-01 12:06:29 2021-03-01"},{"location":"concepts/SQL/joins/page-impressions-analysis/#helper-functions","title":"Helper Functions","text":"<p>Some helpful date functions are: current_date, DATE_ADD(current_date, INTERVAL 1 DAY)</p>"},{"location":"concepts/SQL/joins/page-impressions-analysis/#question-1-write-a-query-to-find-which-visitor-visited-the-most-number-of-distinct-pages-yesterday-and-how-many-distinct-pages-they-visited","title":"Question 1: Write a query to find which visitor visited the most number of distinct pages yesterday and how many distinct pages they visited","text":"<p>Note: You may use ANSI SQL date macros such as subdate(current_date, 1) in your query.</p> <pre><code>-- Query to find the visitor with the most distinct pages visited yesterday\nSELECT visitor_id, COUNT(DISTINCT page_name) AS distinct_pages\nFROM page_impression\nWHERE ds = SUBDATE(CURRENT_DATE, 1)\nGROUP BY visitor_id\nORDER BY distinct_pages DESC\nLIMIT 1;\n</code></pre>"},{"location":"concepts/SQL/joins/page-impressions-analysis/#question-2-write-a-query-to-find-the-total-number-of-actions-taken-on-each-page-ie-page_name-yesterday","title":"Question 2: Write a query to find the total number of actions taken on each page (i.e. page_name) yesterday","text":"<pre><code>-- Query to find total actions per page yesterday\nSELECT pi.page_name, COUNT(ua.action) AS total_actions\nFROM page_impression pi\nLEFT JOIN user_action ua ON pi.id = ua.page_impression_id\nWHERE pi.ds = SUBDATE(CURRENT_DATE, 1)\nGROUP BY pi.page_name;\n</code></pre>"},{"location":"concepts/SQL/joins/page-impressions-analysis/#question-3-write-a-query-to-list-the-pages-ie-page_name-for-page_impressions-where-visitors-used-the-button-click-action-more-than-once-yesterday","title":"Question 3: Write a query to list the pages (i.e. page_name) for page_impressions where visitors used the \"BUTTON CLICK\" action more than once yesterday","text":"<pre><code>-- CTE to find page impressions with more than one button click\nWITH max_actions AS (\n    SELECT page_impression_id, COUNT(*) AS action_count\n    FROM user_action\n    WHERE ds = SUBDATE(CURRENT_DATE, 1)\n    AND action = 'BUTTON_CLICK'\n    GROUP BY page_impression_id\n    HAVING COUNT(*) &gt; 1\n)\n-- Query to get distinct page names for impressions with multiple button clicks\nSELECT DISTINCT pi.page_name\nFROM page_impression pi\nJOIN max_actions ma ON pi.id = ma.page_impression_id;\n</code></pre>"},{"location":"concepts/SQL/joins/page-impressions-analysis/#question-4-we-want-to-periodically-send-a-survey-to-all-super-users-super-users-are-defined-as-visitors-who-have-at-least-one-impression-every-day-over-each-of-the-last-seven-days-write-a-query-that-would-return-the-super-users-as-of-a-given-date","title":"Question 4: We want to periodically send a survey to all \"super users\". \"Super users\" are defined as visitors who have at least one impression every day over each of the last seven days. Write a query that would return the \"super users\" as of a given date","text":"<p>```sql -- Query to find super users: visitors with impressions on every day of the last 7 days SELECT visitor_id FROM page_impression WHERE ds &gt; SUBDATE(CURRENT_DATE, 7) AND ds &lt;= CURRENT_DATE GROUP BY visitor_id HAVING COUNT(DISTINCT ds) = 7;</p>"},{"location":"concepts/SQL/optimization/","title":"Optimization","text":"<ul> <li>SQL Optimization Challenges</li> </ul>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/","title":"SQL Optimization Challenges","text":"<p>This comprehensive guide focuses on query optimization, performance tuning, and efficient SQL patterns for large-scale data processing, with practical examples drawn from Netflix's streaming platform architecture.</p>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#query-performance-optimization","title":"Query Performance Optimization","text":""},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#example-efficient-dau-calculation","title":"Example Efficient DAU Calculation","text":"<p>Optimizing daily active users query for large datasets with billions of watch events:</p> <pre><code>-- Optimized version with date range filtering and proper aggregation\nSELECT\n  DATE(event_ts) AS activity_date,\n  COUNT(DISTINCT user_id) AS daily_active_users\nFROM watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '30' DAY  -- Limit date range\n  AND event_ts &lt; CURRENT_DATE\nGROUP BY DATE(event_ts)\nORDER BY activity_date;\n\n-- Alternative using approximate counting for very large datasets\nSELECT\n  DATE(event_ts) AS activity_date,\n  APPROX_COUNT_DISTINCT(user_id) AS approx_dau\nFROM watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '30' DAY\nGROUP BY DATE(event_ts);\n</code></pre>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#example-most-watched-show-analysis","title":"Example Most Watched Show Analysis","text":"<p>Finding top content efficiently with proper aggregation:</p> <pre><code>SELECT watch_date, show_id\nFROM (\n  SELECT\n    watch_date,\n    show_id,\n    SUM(watch_time_minutes) AS total_watch_time,\n    RANK() OVER (PARTITION BY watch_date ORDER BY SUM(watch_time_minutes) DESC) AS rnk\n  FROM watch_history\n  GROUP BY watch_date, show_id\n) t\nWHERE rnk = 1;\n</code></pre>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#indexing-strategy","title":"Indexing Strategy","text":""},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#example-optimizing-watch-history-queries","title":"Example Optimizing Watch History Queries","text":"<p>Key indexes for common Netflix query patterns with millions of users:</p> <pre><code>-- Composite index for user activity queries (most common access pattern)\nCREATE INDEX idx_user_date ON watch_events (user_id, event_ts);\n\n-- Covering index for content analytics (includes all columns needed)\nCREATE INDEX idx_content_metrics ON watch_events (show_id, event_ts, watch_time);\n\n-- Partial index for recent active users only (reduces index size)\nCREATE INDEX idx_recent_activity ON watch_events (user_id, event_ts)\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '90' DAY;\n\n-- Hash index for exact lookups on high-cardinality columns\nCREATE INDEX idx_user_hash ON users USING HASH (user_id);\n</code></pre>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#partitioning-strategy","title":"Partitioning Strategy","text":""},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#example-time-based-partitioning","title":"Example Time-based Partitioning","text":"<p>Optimizing for time-series queries across massive event data:</p> <pre><code>-- Partition by day for efficient date range queries\nCREATE TABLE watch_events (\n  event_id BIGINT,\n  user_id BIGINT,\n  show_id BIGINT,\n  event_ts TIMESTAMP,\n  watch_time INT,\n  device_type STRING,\n  region STRING\n)\nPARTITION BY DATE(event_ts);\n\n-- Query benefits from partition pruning automatically\nSELECT COUNT(DISTINCT user_id)\nFROM watch_events\nWHERE event_ts &gt;= '2025-01-01'\n  AND event_ts &lt; '2025-02-01';\n\n-- Subpartition by region for geo-specific queries\nPARTITION BY DATE(event_ts), region;\n</code></pre>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#aggregation-optimization","title":"Aggregation Optimization","text":""},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#example-efficient-top-n-queries","title":"Example Efficient Top-N Queries","text":"<p>Optimizing ranking queries with large datasets using proper techniques:</p> <pre><code>-- Use LIMIT with proper ordering for Top-N (prevents full sort)\nSELECT show_id, SUM(watch_time) AS total_watch_time\nFROM watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '7' DAY\nGROUP BY show_id\nORDER BY total_watch_time DESC\nLIMIT 10;\n\n-- Pre-aggregate for frequently accessed metrics (materialized view)\nCREATE MATERIALIZED VIEW daily_show_metrics AS\nSELECT\n  DATE(event_ts) AS watch_date,\n  show_id,\n  COUNT(DISTINCT user_id) AS unique_viewers,\n  SUM(watch_time) AS total_watch_time,\n  AVG(watch_time) AS avg_watch_time\nFROM watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '90' DAY\nGROUP BY DATE(event_ts), show_id;\n\n-- Refresh materialized view periodically\nREFRESH MATERIALIZED VIEW daily_show_metrics;\n</code></pre>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#join-optimization","title":"Join Optimization","text":""},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#example-optimizing-multi-table-analytics","title":"Example Optimizing Multi-Table Analytics","text":"<p>Efficient joins for cross-dimensional analysis across users, content, and regions:</p> <pre><code>-- Use covering indexes and proper join order\nSELECT\n  u.country,\n  s.genre,\n  COUNT(DISTINCT w.user_id) AS unique_users,\n  SUM(w.watch_time) AS total_watch_time\nFROM watch_events w\nJOIN users u ON w.user_id = u.user_id\nJOIN shows s ON w.show_id = s.show_id\nWHERE w.event_ts &gt;= CURRENT_DATE - INTERVAL '30' DAY\nGROUP BY u.country, s.genre;\n\n-- Consider denormalization for performance-critical queries\nCREATE TABLE user_watch_summary (\n  user_id BIGINT,\n  country STRING,\n  total_watch_time BIGINT,\n  favorite_genre STRING,\n  last_watch_date DATE,\n  PRIMARY KEY (user_id)\n);\n</code></pre>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#advanced-pattern-optimization","title":"Advanced Pattern Optimization","text":""},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#example-cte-optimization-for-complex-retention-analysis","title":"Example CTE Optimization for Complex Retention Analysis","text":"<p>Using CTEs for step-by-step optimization of complex user behavior analysis:</p> <pre><code>WITH daily_watch AS (\n  SELECT\n    user_id,\n    DATE(event_ts) AS watch_date,\n    SUM(watch_time) AS total_minutes\n  FROM watch_events\n  WHERE event_ts &gt;= CURRENT_DATE - INTERVAL '30' DAY\n  GROUP BY user_id, DATE(event_ts)\n),\nheavy_watchers AS (\n  SELECT user_id, watch_date\n  FROM daily_watch\n  WHERE total_minutes &gt; 100\n)\nSELECT COUNT(DISTINCT h1.user_id) AS retained_users\nFROM heavy_watchers h1\nJOIN heavy_watchers h2\n  ON h1.user_id = h2.user_id\n  AND h2.watch_date = DATE_ADD(h1.watch_date, INTERVAL 1 DAY);\n</code></pre>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#example-recursive-cte-for-content-hierarchy","title":"Example Recursive CTE for Content Hierarchy","text":"<p>Optimizing hierarchical queries for content organization:</p> <pre><code>WITH RECURSIVE content_hierarchy AS (\n  -- Base case: top-level content (shows/movies)\n  SELECT\n    content_id,\n    parent_id,\n    title,\n    0 AS level,\n    CAST(content_id AS VARCHAR) AS path\n  FROM content\n  WHERE parent_id IS NULL\n\n  UNION ALL\n\n  -- Recursive case: child content (seasons, episodes)\n  SELECT\n    c.content_id,\n    c.parent_id,\n    c.title,\n    ch.level + 1,\n    CONCAT(ch.path, '&gt;', c.content_id)\n  FROM content c\n  JOIN content_hierarchy ch ON c.parent_id = ch.content_id\n)\nSELECT * FROM content_hierarchy\nORDER BY path;\n</code></pre>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#example-query-execution-analysis","title":"Example Query Execution Analysis","text":"<p>Monitoring slow queries and bottlenecks in real-time:</p> <pre><code>-- Identify slow queries with execution metrics\nSELECT\n  query_id,\n  query_text,\n  execution_time_ms,\n  rows_processed,\n  bytes_scanned\nFROM query_history\nWHERE execution_time_ms &gt; 5000\nORDER BY execution_time_ms DESC;\n\n-- Analyze table access patterns and index usage\nEXPLAIN ANALYZE\nSELECT COUNT(DISTINCT user_id)\nFROM watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '1' DAY;\n\n-- Monitor query performance by user region\nSELECT\n  region,\n  AVG(execution_time_ms) AS avg_query_time,\n  COUNT(*) AS query_count\nFROM query_logs\nWHERE query_type = 'recommendation'\nGROUP BY region;\n</code></pre>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand query execution plans and optimization techniques</li> <li>Design efficient indexes for high-throughput OLAP workloads</li> <li>Implement partitioning strategies for time-series data</li> <li>Optimize joins and aggregations for complex analytics</li> <li>Monitor query performance and identify bottlenecks</li> <li>Apply advanced SQL patterns for large-scale data processing</li> </ul>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#best-practices","title":"Best Practices","text":""},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#data-distribution-sharding","title":"Data Distribution &amp; Sharding","text":"<pre><code>-- Hash-based sharding for user data distribution\nCREATE TABLE user_events (\n  user_id BIGINT,\n  event_ts TIMESTAMP,\n  event_type STRING,\n  metadata JSONB\n) PARTITION BY HASH(user_id) PARTITIONS 64;\n\n-- Range-based sharding for time-series data\nPARTITION BY RANGE(event_ts) (\n  PARTITION p202401 VALUES LESS THAN ('2024-02-01'),\n  PARTITION p202402 VALUES LESS THAN ('2024-03-01'),\n  PARTITION p202403 VALUES LESS THAN ('2024-04-01')\n);\n</code></pre>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#query-result-caching","title":"Query Result Caching","text":"<pre><code>-- Cache expensive recommendation queries\nSELECT user_id, show_id, recommendation_score\nFROM user_recommendations\nWHERE user_id = ?\n  AND generated_at &gt;= CURRENT_TIMESTAMP - INTERVAL '1' HOUR\nORDER BY recommendation_score DESC\nLIMIT 50;\n\n-- Use Redis/memcached for frequently accessed results\n</code></pre>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#connection-pooling-resource-management","title":"Connection Pooling &amp; Resource Management","text":"<pre><code>-- Connection pool configuration for high-concurrency workloads\nSET GLOBAL max_connections = 10000;\nSET GLOBAL innodb_buffer_pool_size = '64G';\n\n-- Query result streaming for large datasets\nSELECT * FROM watch_events\nWHERE event_ts &gt;= '2025-01-01'\nINTO OUTFILE '/tmp/results.csv'\nFIELDS TERMINATED BY ',';\n</code></pre>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#cross-references","title":"Cross-References","text":"<ul> <li>Performance Optimization</li> <li>SQL Learning Path</li> <li>Advanced SQL Patterns</li> <li>System Architecture</li> </ul>"},{"location":"concepts/SQL/optimization/sql-optimization-challenges/#notes","title":"Notes","text":"<ul> <li>Partitioning: Use date-based partitioning for time-series data to enable partition pruning</li> <li>Indexing: Balance between read performance and write overhead; use covering indexes for common query patterns</li> <li>Approximate functions: Use <code>APPROX_COUNT_DISTINCT()</code> for large-scale analytics when exact precision isn't required</li> <li>Materialized views: Pre-compute expensive aggregations for dashboard and reporting queries</li> <li>CTEs: Break down complex queries into logical steps for better readability and optimization</li> <li>Query optimization: Always analyze execution plans (<code>EXPLAIN ANALYZE</code>) for expensive queries</li> <li>Caching: Implement multi-level caching (query result, application, database) for frequently accessed data</li> <li>Monitoring: Set up comprehensive monitoring for query performance, resource usage, and slow query detection</li> </ul> <p>Navigate back to SQL Concepts | Concepts Overview</p>"},{"location":"concepts/SQL/problems/","title":"SQL Practice Problems Hub","text":""},{"location":"concepts/SQL/problems/#overview","title":"Overview","text":"<p>This directory serves as a comprehensive collection of SQL practice problems designed to help developers master various SQL concepts through hands-on problem-solving. The problems are organized by difficulty level and cover essential SQL topics including aggregation, joins, window functions, CTEs, subqueries, and advanced patterns.</p>"},{"location":"concepts/SQL/problems/#directory-structure","title":"Directory Structure","text":"<pre><code>problems/\n\u251c\u2500\u2500 01-basic-analytics/          # Fundamental analytics problems\n\n\u251c\u2500\u2500 02-ranking-window-functions/ # Window function ranking problems\n\n\u251c\u2500\u2500 advanced-patterns.md         # Complex CTEs and advanced techniques\n\n\u251c\u2500\u2500 optimization-challenges.md   # Performance optimization problems\n\n\u2514\u2500\u2500 README.md                    # This file\n\n</code></pre>"},{"location":"concepts/SQL/problems/#purpose","title":"Purpose","text":"<p>These problems are designed to:</p> <ul> <li>Build foundational SQL skills through progressive difficulty</li> <li>Practice real-world scenarios commonly encountered in data analysis</li> <li>Master advanced SQL concepts like window functions and CTEs</li> <li>Understand query optimization techniques for large datasets</li> <li>Prepare for technical interviews at top technology companies</li> </ul>"},{"location":"concepts/SQL/problems/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p>Beginner Level Problems</p> </li> <li> <p>Intermediate Level Problems</p> </li> <li> <p>Advanced Level Problems</p> </li> <li> <p>Expert Level Problems</p> </li> <li> <p>Related Concept Documentation</p> </li> </ul>"},{"location":"concepts/SQL/problems/#beginner-level-problems","title":"Beginner Level Problems","text":""},{"location":"concepts/SQL/problems/#basic-analytics","title":"Basic Analytics","text":"<ul> <li><code>01-basic-analytics/daily-active-users.md</code> - Calculate daily active users with COUNT DISTINCT</li> <li><code>01-basic-analytics/day-1-retention.md</code> - User retention analysis with JOINs</li> <li><code>01-basic-analytics/most-watched-show-per-day.md</code> - Find most popular content per day</li> </ul>"},{"location":"concepts/SQL/problems/#aggregation-fundamentals","title":"Aggregation Fundamentals","text":"<ul> <li><code>../aggregation/aggregate-functions.md</code> - Basic aggregation functions (SUM, AVG, COUNT)</li> <li><code>../aggregation/monthly-ratings.md</code> - Monthly average calculations with GROUP BY</li> </ul>"},{"location":"concepts/SQL/problems/#intermediate-level-problems","title":"Intermediate Level Problems","text":""},{"location":"concepts/SQL/problems/#window-functions-ranking","title":"Window Functions &amp; Ranking","text":"<ul> <li><code>02-ranking-window-functions/top-3-shows-per-region.md</code> - Top N per group using DENSE_RANK</li> <li><code>02-ranking-window-functions/consecutive-days-watching.md</code> - Consecutive day analysis patterns</li> </ul>"},{"location":"concepts/SQL/problems/#ctes-common-table-expressions","title":"CTEs (Common Table Expressions)","text":"<ul> <li><code>../cte/customer-sales-analysis.md</code> - Multi-step analysis with CTEs</li> <li><code>../cte/cte-vs-window-comparison.md</code> - CTE vs Window function comparison</li> <li><code>../cte/vacant-days-detailed.md</code> - Complex date range calculations</li> </ul>"},{"location":"concepts/SQL/problems/#joins","title":"Joins","text":"<ul> <li><code>../joins/page-impressions-analysis.md</code> - Multi-table join analysis</li> </ul>"},{"location":"concepts/SQL/problems/#advanced-level-problems","title":"Advanced Level Problems","text":""},{"location":"concepts/SQL/problems/#advanced-sql-patterns","title":"Advanced SQL Patterns","text":"<ul> <li><code>advanced-patterns.md</code> - CTEs, recursive CTEs, and complex joins</li> <li><code>../advanced/advanced-sql-patterns.md</code> - Advanced SQL techniques</li> </ul>"},{"location":"concepts/SQL/problems/#window-functions-deep-dive","title":"Window Functions Deep Dive","text":"<ul> <li><code>../window-functions/window-functions-overview.md</code> - Complete window functions guide</li> <li><code>../window-functions/postgresql-interview-examples.md</code> - Real interview problems with window functions</li> <li><code>../window-functions/postgresql-advanced-concepts.md</code> - Advanced window function concepts</li> <li><code>../window-functions/postgresql-navigation-functions.md</code> - LAG, LEAD, and navigation functions</li> <li><code>../window-functions/postgresql-ranking-functions.md</code> - Ranking and percentile functions</li> <li><code>../window-functions/window-avg-comparison.md</code> - Moving averages and comparisons</li> <li><code>../window-functions/window-avg-simplified.md</code> - Simplified window function examples</li> </ul>"},{"location":"concepts/SQL/problems/#subqueries","title":"Subqueries","text":"<ul> <li><code>../subqueries/subqueries.md</code> - Subquery patterns and techniques</li> </ul>"},{"location":"concepts/SQL/problems/#expert-level-problems","title":"Expert Level Problems","text":""},{"location":"concepts/SQL/problems/#query-optimization","title":"Query Optimization","text":"<ul> <li><code>optimization-challenges.md</code> - Database optimization techniques</li> <li><code>../optimization/sql-optimization-challenges.md</code> - Advanced optimization strategies</li> </ul>"},{"location":"concepts/SQL/problems/#related-concept-documentation","title":"Related Concept Documentation","text":"<p>For a deeper understanding of SQL concepts, explore these related guides:</p>"},{"location":"concepts/SQL/problems/#core-sql-concepts","title":"Core SQL Concepts","text":"<ul> <li><code>../../README.md</code> - SQL concepts overview and comprehensive reference</li> </ul>"},{"location":"concepts/SQL/problems/#data-modeling","title":"Data Modeling","text":"<ul> <li><code>../../Data-Modeling/README.md</code> - Data modeling principles</li> <li><code>../../Data-Modeling/01-core-entities/README.md</code> - Core entity modeling</li> <li><code>../../Data-Modeling/02-relationships-patterns/README.md</code> - Relationship patterns</li> </ul>"},{"location":"concepts/SQL/problems/#getting-started","title":"Getting Started","text":"<ol> <li>Start with Beginner problems to build foundational skills</li> <li>Progress to Intermediate to master joins and window functions</li> <li>Tackle Advanced problems to understand complex patterns</li> <li>Challenge yourself with Expert problems focusing on optimization</li> </ol> <p>Each problem includes:</p> <ul> <li>Clear problem statement</li> <li>Sample data schema</li> <li>Expected output format</li> <li>Solution with detailed explanations</li> <li>Performance considerations</li> </ul>"},{"location":"concepts/SQL/problems/#contributing","title":"Contributing","text":"<p>To add new problems:</p> <ol> <li>Create a new markdown file in the appropriate subdirectory</li> <li>Follow the existing format with problem statement, solution, and notes</li> <li>Update this README with the new problem link</li> <li>Test the solution to ensure correctness</li> </ol>"},{"location":"concepts/SQL/problems/#tips-for-success","title":"Tips for Success","text":"<ul> <li>Understand the data model before writing queries</li> <li>Think step-by-step for complex problems</li> <li>Consider performance implications of your solutions</li> <li>Practice explaining your thought process</li> <li>Review edge cases and NULL handling</li> </ul> <p>Happy querying! Mastering SQL takes practice, and these problems provide the perfect opportunity to hone your skills.</p>"},{"location":"concepts/SQL/problems/advanced-patterns/","title":"Advanced SQL Patterns","text":""},{"location":"concepts/SQL/problems/advanced-patterns/#cte-common-table-expressions","title":"CTE (Common Table Expressions)","text":""},{"location":"concepts/SQL/problems/advanced-patterns/#example-heavy-watchers-retention","title":"Example Heavy Watchers Retention","text":"<p>Find users who watched &gt;100 minutes in a day, and then find how many of them watched again the next day.</p> <pre><code>WITH daily_watch AS (\n  SELECT\n    user_id,\n    DATE(event_ts) AS watch_date,\n    SUM(watch_time) AS total_minutes\n  FROM watch_events\n  GROUP BY user_id, DATE(event_ts)\n),\nheavy_watchers AS (\n  SELECT user_id, watch_date\n  FROM daily_watch\n  WHERE total_minutes &gt; 100\n)\nSELECT COUNT(DISTINCT h1.user_id) AS retained_users\nFROM heavy_watchers h1\nJOIN heavy_watchers h2\n  ON h1.user_id = h2.user_id\n  AND h2.watch_date = DATE_ADD(h1.watch_date, INTERVAL 1 DAY);\n</code></pre>"},{"location":"concepts/SQL/problems/advanced-patterns/#recursive-ctes","title":"Recursive CTEs","text":""},{"location":"concepts/SQL/problems/advanced-patterns/#example-content-hierarchy","title":"Example Content Hierarchy","text":"<p>Model content hierarchy (seasons, episodes) using recursive CTEs:</p> <pre><code>WITH RECURSIVE content_hierarchy AS (\n  -- Base case: top-level content\n  SELECT\n    content_id,\n    parent_id,\n    title,\n    0 AS level,\n    CAST(content_id AS VARCHAR) AS path\n  FROM content\n  WHERE parent_id IS NULL\n\n  UNION ALL\n\n  -- Recursive case: child content\n  SELECT\n    c.content_id,\n    c.parent_id,\n    c.title,\n    ch.level + 1,\n    CONCAT(ch.path, '&gt;', c.content_id)\n  FROM content c\n  JOIN content_hierarchy ch ON c.parent_id = ch.content_id\n)\nSELECT * FROM content_hierarchy ORDER BY path;\n</code></pre>"},{"location":"concepts/SQL/problems/advanced-patterns/#advanced-window-functions","title":"Advanced Window Functions","text":""},{"location":"concepts/SQL/problems/advanced-patterns/#example-percentile-analysis","title":"Example Percentile Analysis","text":"<p>Find the top 1% of users by total watch time:</p> <pre><code>WITH user_totals AS (\n  SELECT\n    user_id,\n    SUM(watch_time) AS total_watch_time\n  FROM watch_events\n  GROUP BY user_id\n),\nranked AS (\n  SELECT\n    user_id,\n    total_watch_time,\n    PERCENT_RANK() OVER (ORDER BY total_watch_time DESC) AS pct_rank\n  FROM user_totals\n)\nSELECT user_id, total_watch_time\nFROM ranked\nWHERE pct_rank &lt;= 0.01;\n</code></pre>"},{"location":"concepts/SQL/problems/advanced-patterns/#complex-joins-with-filtering","title":"Complex Joins with Filtering","text":""},{"location":"concepts/SQL/problems/advanced-patterns/#example-recommendation-acceptance-rate","title":"Example Recommendation Acceptance Rate","text":"<p>Measure how many recommended shows were actually watched:</p> <pre><code>SELECT\n  COUNT(DISTINCT CASE WHEN w.user_id IS NOT NULL THEN r.user_id END) * 100.0 /\n  COUNT(DISTINCT r.user_id) AS acceptance_rate_pct\nFROM recommendations r\nLEFT JOIN watch_events w\n  ON r.user_id = w.user_id\n  AND r.show_id = w.show_id\n  AND w.event_ts BETWEEN r.recommendation_ts\n                     AND r.recommendation_ts + INTERVAL '24' HOUR;\n</code></pre>"},{"location":"concepts/SQL/problems/advanced-patterns/#notes","title":"Notes","text":"<ul> <li>CTEs improve readability and allow step-by-step problem solving</li> <li>Recursive CTEs handle hierarchical or tree-like data structures</li> <li>Advanced window functions like <code>PERCENT_RANK()</code> enable statistical analysis</li> <li>Complex joins with time-based filtering common in event analysis</li> </ul>"},{"location":"concepts/SQL/problems/optimization-challenges/","title":"SQL Optimization Challenges","text":""},{"location":"concepts/SQL/problems/optimization-challenges/#query-performance-optimization","title":"Query Performance Optimization","text":""},{"location":"concepts/SQL/problems/optimization-challenges/#example-efficient-dau-calculation","title":"Example Efficient DAU Calculation","text":"<p>Optimizing daily active users query for large datasets:</p> <pre><code>-- Optimized version with proper partitioning\nSELECT\n  DATE(event_ts) AS activity_date,\n  COUNT(DISTINCT user_id) AS daily_active_users\nFROM watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '30' DAY  -- Limit date range\n  AND event_ts &lt; CURRENT_DATE\nGROUP BY DATE(event_ts)\nORDER BY activity_date;\n\n-- Alternative using approximate counting for very large datasets\nSELECT\n  DATE(event_ts) AS activity_date,\n  APPROX_COUNT_DISTINCT(user_id) AS approx_dau\nFROM watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '30' DAY\nGROUP BY DATE(event_ts);\n</code></pre>"},{"location":"concepts/SQL/problems/optimization-challenges/#indexing-strategy","title":"Indexing Strategy","text":""},{"location":"concepts/SQL/problems/optimization-challenges/#example-optimizing-watch-history-queries","title":"Example Optimizing Watch History Queries","text":"<p>Key indexes for common Netflix query patterns:</p> <pre><code>-- Composite index for user activity queries\nCREATE INDEX idx_user_date ON watch_events (user_id, event_ts);\n\n-- Covering index for content analytics\nCREATE INDEX idx_content_metrics ON watch_events (show_id, event_ts, watch_time);\n\n-- Partial index for active users only\nCREATE INDEX idx_recent_activity ON watch_events (user_id, event_ts)\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '90' DAY;\n</code></pre>"},{"location":"concepts/SQL/problems/optimization-challenges/#partitioning-strategy","title":"Partitioning Strategy","text":""},{"location":"concepts/SQL/problems/optimization-challenges/#example-time-based-partitioning","title":"Example Time-based Partitioning","text":"<p>Optimizing for time-series queries:</p> <pre><code>-- Partition by day for efficient date range queries\nCREATE TABLE watch_events (\n  event_id BIGINT,\n  user_id BIGINT,\n  show_id BIGINT,\n  event_ts TIMESTAMP,\n  watch_time INT\n)\nPARTITION BY DATE(event_ts);\n\n-- Query benefits from partition pruning\nSELECT COUNT(DISTINCT user_id)\nFROM watch_events\nWHERE event_ts &gt;= '2025-01-01'\n  AND event_ts &lt; '2025-02-01';\n</code></pre>"},{"location":"concepts/SQL/problems/optimization-challenges/#aggregation-optimization","title":"Aggregation Optimization","text":""},{"location":"concepts/SQL/problems/optimization-challenges/#example-efficient-top-n-queries","title":"Example Efficient Top-N Queries","text":"<p>Optimizing ranking queries with large datasets:</p> <pre><code>-- Use LIMIT with proper ordering for Top-N\nSELECT show_id, SUM(watch_time) AS total_watch_time\nFROM watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '7' DAY\nGROUP BY show_id\nORDER BY total_watch_time DESC\nLIMIT 10;\n\n-- Pre-aggregate for frequently accessed metrics\nCREATE MATERIALIZED VIEW daily_show_metrics AS\nSELECT\n  DATE(event_ts) AS watch_date,\n  show_id,\n  COUNT(DISTINCT user_id) AS unique_viewers,\n  SUM(watch_time) AS total_watch_time\nFROM watch_events\nGROUP BY DATE(event_ts), show_id;\n</code></pre>"},{"location":"concepts/SQL/problems/optimization-challenges/#join-optimization","title":"Join Optimization","text":""},{"location":"concepts/SQL/problems/optimization-challenges/#example-optimizing-multi-table-analytics","title":"Example Optimizing Multi-Table Analytics","text":"<p>Efficient joins for cross-dimensional analysis:</p> <pre><code>-- Use covering indexes and proper join order\nSELECT\n  u.country,\n  s.genre,\n  COUNT(DISTINCT w.user_id) AS unique_users,\n  SUM(w.watch_time) AS total_watch_time\nFROM watch_events w\nJOIN users u ON w.user_id = u.user_id\nJOIN shows s ON w.show_id = s.show_id\nWHERE w.event_ts &gt;= CURRENT_DATE - INTERVAL '30' DAY\nGROUP BY u.country, s.genre;\n\n-- Consider denormalization for performance-critical queries\nCREATE TABLE user_watch_summary (\n  user_id BIGINT,\n  country STRING,\n  total_watch_time BIGINT,\n  favorite_genre STRING,\n  last_watch_date DATE\n);\n</code></pre>"},{"location":"concepts/SQL/problems/optimization-challenges/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"concepts/SQL/problems/optimization-challenges/#example-query-execution-analysis","title":"Example Query Execution Analysis","text":"<p>Monitoring slow queries and bottlenecks:</p> <pre><code>-- Identify slow queries\nSELECT\n  query_id,\n  query_text,\n  execution_time_ms,\n  rows_processed\nFROM query_history\nWHERE execution_time_ms &gt; 5000\nORDER BY execution_time_ms DESC;\n\n-- Analyze table access patterns\nEXPLAIN ANALYZE\nSELECT COUNT(DISTINCT user_id)\nFROM watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '1' DAY;\n</code></pre>"},{"location":"concepts/SQL/problems/optimization-challenges/#notes","title":"Notes","text":"<ul> <li>Partitioning: Use date-based partitioning for time-series data</li> <li>Indexing: Balance between read performance and write overhead</li> <li>Approximate functions: Use for large-scale analytics when exact precision isn't required</li> <li>Materialized views: Pre-compute expensive aggregations for dashboard queries</li> <li>Query optimization: Always analyze execution plans for expensive queries</li> </ul>"},{"location":"concepts/SQL/problems/01-basic-analytics/","title":"Basic Analytics Problems","text":"<p>This directory contains fundamental SQL problems that test basic analytics skills commonly asked in Netflix data engineering interviews.</p>"},{"location":"concepts/SQL/problems/01-basic-analytics/#problems","title":"Problems","text":""},{"location":"concepts/SQL/problems/01-basic-analytics/#daily-active-usersmd","title":"<code>daily-active-users.md</code>","text":"<p>Calculate daily active users (DAU) from user activity data using basic aggregation.</p>"},{"location":"concepts/SQL/problems/01-basic-analytics/#most-watched-show-per-daymd","title":"<code>most-watched-show-per-day.md</code>","text":"<p>Find the most watched show per day based on total watch time using window functions.</p>"},{"location":"concepts/SQL/problems/01-basic-analytics/#day-1-retentionmd","title":"<code>day-1-retention.md</code>","text":"<p>Calculate Day-1 user retention using joins and date arithmetic.</p>"},{"location":"concepts/SQL/problems/01-basic-analytics/#key-concepts-covered","title":"Key Concepts Covered","text":"<ul> <li>Basic Aggregation: COUNT, SUM, GROUP BY</li> <li>Date Functions: DATE operations for time-based analysis</li> <li>User Metrics: DAU, retention analysis</li> <li>Window Functions: RANK for top-N analysis</li> </ul>"},{"location":"concepts/SQL/problems/01-basic-analytics/#common-patterns","title":"Common Patterns","text":"<pre><code>-- Basic DAU calculation\nSELECT\n  activity_date,\n  COUNT(DISTINCT user_id) AS daily_active_users\nFROM user_activity\nGROUP BY activity_date;\n\n-- Retention analysis\nSELECT\n  COUNT(DISTINCT a.user_id) * 1.0 / COUNT(DISTINCT s.user_id) AS retention_rate\nFROM user_signup s\nLEFT JOIN user_activity a\n  ON s.user_id = a.user_id\n  AND a.activity_date = DATE_ADD(s.signup_date, INTERVAL 1 DAY);\n</code></pre>"},{"location":"concepts/SQL/problems/01-basic-analytics/#interview-tips","title":"Interview Tips","text":"<ul> <li>Start with basic aggregation for user counts</li> <li>Use LEFT JOIN for retention to include users who didn't return</li> <li>Consider NULL handling in percentage calculations</li> <li>Practice date arithmetic for retention periods</li> </ul>"},{"location":"concepts/SQL/problems/01-basic-analytics/daily-active-users/","title":"Daily Active Users (DAU)","text":""},{"location":"concepts/SQL/problems/01-basic-analytics/daily-active-users/#question","title":"Question","text":"<p>You have a table <code>user_activity</code>:</p> user_id activity_date activity_type 101 2025-08-01 play 101 2025-08-01 pause 102 2025-08-01 play 103 2025-08-02 play <p>Write a query to find the number of unique active users per day.</p>"},{"location":"concepts/SQL/problems/01-basic-analytics/daily-active-users/#answer","title":"Answer","text":"<pre><code>SELECT\n  activity_date,\n  COUNT(DISTINCT user_id) AS daily_active_users\nFROM user_activity\nGROUP BY activity_date\nORDER BY activity_date;\n</code></pre>"},{"location":"concepts/SQL/problems/01-basic-analytics/daily-active-users/#notes","title":"Notes","text":"<ul> <li>Groups by day and counts distinct users</li> <li>Basic aggregation pattern for DAU calculation</li> <li>Common Netflix interview question for analytics basics</li> </ul>"},{"location":"concepts/SQL/problems/01-basic-analytics/day-1-retention/","title":"Day-1 Retention","text":""},{"location":"concepts/SQL/problems/01-basic-analytics/day-1-retention/#question","title":"Question","text":"<p>From <code>user_signup(user_id, signup_date)</code> and <code>user_activity(user_id, activity_date)</code>, calculate Day-1 retention (users who signed up on Day X and came back on Day X+1).</p>"},{"location":"concepts/SQL/problems/01-basic-analytics/day-1-retention/#answer","title":"Answer","text":"<pre><code>SELECT\n  s.signup_date,\n  COUNT(DISTINCT a.user_id) * 1.0 / COUNT(DISTINCT s.user_id) AS day1_retention\nFROM user_signup s\nLEFT JOIN user_activity a\n  ON s.user_id = a.user_id\n  AND a.activity_date = DATE_ADD(s.signup_date, INTERVAL 1 DAY)\nGROUP BY s.signup_date;\n</code></pre>"},{"location":"concepts/SQL/problems/01-basic-analytics/day-1-retention/#notes","title":"Notes","text":"<ul> <li>Numerator: users active next day after signup</li> <li>Denominator: total signups for that day</li> <li>Uses <code>LEFT JOIN</code> with date filtering for the retention logic</li> <li>Common cohort analysis pattern in user analytics</li> </ul>"},{"location":"concepts/SQL/problems/01-basic-analytics/most-watched-show-per-day/","title":"Most Watched Show per Day","text":""},{"location":"concepts/SQL/problems/01-basic-analytics/most-watched-show-per-day/#question","title":"Question","text":"<p>Table <code>watch_history</code>:</p> user_id show_id watch_date watch_time_minutes 1 A 2025-08-01 30 2 A 2025-08-01 50 3 B 2025-08-01 80 <p>Find the most watched show per day (based on total minutes).</p>"},{"location":"concepts/SQL/problems/01-basic-analytics/most-watched-show-per-day/#answer","title":"Answer","text":"<pre><code>SELECT watch_date, show_id\nFROM (\n  SELECT\n    watch_date,\n    show_id,\n    SUM(watch_time_minutes) AS total_watch_time,\n    RANK() OVER (PARTITION BY watch_date ORDER BY SUM(watch_time_minutes) DESC) AS rnk\n  FROM watch_history\n  GROUP BY watch_date, show_id\n) t\nWHERE rnk = 1;\n</code></pre>"},{"location":"concepts/SQL/problems/01-basic-analytics/most-watched-show-per-day/#notes","title":"Notes","text":"<ul> <li>Uses window function <code>RANK()</code> to find the top show per day</li> <li>Groups by date and show first, then ranks within each date</li> <li>Basic ranking pattern for top-N per category problems</li> </ul>"},{"location":"concepts/SQL/problems/02-ranking-window-functions/","title":"Ranking Window Functions Problems","text":"<p>This directory contains intermediate-level SQL problems that focus on window functions and ranking operations commonly asked in Netflix data engineering interviews.</p>"},{"location":"concepts/SQL/problems/02-ranking-window-functions/#navigation","title":"Navigation","text":"<ul> <li>Back to SQL Problems</li> <li>Back to SQL Concepts</li> </ul>"},{"location":"concepts/SQL/problems/02-ranking-window-functions/#problems","title":"Problems","text":""},{"location":"concepts/SQL/problems/02-ranking-window-functions/#top-3-shows-per-regionmd","title":"<code>top-3-shows-per-region.md</code>","text":"<p>Find the top 3 most watched shows per region using DENSE_RANK() window function for ranking within partitions.</p>"},{"location":"concepts/SQL/problems/02-ranking-window-functions/#consecutive-days-watchingmd","title":"<code>consecutive-days-watching.md</code>","text":"<p>Identify users who have watched content for 3 consecutive days using LAG() window function and date arithmetic.</p>"},{"location":"concepts/SQL/problems/02-ranking-window-functions/#key-concepts-covered","title":"Key Concepts Covered","text":"<ul> <li>Window Functions: PARTITION BY, ORDER BY in OVER clauses</li> <li>Ranking Functions: DENSE_RANK() vs RANK() for top-N analysis</li> <li>Navigation Functions: LAG() for accessing previous rows</li> <li>Date Arithmetic: DATEDIFF() for consecutive day calculations</li> <li>Partitioning: Grouping data for per-category analysis</li> </ul>"},{"location":"concepts/SQL/problems/02-ranking-window-functions/#common-patterns","title":"Common Patterns","text":"<pre><code>-- Top N per group using DENSE_RANK\nSELECT category, item, metric\nFROM (\n  SELECT\n    category,\n    item,\n    SUM(metric) AS total_metric,\n    DENSE_RANK() OVER (PARTITION BY category ORDER BY SUM(metric) DESC) AS rnk\n  FROM data_table\n  GROUP BY category, item\n) t\nWHERE rnk &lt;= 3;\n\n-- Consecutive days pattern with LAG\nSELECT DISTINCT user_id\nFROM (\n  SELECT\n    user_id,\n    activity_date,\n    LAG(activity_date, 1) OVER (PARTITION BY user_id ORDER BY activity_date) AS prev_day,\n    LAG(activity_date, 2) OVER (PARTITION BY user_id ORDER BY activity_date) AS prev2_day\n  FROM user_activity\n) t\nWHERE DATEDIFF(activity_date, prev_day) = 1\n  AND DATEDIFF(prev_day, prev2_day) = 1;\n</code></pre>"},{"location":"concepts/SQL/problems/02-ranking-window-functions/#interview-tips","title":"Interview Tips","text":"<ul> <li>Choose DENSE_RANK() over RANK() when you need consecutive ranking without gaps</li> <li>Use PARTITION BY to create separate ranking contexts for each group</li> <li>Combine aggregation with window functions by wrapping in subqueries</li> <li>Consider date boundaries and NULL handling in consecutive day problems</li> <li>Practice both ranking and navigation window functions for comprehensive coverage</li> </ul>"},{"location":"concepts/SQL/problems/02-ranking-window-functions/consecutive-days-watching/","title":"Consecutive Days Watching","text":""},{"location":"concepts/SQL/problems/02-ranking-window-functions/consecutive-days-watching/#question","title":"Question","text":"<p>From <code>user_activity(user_id, activity_date)</code>, find users who have watched content for 3 consecutive days.</p>"},{"location":"concepts/SQL/problems/02-ranking-window-functions/consecutive-days-watching/#answer","title":"Answer","text":"<pre><code>SELECT DISTINCT user_id\nFROM (\n  SELECT\n    user_id,\n    activity_date,\n    LAG(activity_date,1) OVER (PARTITION BY user_id ORDER BY activity_date) AS prev_day,\n    LAG(activity_date,2) OVER (PARTITION BY user_id ORDER BY activity_date) AS prev2_day\n  FROM user_activity\n) t\nWHERE DATEDIFF(activity_date, prev_day) = 1\n  AND DATEDIFF(prev_day, prev2_day) = 1;\n</code></pre>"},{"location":"concepts/SQL/problems/02-ranking-window-functions/consecutive-days-watching/#notes","title":"Notes","text":"<ul> <li>Uses <code>LAG()</code> window function to look back at previous activity dates</li> <li><code>DATEDIFF()</code> checks for consecutive days (difference = 1)</li> <li>Alternative approach: use <code>ROW_NUMBER()</code> and date arithmetic for more complex streak detection</li> <li>Common pattern for \"consecutive days\" or \"streak\" problems</li> </ul>"},{"location":"concepts/SQL/problems/02-ranking-window-functions/top-3-shows-per-region/","title":"Top 3 Shows per Region","text":""},{"location":"concepts/SQL/problems/02-ranking-window-functions/top-3-shows-per-region/#question","title":"Question","text":"<p>Table <code>viewership(user_id, show_id, region, watch_time)</code>. Find the top 3 most watched shows per region by total minutes.</p>"},{"location":"concepts/SQL/problems/02-ranking-window-functions/top-3-shows-per-region/#answer","title":"Answer","text":"<pre><code>SELECT region, show_id, total_watch_time\nFROM (\n  SELECT\n    region,\n    show_id,\n    SUM(watch_time) AS total_watch_time,\n    DENSE_RANK() OVER (PARTITION BY region ORDER BY SUM(watch_time) DESC) AS rnk\n  FROM viewership\n  GROUP BY region, show_id\n) t\nWHERE rnk &lt;= 3;\n</code></pre>"},{"location":"concepts/SQL/problems/02-ranking-window-functions/top-3-shows-per-region/#notes","title":"Notes","text":"<ul> <li>Uses <code>DENSE_RANK()</code> for ranking within each region</li> <li><code>DENSE_RANK()</code> vs <code>RANK()</code>: no gaps in ranking (1,2,2,3 vs 1,2,2,4)</li> <li>Groups by region and show first, then applies ranking window function</li> <li>Common pattern for \"top N per category\" problems</li> </ul>"},{"location":"concepts/SQL/subqueries/","title":"SQL Subqueries","text":"<p>This directory contains SQL subquery concepts and examples.</p>"},{"location":"concepts/SQL/subqueries/#files","title":"Files","text":"<ul> <li><code>subqueries.md</code> - SQL subquery patterns and techniques</li> </ul>"},{"location":"concepts/SQL/subqueries/subqueries/","title":"\ud83d\udd39 Subqueries","text":""},{"location":"concepts/SQL/subqueries/subqueries/#definition","title":"\u2705 Definition","text":"<p>A subquery is a query nested inside another query. It can return scalar, single-row, or multi-row results.</p> <p>Types:</p> <ul> <li>Scalar subquery: Returns one value</li> <li>Correlated subquery: Refers to outer query</li> <li>IN/EXISTS subquery: Checks for inclusion</li> </ul>"},{"location":"concepts/SQL/subqueries/subqueries/#syntax","title":"\ud83e\udde9 Syntax","text":"<pre><code>SELECT ...\nFROM ...\nWHERE column IN (\n    SELECT column FROM ...\n);\n</code></pre>"},{"location":"concepts/SQL/subqueries/subqueries/#examples","title":"\ud83d\udcd8 Examples","text":""},{"location":"concepts/SQL/subqueries/subqueries/#a-scalar-subquery","title":"a. Scalar subquery","text":"<p>Get employees who earn more than the average salary:</p> <pre><code>SELECT name, salary\nFROM employees\nWHERE salary &gt; (SELECT AVG(salary) FROM employees);\n</code></pre>"},{"location":"concepts/SQL/subqueries/subqueries/#b-correlated-subquery","title":"b. Correlated subquery","text":"<p>Get employees who earn more than the average in their department:</p> <pre><code>SELECT e1.name, e1.salary\nFROM employees e1\nWHERE e1.salary &gt; (\n    SELECT AVG(e2.salary)\n    FROM employees e2\n    WHERE e2.department_id = e1.department_id\n);\n</code></pre>"},{"location":"concepts/SQL/subqueries/subqueries/#c-exists","title":"c. EXISTS","text":"<p>List departments that have employees:</p> <p>```sql SELECT d.department_id, d.name FROM departments d WHERE EXISTS (     SELECT 1     FROM employees e     WHERE e.department_id = d.department_id );</p>"},{"location":"concepts/SQL/window-functions/","title":"PostgreSQL Window Functions","text":"<p>Window functions perform calculations across related rows while preserving individual row data. Use <code>OVER()</code> clause to define window specifications.</p> <pre><code>function_name(expression) OVER (\n    [PARTITION BY partition_expression]\n    [ORDER BY sort_expression]\n    [frame_clause]\n)\n</code></pre>"},{"location":"concepts/SQL/window-functions/#files","title":"Files","text":""},{"location":"concepts/SQL/window-functions/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>window-functions-overview.md</code> - Syntax, categories, examples</li> <li><code>postgresql-advanced-concepts.md</code> - Frames, named windows, optimization</li> </ul>"},{"location":"concepts/SQL/window-functions/#function-categories","title":"Function Categories","text":"<ul> <li><code>postgresql-ranking-functions.md</code> - ROW_NUMBER(), RANK(), DENSE_RANK(), PERCENT_RANK(), NTILE()</li> <li><code>postgresql-navigation-functions.md</code> - LAG(), LEAD(), FIRST_VALUE(), LAST_VALUE()</li> </ul>"},{"location":"concepts/SQL/window-functions/#advanced-applications","title":"Advanced Applications","text":"<ul> <li><code>postgresql-interview-examples.md</code> - Interview problems from Netflix, Amazon, Google</li> <li><code>window-avg-comparison.md</code> - Advanced AVG comparisons</li> <li><code>window-avg-simplified.md</code> - PostgreSQL-optimized AVG solutions</li> </ul>"},{"location":"concepts/SQL/window-functions/#learning-path","title":"Learning Path","text":"<ol> <li><code>window-functions-overview.md</code> - Basics</li> <li><code>postgresql-ranking-functions.md</code> - Ranking</li> <li><code>postgresql-navigation-functions.md</code> - Navigation</li> <li><code>postgresql-advanced-concepts.md</code> - Advanced concepts</li> <li>Choose: <code>window-avg-comparison.md</code> or <code>window-avg-simplified.md</code></li> <li><code>postgresql-interview-examples.md</code> - Real-world problems</li> </ol>"},{"location":"concepts/SQL/window-functions/#when-to-use","title":"When to Use","text":"<ul> <li>Running totals and moving averages</li> <li>Ranking within groups</li> <li>Time-series analysis</li> <li>Comparing rows to aggregates</li> <li>Complex analytical reporting</li> </ul>"},{"location":"concepts/SQL/window-functions/#quick-examples","title":"Quick Examples","text":"<pre><code>-- Department average\nAVG(salary) OVER (PARTITION BY department)\n\n-- Top 3 per category\nROW_NUMBER() OVER (PARTITION BY category ORDER BY sales DESC)\n\n-- Month-over-month growth\nLAG(revenue) OVER (ORDER BY month)\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/","title":"PostgreSQL Advanced Window Functions - Complete Guide","text":""},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#overview","title":"\ud83d\udd39 Overview","text":"<p>This guide covers advanced PostgreSQL window function concepts including frame specifications, named windows, performance optimization, and complex analytical patterns.</p>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#window-frame-specifications","title":"\ud83d\uddbc\ufe0f Window Frame Specifications","text":""},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#understanding-window-frames","title":"Understanding Window Frames","text":"<p>Window frames define which rows are included in the calculation for each row. PostgreSQL supports three frame types:</p>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#rows-physical-row-based-frames","title":"ROWS - Physical Row-Based Frames","text":"<pre><code>ROWS frame_extent\n</code></pre> <p>Frame Extents::</p> <ul> <li><code>UNBOUNDED PRECEDING</code> - All rows from partition start to current row</li> <li><code>n PRECEDING</code> - n rows before current row</li> <li><code>CURRENT ROW</code> - Current row only</li> <li><code>n FOLLOWING</code> - n rows after current row</li> <li><code>UNBOUNDED FOLLOWING</code> - All rows from current row to partition end</li> </ul> <p>Examples::</p> <pre><code>-- Last 3 rows including current\nROWS 2 PRECEDING\n\n-- Current row and next 2\nROWS BETWEEN CURRENT ROW AND 2 FOLLOWING\n\n-- All rows from start to end (complete partition)\nROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#range-logical-value-based-frames","title":"RANGE - Logical Value-Based Frames","text":"<pre><code>RANGE frame_extent\n</code></pre> <p>Frame Extents (with ORDER BY required)::</p> <ul> <li><code>UNBOUNDED PRECEDING</code> - All rows with values &lt;= current row's value</li> <li><code>n PRECEDING</code> - Rows with values in range [current - n, current]</li> <li><code>CURRENT ROW</code> - Rows with same value as current row</li> <li><code>n FOLLOWING</code> - Rows with values in range [current, current + n]</li> <li><code>UNBOUNDED FOLLOWING</code> - All rows with values &gt;= current row's value</li> </ul> <p>Examples::</p> <pre><code>-- All rows with same or smaller ORDER BY value\nRANGE UNBOUNDED PRECEDING\n\n-- Rows within 5 units of current value\nRANGE BETWEEN 5 PRECEDING AND 5 FOLLOWING\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#groups-group-based-frames","title":"GROUPS - Group-Based Frames","text":"<pre><code>GROUPS frame_extent\n</code></pre> <p>Frame Extents::</p> <ul> <li><code>UNBOUNDED PRECEDING</code> - All peer groups from start to current</li> <li><code>n PRECEDING</code> - n peer groups before current group</li> <li><code>CURRENT ROW</code> - Current peer group</li> <li><code>n FOLLOWING</code> - n peer groups after current group</li> <li><code>UNBOUNDED FOLLOWING</code> - All peer groups from current to end</li> </ul>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#default-frame-behavior","title":"Default Frame Behavior","text":"<p>When no frame is specified:</p> <ul> <li>With ORDER BY: <code>RANGE UNBOUNDED PRECEDING</code></li> <li>Without ORDER BY: <code>ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING</code></li> </ul>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#named-window-specifications","title":"\ud83c\udff7\ufe0f Named Window Specifications","text":""},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#basic-named-windows","title":"Basic Named Windows","text":"<p>Use the <code>WINDOW</code> clause to define reusable window specifications:</p> <pre><code>SELECT\n    col1,\n    col2,\n    function1() OVER my_window,\n    function2() OVER my_window\nFROM table1\nWINDOW my_window AS (PARTITION BY col1 ORDER BY col2);\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#advanced-named-window-techniques","title":"Advanced Named Window Techniques","text":""},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#window-inheritance","title":"Window Inheritance","text":"<pre><code>SELECT\n    col1, col2, col3,\n    SUM(col3) OVER base_window,\n    AVG(col3) OVER (base_window ROWS 2 PRECEDING),\n    MAX(col3) OVER (base_window ORDER BY col1)\nFROM table1\nWINDOW base_window AS (PARTITION BY col1 ORDER BY col2);\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#complex-window-combinations","title":"Complex Window Combinations","text":"<pre><code>SELECT\n    employee_id,\n    department,\n    salary,\n    hire_date,\n\n    -- Department ranking\n    ROW_NUMBER() OVER dept_window AS dept_rank,\n\n    -- Department salary percentile\n    PERCENT_RANK() OVER dept_window AS dept_percentile,\n\n    -- Company-wide ranking\n    ROW_NUMBER() OVER company_window AS company_rank,\n\n    -- Moving average in department\n    AVG(salary) OVER (dept_window ROWS 2 PRECEDING) AS dept_moving_avg,\n\n    -- Cumulative salary by hire date\n    SUM(salary) OVER (hire_window ROWS UNBOUNDED PRECEDING) AS cumulative_salary\n\nFROM employees\nWINDOW\n    dept_window AS (PARTITION BY department ORDER BY salary DESC),\n    company_window AS (ORDER BY salary DESC),\n    hire_window AS (ORDER BY hire_date);\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#advanced-interview-examples","title":"\ud83c\udfaf Advanced Interview Examples","text":""},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#example-1-complex-sales-analysis","title":"Example 1 Complex Sales Analysis","text":"<pre><code>CREATE TABLE sales_data (\n    sale_id INT,\n    product_id INT,\n    store_id INT,\n    sale_date DATE,\n    quantity INT,\n    price DECIMAL(10,2)\n);\n\nINSERT INTO sales_data VALUES\n(1, 1, 1, '2024-01-01', 10, 100),\n(2, 1, 1, '2024-01-02', 15, 100),\n(3, 1, 2, '2024-01-01', 8, 100),\n(4, 2, 1, '2024-01-01', 20, 50),\n(5, 2, 2, '2024-01-02', 25, 50);\n\nSELECT\n    sale_date,\n    product_id,\n    store_id,\n    quantity,\n    price,\n    quantity * price AS revenue,\n\n    -- Product's daily share within store\n    ROUND(\n        (quantity * price) /\n        SUM(quantity * price) OVER (\n            PARTITION BY store_id, sale_date\n        ) * 100, 2\n    ) AS store_daily_share_pct,\n\n    -- Product's performance vs previous day (same store)\n    quantity - LAG(quantity) OVER (\n        PARTITION BY product_id, store_id\n        ORDER BY sale_date\n    ) AS qty_change_from_prev_day,\n\n    -- 7-day moving average (same product, same store)\n    ROUND(\n        AVG(quantity) OVER (\n            PARTITION BY product_id, store_id\n            ORDER BY sale_date\n            ROWS 6 PRECEDING\n        ), 2\n    ) AS moving_avg_7d,\n\n    -- Rank within day across all stores\n    ROW_NUMBER() OVER (\n        PARTITION BY sale_date\n        ORDER BY quantity * price DESC\n    ) AS daily_rank,\n\n    -- Cumulative revenue by product across all stores\n    SUM(quantity * price) OVER (\n        PARTITION BY product_id\n        ORDER BY sale_date\n        ROWS UNBOUNDED PRECEDING\n    ) AS cumulative_product_revenue,\n\n    -- Best performing store for this product so far\n    FIRST_VALUE(store_id) OVER (\n        PARTITION BY product_id\n        ORDER BY quantity * price DESC\n        ROWS UNBOUNDED PRECEDING\n    ) AS best_store_so_far\n\nFROM sales_data\nORDER BY product_id, sale_date, store_id;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#example-2-financial-time-series-analysis","title":"Example 2 Financial Time Series Analysis","text":"<pre><code>CREATE TABLE stock_prices (\n    symbol VARCHAR(10),\n    trade_date DATE,\n    price DECIMAL(10,2),\n    volume INT\n);\n\n-- Advanced financial metrics\nSELECT\n    symbol,\n    trade_date,\n    price,\n    volume,\n\n    -- Price momentum (vs 5 days ago)\n    ROUND(\n        (price - LAG(price, 5) OVER (\n            PARTITION BY symbol\n            ORDER BY trade_date\n        )) / LAG(price, 5) OVER (\n            PARTITION BY symbol\n            ORDER BY trade_date\n        ) * 100, 2\n    ) AS momentum_5d_pct,\n\n    -- 20-day moving average\n    ROUND(\n        AVG(price) OVER (\n            PARTITION BY symbol\n            ORDER BY trade_date\n            ROWS 19 PRECEDING\n        ), 2\n    ) AS ma_20d,\n\n    -- Bollinger Band (simplified)\n    ROUND(\n        AVG(price) OVER (\n            PARTITION BY symbol\n            ORDER BY trade_date\n            ROWS 19 PRECEDING\n        ) + 2 * STDDEV(price) OVER (\n            PARTITION BY symbol\n            ORDER BY trade_date\n            ROWS 19 PRECEDING\n        ), 2\n    ) AS bollinger_upper,\n\n    -- Volume weighted average price (VWAP)\n    ROUND(\n        SUM(price * volume) OVER (\n            PARTITION BY symbol\n            ORDER BY trade_date\n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        ) / SUM(volume) OVER (\n            PARTITION BY symbol\n            ORDER BY trade_date\n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        ), 2\n    ) AS vwap,\n\n    -- Relative Strength Index (simplified)\n    CASE\n        WHEN ROW_NUMBER() OVER (\n            PARTITION BY symbol\n            ORDER BY trade_date\n        ) &gt;= 14 THEN\n            ROUND(\n                100 - 100 / (1 + (\n                    AVG(CASE WHEN price &gt; LAG(price) OVER (\n                        PARTITION BY symbol ORDER BY trade_date\n                    ) THEN price - LAG(price) OVER (\n                        PARTITION BY symbol ORDER BY trade_date\n                    ) END) OVER (\n                        PARTITION BY symbol\n                        ORDER BY trade_date\n                        ROWS 13 PRECEDING\n                    ) /\n                    AVG(CASE WHEN price &lt; LAG(price) OVER (\n                        PARTITION BY symbol ORDER BY trade_date\n                    ) THEN LAG(price) OVER (\n                        PARTITION BY symbol ORDER BY trade_date\n                    ) - price END) OVER (\n                        PARTITION BY symbol\n                        ORDER BY trade_date\n                        ROWS 13 PRECEDING\n                    )\n                )), 2\n            )\n    END AS rsi_14\n\nFROM stock_prices\nORDER BY symbol, trade_date;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#example-3-customer-behavior-analysis","title":"Example 3 Customer Behavior Analysis","text":"<pre><code>CREATE TABLE customer_events (\n    customer_id INT,\n    event_date DATE,\n    event_type VARCHAR(20),\n    event_value DECIMAL(10,2)\n);\n\n-- Advanced customer analytics\nWITH customer_metrics AS (\n    SELECT\n        customer_id,\n        event_date,\n        event_type,\n        event_value,\n\n        -- Days since first event\n        event_date - MIN(event_date) OVER (\n            PARTITION BY customer_id\n        ) AS days_since_first_event,\n\n        -- Days since previous event\n        event_date - LAG(event_date) OVER (\n            PARTITION BY customer_id\n            ORDER BY event_date\n        ) AS days_since_prev_event,\n\n        -- Event sequence number\n        ROW_NUMBER() OVER (\n            PARTITION BY customer_id\n            ORDER BY event_date\n        ) AS event_sequence,\n\n        -- Cumulative value by customer\n        SUM(event_value) OVER (\n            PARTITION BY customer_id\n            ORDER BY event_date\n            ROWS UNBOUNDED PRECEDING\n        ) AS cumulative_value,\n\n        -- Rolling 30-day activity\n        COUNT(*) OVER (\n            PARTITION BY customer_id\n            ORDER BY event_date\n            RANGE BETWEEN INTERVAL '30 days' PRECEDING AND CURRENT ROW\n        ) AS activity_30d,\n\n        -- Customer lifetime value percentile\n        PERCENT_RANK() OVER (\n            ORDER BY SUM(event_value) OVER (\n                PARTITION BY customer_id\n                ORDER BY event_date\n                ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n            )\n        ) AS lifetime_value_percentile\n\n    FROM customer_events\n)\nSELECT\n    customer_id,\n    event_date,\n    event_type,\n    event_value,\n    days_since_first_event,\n    days_since_prev_event,\n    event_sequence,\n    cumulative_value,\n    activity_30d,\n    lifetime_value_percentile,\n\n    -- Customer segment based on activity\n    CASE\n        WHEN activity_30d &gt;= 10 THEN 'High Activity'\n        WHEN activity_30d &gt;= 5 THEN 'Medium Activity'\n        ELSE 'Low Activity'\n    END AS activity_segment,\n\n    -- Value segment based on percentile\n    CASE\n        WHEN lifetime_value_percentile &gt;= 0.8 THEN 'Top 20%'\n        WHEN lifetime_value_percentile &gt;= 0.6 THEN '60-80%'\n        ELSE 'Bottom 60%'\n    END AS value_segment\n\nFROM customer_metrics\nORDER BY customer_id, event_date;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#performance-optimization","title":"\u26a1 Performance Optimization","text":""},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#index-strategy-for-window-functions","title":"Index Strategy for Window Functions","text":""},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#optimal-indexing-patterns","title":"Optimal Indexing Patterns","text":"<pre><code>-- For PARTITION BY + ORDER BY\nCREATE INDEX idx_table_partition_order ON table_name (partition_col, order_col);\n\n-- For time-based windows\nCREATE INDEX idx_table_time ON table_name (partition_col, timestamp_col);\n\n-- For range frames\nCREATE INDEX idx_table_range ON table_name (partition_col, numeric_col);\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#performance-best-practices","title":"Performance Best Practices","text":"<ol> <li>Choose Appropriate Frame Types:</li> <li>Use <code>ROWS</code> for physical row navigation</li> <li>Use <code>RANGE</code> for value-based calculations</li> <li> <p>Use <code>GROUPS</code> for peer group analysis</p> </li> <li> <p>Minimize Frame Size:</p> </li> <li>Explicit frames perform better than defaults</li> <li> <p>Smaller frames use less memory and CPU</p> </li> <li> <p>Partition Strategy:</p> </li> <li>Smaller partitions = better performance</li> <li> <p>Consider partition pruning with proper indexes</p> </li> <li> <p>Memory Management:</p> </li> <li>Monitor work_mem for large result sets</li> <li>Consider temp_file_limit for disk spilling</li> </ol>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#postgresql-specific-optimizations","title":"PostgreSQL-Specific Optimizations","text":"<pre><code>-- Enable parallel processing (if beneficial)\nSET max_parallel_workers_per_gather = 4;\n\n-- Monitor window function performance\nEXPLAIN (ANALYZE, BUFFERS)\nSELECT\n    id,\n    SUM(value) OVER (ORDER BY date ROWS UNBOUNDED PRECEDING)\nFROM large_table;\n\n-- Use appropriate work memory\nSET work_mem = '256MB';\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#troubleshooting-common-issues","title":"\ud83d\udd0d Troubleshooting Common Issues","text":""},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#issue-1-unexpected-results-with-default-frames","title":"Issue 1 Unexpected Results with Default Frames","text":"<pre><code>-- Problem: LAST_VALUE returns wrong result\nSELECT id, value, LAST_VALUE(value) OVER (ORDER BY id) FROM table1;\n\n-- Solution: Specify explicit frame\nSELECT id, value, LAST_VALUE(value) OVER (\n    ORDER BY id\n    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n) FROM table1;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#issue-2-performance-degradation","title":"Issue 2 Performance Degradation","text":"<pre><code>-- Problem: Slow query with large partitions\nSELECT * FROM (\n    SELECT *, ROW_NUMBER() OVER (PARTITION BY large_partition ORDER BY col) rn\n    FROM huge_table\n) t WHERE rn &lt;= 10;\n\n-- Solution: Filter before window function\nSELECT * FROM (\n    SELECT *, ROW_NUMBER() OVER (ORDER BY col) rn\n    FROM (SELECT * FROM huge_table WHERE large_partition = 'target' LIMIT 1000) t\n) t WHERE rn &lt;= 10;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#issue-3-memory-issues","title":"Issue 3 Memory Issues","text":"<pre><code>-- Problem: Out of memory with large frames\nSELECT id, SUM(value) OVER (ORDER BY date) FROM large_table;\n\n-- Solution: Use smaller frames or increase work_mem\nSET work_mem = '1GB';\nSELECT id, SUM(value) OVER (\n    ORDER BY date\n    ROWS 1000 PRECEDING\n) FROM large_table;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#advanced-patterns","title":"\ud83c\udfaf Advanced Patterns","text":""},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#pattern-1-complex-aggregations","title":"Pattern 1 Complex Aggregations","text":"<pre><code>SELECT\n    category,\n    subcategory,\n    value,\n    -- Category total\n    SUM(value) OVER (category_window) AS category_total,\n    -- Category average\n    AVG(value) OVER (category_window) AS category_avg,\n    -- Category rank\n    ROW_NUMBER() OVER (category_window) AS category_rank,\n    -- Subcategory contribution to category\n    ROUND(value / SUM(value) OVER (category_window) * 100, 2) AS category_contribution_pct,\n    -- Global rank within category\n    ROW_NUMBER() OVER (ORDER BY value DESC) AS global_rank\nFROM data_table\nWINDOW category_window AS (PARTITION BY category);\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#pattern-2-time-series-with-gaps","title":"Pattern 2 Time Series with Gaps","text":"<pre><code>SELECT\n    date,\n    value,\n    -- Fill gaps using last known value\n    COALESCE(\n        value,\n        LAST_VALUE(value) IGNORE NULLS OVER (\n            ORDER BY date\n            ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING\n        )\n    ) AS filled_value,\n    -- Moving average excluding NULLs\n    AVG(value) IGNORE NULLS OVER (\n        ORDER BY date\n        ROWS 6 PRECEDING\n    ) AS moving_avg_ignore_nulls\nFROM time_series;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#pattern-3-sessionization","title":"Pattern 3 Sessionization","text":"<pre><code>SELECT\n    user_id,\n    event_time,\n    event_type,\n    -- Session ID based on 30-minute gaps\n    SUM(\n        CASE WHEN EXTRACT(EPOCH FROM (\n            event_time - LAG(event_time) OVER (\n                PARTITION BY user_id ORDER BY event_time\n            )\n        )) / 60 &gt; 30 THEN 1 ELSE 0 END\n    ) OVER (PARTITION BY user_id ORDER BY event_time) AS session_id,\n    -- Events in current session\n    COUNT(*) OVER (\n        PARTITION BY user_id\n        ORDER BY event_time\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ) - COALESCE(\n        SUM(\n            CASE WHEN EXTRACT(EPOCH FROM (\n                event_time - LAG(event_time) OVER (\n                    PARTITION BY user_id ORDER BY event_time\n                )\n            )) / 60 &gt; 30 THEN COUNT(*) OVER (\n                PARTITION BY user_id\n                ORDER BY event_time\n                ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n            ) END\n        ) OVER (PARTITION BY user_id ORDER BY event_time), 0\n    ) AS session_event_count\nFROM user_events;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-advanced-concepts/#related-topics","title":"\ud83d\udcda Related Topics","text":"<ul> <li><code>concepts/SQL/window-functions/window-functions-overview.md</code> - Complete window functions guide</li> <li><code>concepts/SQL/window-functions/postgresql-ranking-functions.md</code> - Ranking functions</li> <li><code>concepts/SQL/window-functions/postgresql-navigation-functions.md</code> - Navigation functions</li> </ul>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/","title":"PostgreSQL Window Functions - Interview Examples","text":""},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#overview","title":"\ud83d\udd39 Overview","text":"<p>This guide provides practical PostgreSQL window function examples commonly encountered in technical interviews at top companies like Google, Amazon, Meta, and Netflix.</p>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#common-interview-problem-patterns","title":"\ud83d\udcca Common Interview Problem Patterns","text":""},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#pattern-1-top-n-per-group","title":"Pattern 1 Top N per Group","text":"<p>Problem: Find the top 3 highest-paid employees in each department.</p> <pre><code>-- Solution using ROW_NUMBER()\nSELECT department, name, salary\nFROM (\n    SELECT\n        department,\n        name,\n        salary,\n        ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS rn\n    FROM employees\n) ranked\nWHERE rn &lt;= 3\nORDER BY department, salary DESC;\n\n-- Alternative using RANK() (handles ties differently)\nSELECT department, name, salary\nFROM (\n    SELECT\n        department,\n        name,\n        salary,\n        RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS rank\n    FROM employees\n) ranked\nWHERE rank &lt;= 3\nORDER BY department, rank, name;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#pattern-2-running-totals-and-cumulative-sums","title":"Pattern 2 Running Totals and Cumulative Sums","text":"<p>Problem: Calculate running total of sales by month for each product.</p> <pre><code>SELECT\n    product_id,\n    sale_date,\n    daily_sales,\n    SUM(daily_sales) OVER (\n        PARTITION BY product_id\n        ORDER BY sale_date\n        ROWS UNBOUNDED PRECEDING\n    ) AS running_total,\n    AVG(daily_sales) OVER (\n        PARTITION BY product_id\n        ORDER BY sale_date\n        ROWS 6 PRECEDING\n    ) AS moving_avg_7d\nFROM sales\nORDER BY product_id, sale_date;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#pattern-3-month-over-month-growth","title":"Pattern 3 Month-over-Month Growth","text":"<p>Problem: Calculate month-over-month growth rate for each product.</p> <pre><code>WITH monthly_sales AS (\n    SELECT\n        product_id,\n        DATE_TRUNC('month', sale_date) AS month,\n        SUM(amount) AS monthly_amount\n    FROM sales\n    GROUP BY product_id, DATE_TRUNC('month', sale_date)\n)\nSELECT\n    product_id,\n    month,\n    monthly_amount,\n    LAG(monthly_amount) OVER (\n        PARTITION BY product_id\n        ORDER BY month\n    ) AS prev_month_amount,\n    ROUND(\n        (monthly_amount - LAG(monthly_amount) OVER (\n            PARTITION BY product_id ORDER BY month\n        )) / LAG(monthly_amount) OVER (\n            PARTITION BY product_id ORDER BY month\n        ) * 100, 2\n    ) AS growth_rate_pct\nFROM monthly_sales\nORDER BY product_id, month;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#netflix-style-interview-problems","title":"\ud83c\udfaf Netflix-Style Interview Problems","text":""},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#problem-1-user-engagement-analysis","title":"Problem 1 User Engagement Analysis","text":"<p>Context: Given user viewing sessions, find the most engaged users and their viewing patterns.</p> <pre><code>-- Sample data\nCREATE TABLE viewing_sessions (\n    user_id INT,\n    session_start TIMESTAMP,\n    session_end TIMESTAMP,\n    content_id INT,\n    minutes_watched INT\n);\n\n-- Solution\nWITH user_stats AS (\n    SELECT\n        user_id,\n        COUNT(*) AS total_sessions,\n        SUM(minutes_watched) AS total_minutes,\n        AVG(minutes_watched) AS avg_session_length,\n        MAX(session_start) AS last_session_date\n    FROM viewing_sessions\n    GROUP BY user_id\n),\nranked_users AS (\n    SELECT\n        user_id,\n        total_sessions,\n        total_minutes,\n        avg_session_length,\n        last_session_date,\n        ROW_NUMBER() OVER (ORDER BY total_minutes DESC) AS engagement_rank,\n        PERCENT_RANK() OVER (ORDER BY total_minutes DESC) AS engagement_percentile\n    FROM user_stats\n)\nSELECT\n    user_id,\n    total_sessions,\n    total_minutes,\n    avg_session_length,\n    last_session_date,\n    engagement_rank,\n    ROUND(engagement_percentile * 100, 1) AS engagement_percentile\nFROM ranked_users\nWHERE engagement_rank &lt;= 100  -- Top 100 most engaged users\nORDER BY total_minutes DESC;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#problem-2-content-popularity-trends","title":"Problem 2 Content Popularity Trends","text":"<p>Context: Track how content popularity changes over time and identify trending content.</p> <pre><code>-- Sample data\nCREATE TABLE content_views (\n    content_id INT,\n    view_date DATE,\n    view_count INT,\n    unique_viewers INT\n);\n\n-- Solution\nWITH daily_content_stats AS (\n    SELECT\n        content_id,\n        view_date,\n        view_count,\n        unique_viewers,\n        SUM(view_count) OVER (\n            PARTITION BY content_id\n            ORDER BY view_date\n            ROWS 6 PRECEDING\n        ) AS views_7d,\n        AVG(view_count) OVER (\n            PARTITION BY content_id\n            ORDER BY view_date\n            ROWS 6 PRECEDING\n        ) AS avg_views_7d\n    FROM content_views\n),\ncontent_trends AS (\n    SELECT\n        content_id,\n        view_date,\n        view_count,\n        views_7d,\n        avg_views_7d,\n        -- Trend indicator: current day vs 7-day average\n        CASE\n            WHEN view_count &gt; avg_views_7d * 1.5 THEN 'Strong Uptrend'\n            WHEN view_count &gt; avg_views_7d * 1.2 THEN 'Uptrend'\n            WHEN view_count &lt; avg_views_7d * 0.8 THEN 'Downtrend'\n            ELSE 'Stable'\n        END AS trend_indicator,\n        -- Popularity rank for the day\n        ROW_NUMBER() OVER (\n            PARTITION BY view_date\n            ORDER BY view_count DESC\n        ) AS daily_popularity_rank\n    FROM daily_content_stats\n)\nSELECT\n    content_id,\n    view_date,\n    view_count,\n    views_7d,\n    ROUND(avg_views_7d, 2) AS avg_views_7d,\n    trend_indicator,\n    daily_popularity_rank\nFROM content_trends\nWHERE trend_indicator IN ('Strong Uptrend', 'Uptrend')\n  AND daily_popularity_rank &lt;= 10\nORDER BY view_date DESC, view_count DESC;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#problem-3-ab-test-analysis","title":"Problem 3 A/B Test Analysis","text":"<p>Context: Analyze A/B test results with user segmentation and performance metrics.</p> <pre><code>-- Sample data\nCREATE TABLE ab_test_results (\n    user_id INT,\n    test_group VARCHAR(10), -- 'control' or 'variant'\n    signup_date DATE,\n    revenue DECIMAL(10,2),\n    sessions INT\n);\n\n-- Solution\nWITH user_metrics AS (\n    SELECT\n        user_id,\n        test_group,\n        signup_date,\n        revenue,\n        sessions,\n        revenue / NULLIF(sessions, 0) AS revenue_per_session,\n        ROW_NUMBER() OVER (\n            PARTITION BY test_group\n            ORDER BY revenue DESC\n        ) AS revenue_rank,\n        PERCENT_RANK() OVER (\n            PARTITION BY test_group\n            ORDER BY revenue\n        ) AS revenue_percentile\n    FROM ab_test_results\n),\ngroup_stats AS (\n    SELECT\n        test_group,\n        COUNT(*) AS user_count,\n        AVG(revenue) AS avg_revenue,\n        MEDIAN(revenue) AS median_revenue,\n        STDDEV(revenue) AS revenue_stddev,\n        SUM(revenue) AS total_revenue,\n        AVG(sessions) AS avg_sessions\n    FROM ab_test_results\n    GROUP BY test_group\n)\nSELECT\n    u.user_id,\n    u.test_group,\n    u.signup_date,\n    u.revenue,\n    u.sessions,\n    u.revenue_per_session,\n    u.revenue_rank,\n    ROUND(u.revenue_percentile * 100, 1) AS revenue_percentile,\n    -- Group comparison\n    g.avg_revenue,\n    ROUND((u.revenue - g.avg_revenue) / g.avg_revenue * 100, 2) AS revenue_vs_group_avg_pct,\n    g.median_revenue,\n    g.user_count,\n    g.total_revenue\nFROM user_metrics u\nJOIN group_stats g ON u.test_group = g.test_group\nORDER BY u.test_group, u.revenue DESC;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#amazon-style-problems","title":"\ud83c\udfe2 Amazon-Style Problems","text":""},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#problem-1-product-category-analysis","title":"Problem 1 Product Category Analysis","text":"<p>Context: Analyze product performance within categories and identify top performers.</p> <pre><code>CREATE TABLE product_sales (\n    product_id INT,\n    category VARCHAR(50),\n    sales_date DATE,\n    units_sold INT,\n    revenue DECIMAL(10,2)\n);\n\n-- Solution\nWITH product_performance AS (\n    SELECT\n        product_id,\n        category,\n        SUM(units_sold) AS total_units,\n        SUM(revenue) AS total_revenue,\n        AVG(revenue) AS avg_daily_revenue,\n        ROW_NUMBER() OVER (\n            PARTITION BY category\n            ORDER BY SUM(revenue) DESC\n        ) AS category_rank,\n        PERCENT_RANK() OVER (\n            PARTITION BY category\n            ORDER BY SUM(revenue) DESC\n        ) AS category_percentile\n    FROM product_sales\n    WHERE sales_date &gt;= CURRENT_DATE - INTERVAL '30 days'\n    GROUP BY product_id, category\n)\nSELECT\n    p.product_id,\n    p.category,\n    p.total_units,\n    p.total_revenue,\n    p.avg_daily_revenue,\n    p.category_rank,\n    ROUND(p.category_percentile * 100, 1) AS category_percentile,\n    -- Category performance\n    c.category_total_revenue,\n    ROUND(p.total_revenue / c.category_total_revenue * 100, 2) AS category_share_pct\nFROM product_performance p\nJOIN (\n    SELECT\n        category,\n        SUM(total_revenue) AS category_total_revenue\n    FROM product_performance\n    GROUP BY category\n) c ON p.category = c.category\nWHERE p.category_rank &lt;= 5\nORDER BY p.category, p.category_rank;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#problem-2-customer-segmentation","title":"Problem 2 Customer Segmentation","text":"<p>Context: Segment customers based on purchase behavior and lifetime value.</p> <pre><code>CREATE TABLE customer_orders (\n    customer_id INT,\n    order_date DATE,\n    order_value DECIMAL(10,2),\n    order_items INT\n);\n\n-- Solution\nWITH customer_lifetime_value AS (\n    SELECT\n        customer_id,\n        COUNT(*) AS total_orders,\n        SUM(order_value) AS lifetime_value,\n        AVG(order_value) AS avg_order_value,\n        MAX(order_date) AS last_order_date,\n        MIN(order_date) AS first_order_date,\n        -- Customer age in days\n        EXTRACT(DAY FROM MAX(order_date) - MIN(order_date)) AS customer_age_days,\n        -- Recency in days\n        EXTRACT(DAY FROM CURRENT_DATE - MAX(order_date)) AS recency_days\n    FROM customer_orders\n    GROUP BY customer_id\n),\ncustomer_segments AS (\n    SELECT\n        customer_id,\n        total_orders,\n        lifetime_value,\n        avg_order_value,\n        customer_age_days,\n        recency_days,\n        -- RFM-style segmentation\n        NTILE(5) OVER (ORDER BY recency_days ASC) AS recency_score,\n        NTILE(5) OVER (ORDER BY total_orders DESC) AS frequency_score,\n        NTILE(5) OVER (ORDER BY lifetime_value DESC) AS monetary_score,\n        -- Overall percentile\n        PERCENT_RANK() OVER (ORDER BY lifetime_value DESC) AS lifetime_value_percentile\n    FROM customer_lifetime_value\n)\nSELECT\n    customer_id,\n    total_orders,\n    lifetime_value,\n    avg_order_value,\n    customer_age_days,\n    recency_days,\n    recency_score,\n    frequency_score,\n    monetary_score,\n    ROUND(lifetime_value_percentile * 100, 1) AS lifetime_value_percentile,\n    -- Customer segment based on RFM\n    CASE\n        WHEN recency_score &gt;= 4 AND frequency_score &gt;= 4 AND monetary_score &gt;= 4 THEN 'Champions'\n        WHEN recency_score &gt;= 3 AND frequency_score &gt;= 3 AND monetary_score &gt;= 3 THEN 'Loyal Customers'\n        WHEN recency_score &gt;= 2 AND frequency_score &gt;= 2 THEN 'Potential Loyalists'\n        WHEN recency_score &lt;= 2 THEN 'At Risk'\n        ELSE 'Regular Customers'\n    END AS customer_segment\nFROM customer_segments\nORDER BY lifetime_value DESC;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#google-style-analytical-problems","title":"\ud83d\udcc8 Google-Style Analytical Problems","text":""},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#problem-1-search-query-analysis","title":"Problem 1 Search Query Analysis","text":"<p>Context: Analyze search query patterns and trending terms.</p> <pre><code>CREATE TABLE search_queries (\n    query_id INT,\n    user_id INT,\n    query_text VARCHAR(255),\n    search_date TIMESTAMP,\n    results_count INT,\n    clicked BOOLEAN\n);\n\n-- Solution for trending queries\nWITH query_daily_stats AS (\n    SELECT\n        DATE(search_date) AS search_day,\n        query_text,\n        COUNT(*) AS daily_searches,\n        COUNT(*) FILTER (WHERE clicked) AS daily_clicks,\n        AVG(results_count) AS avg_results\n    FROM search_queries\n    WHERE search_date &gt;= CURRENT_DATE - INTERVAL '30 days'\n    GROUP BY DATE(search_date), query_text\n),\nquery_trends AS (\n    SELECT\n        search_day,\n        query_text,\n        daily_searches,\n        daily_clicks,\n        ROUND(daily_clicks::DECIMAL / NULLIF(daily_searches, 0) * 100, 2) AS click_rate_pct,\n        -- 7-day moving average\n        AVG(daily_searches) OVER (\n            PARTITION BY query_text\n            ORDER BY search_day\n            ROWS 6 PRECEDING\n        ) AS searches_7d_avg,\n        -- Trend vs previous week\n        daily_searches - LAG(daily_searches, 7) OVER (\n            PARTITION BY query_text\n            ORDER BY search_day\n        ) AS vs_prev_week,\n        -- Popularity rank for the day\n        ROW_NUMBER() OVER (\n            PARTITION BY search_day\n            ORDER BY daily_searches DESC\n        ) AS daily_rank\n    FROM query_daily_stats\n)\nSELECT\n    search_day,\n    query_text,\n    daily_searches,\n    daily_clicks,\n    click_rate_pct,\n    ROUND(searches_7d_avg, 2) AS searches_7d_avg,\n    vs_prev_week,\n    CASE\n        WHEN vs_prev_week &gt; 0 THEN 'Trending Up'\n        WHEN vs_prev_week &lt; 0 THEN 'Trending Down'\n        ELSE 'Stable'\n    END AS trend_direction,\n    daily_rank\nFROM query_trends\nWHERE daily_rank &lt;= 20  -- Top 20 queries per day\nORDER BY search_day DESC, daily_searches DESC;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#performance-optimization-interview-questions","title":"\ud83d\ude80 Performance Optimization Interview Questions","text":""},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#problem-1-optimize-slow-window-function-query","title":"Problem 1 Optimize Slow Window Function Query","text":"<p>Context: Given a slow query, identify optimization opportunities.</p> <pre><code>-- Original slow query\nSELECT\n    user_id,\n    transaction_date,\n    amount,\n    SUM(amount) OVER (PARTITION BY user_id ORDER BY transaction_date) AS running_total\nFROM transactions\nWHERE transaction_date &gt;= '2024-01-01'\nORDER BY user_id, transaction_date;\n\n-- Optimization strategies:\n-- 1. Add composite index on (user_id, transaction_date)\nCREATE INDEX idx_transactions_user_date ON transactions (user_id, transaction_date);\n\n-- 2. Use smaller frame for better performance\nSELECT\n    user_id,\n    transaction_date,\n    amount,\n    SUM(amount) OVER (\n        PARTITION BY user_id\n        ORDER BY transaction_date\n        ROWS 100 PRECEDING  -- Limit frame size instead of UNBOUNDED\n    ) AS recent_running_total\nFROM transactions\nWHERE transaction_date &gt;= '2024-01-01'\nORDER BY user_id, transaction_date;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#problem-2-handle-large-dataset-window-functions","title":"Problem 2 Handle Large Dataset Window Functions","text":"<p>Context: Process large datasets efficiently with window functions.</p> <pre><code>-- Efficient batch processing\nWITH numbered_rows AS (\n    SELECT\n        *,\n        ROW_NUMBER() OVER (ORDER BY id) AS rn,\n        COUNT(*) OVER () AS total_count\n    FROM large_table\n),\nbatches AS (\n    SELECT\n        *,\n        NTILE(10) OVER (ORDER BY rn) AS batch_number\n    FROM numbered_rows\n)\nSELECT\n    batch_number,\n    COUNT(*) AS batch_size,\n    MIN(id) AS min_id,\n    MAX(id) AS max_id\nFROM batches\nGROUP BY batch_number\nORDER BY batch_number;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#common-pitfalls-and-solutions","title":"\ud83d\udd27 Common Pitfalls and Solutions","text":""},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#pitfall-1-wrong-default-frame","title":"Pitfall 1 Wrong Default Frame","text":"<pre><code>-- Problem: Unexpected results with LAST_VALUE\nSELECT id, value, LAST_VALUE(value) OVER (ORDER BY id) FROM table1;\n\n-- Solution: Specify explicit frame\nSELECT id, value, LAST_VALUE(value) OVER (\n    ORDER BY id\n    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n) FROM table1;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#pitfall-2-null-handling-in-navigation-functions","title":"Pitfall 2 NULL Handling in Navigation Functions","text":"<pre><code>-- Problem: NULL values break navigation functions\nSELECT id, value, LAG(value) OVER (ORDER BY id) FROM table_with_nulls;\n\n-- Solution: Use IGNORE NULLS or COALESCE\nSELECT\n    id,\n    value,\n    LAG(value) IGNORE NULLS OVER (ORDER BY id) AS prev_non_null_value,\n    COALESCE(\n        LAG(value) OVER (ORDER BY id),\n        0\n    ) AS prev_with_default\nFROM table_with_nulls;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-interview-examples/#related-topics","title":"\ud83d\udcda Related Topics","text":"<ul> <li><code>concepts/SQL/window-functions/window-functions-overview.md</code> - Complete window functions guide</li> <li><code>concepts/SQL/window-functions/postgresql-ranking-functions.md</code> - Ranking functions</li> <li><code>concepts/SQL/window-functions/postgresql-navigation-functions.md</code> - Navigation functions</li> <li><code>concepts/SQL/window-functions/postgresql-advanced-concepts.md</code> - Advanced concepts</li> </ul>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/","title":"PostgreSQL Navigation Functions - Complete Guide","text":""},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#overview","title":"\ud83d\udd39 Overview","text":"<p>Navigation functions in PostgreSQL allow you to access values from different rows within the same result set. These functions are essential for time-series analysis, trend calculations, and comparing values across rows.</p>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#available-navigation-functions","title":"\ud83d\udcca Available Navigation Functions","text":""},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#lag-access-previous-rows","title":"LAG() - Access Previous Rows","text":"<p>Returns: Value from a previous row in the partition</p> <pre><code>LAG(expression [, offset [, default_value]]) OVER (\n    [PARTITION BY partition_column]\n    ORDER BY sort_column\n)\n</code></pre> <p>Parameters::</p> <ul> <li><code>expression</code>: The column/expression to retrieve</li> <li><code>offset</code>: How many rows back (default: 1)</li> <li><code>default_value</code>: Value to return if offset goes beyond partition (default: NULL)</li> </ul>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#lead-access-following-rows","title":"LEAD() - Access Following Rows","text":"<p>Returns: Value from a following row in the partition</p> <pre><code>LEAD(expression [, offset [, default_value]]) OVER (\n    [PARTITION BY partition_column]\n    ORDER BY sort_column\n)\n</code></pre> <p>Parameters: Same as LAG()</p>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#first_value-first-value-in-window-frame","title":"FIRST_VALUE() - First Value in Window Frame","text":"<p>Returns: First value in the current window frame</p> <pre><code>FIRST_VALUE(expression) OVER (\n    [PARTITION BY partition_column]\n    ORDER BY sort_column\n    [frame_clause]\n)\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#last_value-last-value-in-window-frame","title":"LAST_VALUE() - Last Value in Window Frame","text":"<p>Returns: Last value in the current window frame</p> <pre><code>LAST_VALUE(expression) OVER (\n    [PARTITION BY partition_column]\n    ORDER BY sort_column\n    [frame_clause]\n)\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#nth_value-nth-value-in-window-frame","title":"NTH_VALUE() - Nth Value in Window Frame","text":"<p>Returns: Nth value in the current window frame</p> <pre><code>NTH_VALUE(expression, n) OVER (\n    [PARTITION BY partition_column]\n    ORDER BY sort_column\n    [frame_clause]\n)\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#interview-ready-examples","title":"\ud83c\udfaf Interview-Ready Examples","text":""},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#example-1-month-over-month-sales-growth","title":"Example 1 Month-over-Month Sales Growth","text":"<pre><code>-- Sample data setup\nCREATE TABLE monthly_sales (\n    id SERIAL PRIMARY KEY,\n    product_name VARCHAR(50),\n    sale_date DATE,\n    sales_amount DECIMAL(10,2)\n);\n\nINSERT INTO monthly_sales (product_name, sale_date, sales_amount) VALUES\n('Widget A', '2024-01-01', 10000),\n('Widget A', '2024-02-01', 12000),\n('Widget A', '2024-03-01', 15000),\n('Widget A', '2024-04-01', 13000),\n('Widget B', '2024-01-01', 8000),\n('Widget B', '2024-02-01', 9500),\n('Widget B', '2024-03-01', 11000);\n\n-- Month-over-month growth calculation\nSELECT\n    product_name,\n    sale_date,\n    sales_amount,\n    LAG(sales_amount) OVER (PARTITION BY product_name ORDER BY sale_date) AS prev_month_sales,\n    ROUND(\n        (sales_amount - LAG(sales_amount) OVER (PARTITION BY product_name ORDER BY sale_date))\n        / LAG(sales_amount) OVER (PARTITION BY product_name ORDER BY sale_date)\n        * 100, 2\n    ) AS growth_percentage\nFROM monthly_sales\nORDER BY product_name, sale_date;\n</code></pre> <p>Results::</p> product_name sale_date sales_amount prev_month_sales growth_percentage Widget A 2024-01-01 10000 NULL NULL Widget A 2024-02-01 12000 10000 20.00 Widget A 2024-03-01 15000 12000 25.00 Widget A 2024-04-01 13000 15000 -13.33"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#example-2-running-averages-and-trends","title":"Example 2 Running Averages and Trends","text":"<pre><code>SELECT\n    product_name,\n    sale_date,\n    sales_amount,\n    -- 3-month moving average\n    ROUND(AVG(sales_amount) OVER (\n        PARTITION BY product_name\n        ORDER BY sale_date\n        ROWS 2 PRECEDING\n    ), 2) AS moving_avg_3_month,\n\n    -- Compare with first month of the year\n    FIRST_VALUE(sales_amount) OVER (\n        PARTITION BY product_name, EXTRACT(YEAR FROM sale_date)\n        ORDER BY sale_date\n    ) AS first_month_sales,\n\n    -- Get the peak sales so far\n    MAX(sales_amount) OVER (\n        PARTITION BY product_name\n        ORDER BY sale_date\n        ROWS UNBOUNDED PRECEDING\n    ) AS peak_sales_so_far,\n\n    -- Next month's sales (for prediction analysis)\n    LEAD(sales_amount) OVER (\n        PARTITION BY product_name\n        ORDER BY sale_date\n    ) AS next_month_sales\nFROM monthly_sales\nORDER BY product_name, sale_date;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#example-3-employee-performance-tracking","title":"Example 3 Employee Performance Tracking","text":"<pre><code>CREATE TABLE employee_performance (\n    employee_id INT,\n    review_date DATE,\n    performance_score DECIMAL(3,1),\n    salary DECIMAL(10,2)\n);\n\nINSERT INTO employee_performance VALUES\n(1, '2024-01-01', 8.5, 75000),\n(1, '2024-04-01', 9.0, 78000),\n(1, '2024-07-01', 8.8, 80000),\n(2, '2024-01-01', 7.5, 65000),\n(2, '2024-04-01', 8.0, 67000),\n(2, '2024-07-01', 8.5, 69000);\n\nSELECT\n    employee_id,\n    review_date,\n    performance_score,\n    salary,\n    -- Performance change from previous review\n    performance_score - LAG(performance_score) OVER (\n        PARTITION BY employee_id\n        ORDER BY review_date\n    ) AS performance_change,\n\n    -- Salary growth rate\n    ROUND(\n        (salary - LAG(salary) OVER (\n            PARTITION BY employee_id\n            ORDER BY review_date\n        )) / LAG(salary) OVER (\n            PARTITION BY employee_id\n            ORDER BY review_date\n        ) * 100, 2\n    ) AS salary_growth_pct,\n\n    -- Best performance so far\n    MAX(performance_score) OVER (\n        PARTITION BY employee_id\n        ORDER BY review_date\n        ROWS UNBOUNDED PRECEDING\n    ) AS best_performance_so_far,\n\n    -- Performance percentile in current review period\n    ROUND(\n        PERCENT_RANK() OVER (\n            PARTITION BY review_date\n            ORDER BY performance_score\n        ) * 100, 1\n    ) AS performance_percentile\nFROM employee_performance\nORDER BY employee_id, review_date;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#window-frame-behavior","title":"\ud83d\udd0d Window Frame Behavior","text":""},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#understanding-window-frames","title":"Understanding Window Frames","text":"<p>Navigation functions depend heavily on window frame specifications:</p> <pre><code>-- Default frame (RANGE UNBOUNDED PRECEDING)\nFIRST_VALUE(salary) OVER (ORDER BY salary)  -- First salary overall\n\n-- Explicit frame control\nFIRST_VALUE(salary) OVER (\n    ORDER BY salary\n    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n)  -- First salary up to current row\n\n-- Moving window\nFIRST_VALUE(salary) OVER (\n    ORDER BY date\n    ROWS 2 PRECEDING\n)  -- First salary in last 3 rows\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#frame-types-impact-on-functions","title":"Frame Types Impact on Functions","text":"Function Default Frame Common Use Case LAG() Not applicable Previous row values LEAD() Not applicable Next row values FIRST_VALUE() RANGE UNBOUNDED PRECEDING Running minimum/maximum LAST_VALUE() RANGE UNBOUNDED PRECEDING Running totals, moving averages NTH_VALUE() RANGE UNBOUNDED PRECEDING Specific position access"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#performance-considerations","title":"\u26a1 Performance Considerations","text":""},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Index Strategy: Ensure ORDER BY columns are indexed</li> <li>Frame Specification: Be explicit about frames to avoid expensive defaults</li> <li>Partition Size: Smaller partitions perform better</li> <li>Memory Usage: Large frames may require significant memory</li> </ol>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#postgresql-specific-optimizations","title":"PostgreSQL-Specific Optimizations","text":"<pre><code>-- Efficient: Uses index on date column\nSELECT\n    date,\n    price,\n    LAG(price) OVER (ORDER BY date)\nFROM stock_prices;\n\n-- Less efficient: No index on complex expression\nSELECT\n    date,\n    price,\n    LAG(price) OVER (ORDER BY EXTRACT(YEAR FROM date))\nFROM stock_prices;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#common-interview-patterns","title":"\ud83c\udfaf Common Interview Patterns","text":""},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#pattern-1-time-series-analysis","title":"Pattern 1 Time Series Analysis","text":"<pre><code>SELECT\n    date,\n    value,\n    LAG(value, 1) OVER (ORDER BY date) AS prev_value,\n    LEAD(value, 1) OVER (ORDER BY date) AS next_value,\n    value - LAG(value, 1) OVER (ORDER BY date) AS change,\n    ROUND(\n        (value - LAG(value, 1) OVER (ORDER BY date))\n        / LAG(value, 1) OVER (ORDER BY date) * 100, 2\n    ) AS change_pct\nFROM time_series_data;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#pattern-2-running-statistics","title":"Pattern 2 Running Statistics","text":"<pre><code>SELECT\n    id,\n    value,\n    ROW_NUMBER() OVER (ORDER BY value) AS row_num,\n    FIRST_VALUE(value) OVER (ORDER BY value) AS min_value,\n    LAST_VALUE(value) OVER (\n        ORDER BY value\n        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n    ) AS max_value,\n    AVG(value) OVER (\n        ORDER BY value\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ) AS running_avg\nFROM data_table;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#pattern-3-finding-sequences","title":"Pattern 3 Finding Sequences","text":"<pre><code>-- Find consecutive rows meeting criteria\nSELECT *\nFROM (\n    SELECT\n        *,\n        CASE WHEN value &gt; threshold THEN 1 ELSE 0 END AS meets_criteria,\n        LAG(CASE WHEN value &gt; threshold THEN 1 ELSE 0 END) OVER (ORDER BY date) AS prev_meets\n    FROM data_table\n) t\nWHERE meets_criteria = 1 AND prev_meets = 1;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#pattern-4-cohort-analysis","title":"Pattern 4 Cohort Analysis","text":"<pre><code>SELECT\n    user_id,\n    signup_date,\n    first_purchase_date,\n    -- Days to first purchase\n    first_purchase_date - signup_date AS days_to_first_purchase,\n    -- Compare with user's first purchase amount\n    amount - FIRST_VALUE(amount) OVER (\n        PARTITION BY user_id\n        ORDER BY purchase_date\n    ) AS amount_vs_first_purchase\nFROM user_purchases\nWHERE purchase_date &gt;= signup_date;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#advanced-examples","title":"\ud83d\ude80 Advanced Examples","text":""},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#complex-frame-specifications","title":"Complex Frame Specifications","text":"<pre><code>SELECT\n    department,\n    employee_id,\n    salary,\n    hire_date,\n\n    -- First hired employee in department\n    FIRST_VALUE(employee_id) OVER (\n        PARTITION BY department\n        ORDER BY hire_date\n        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n    ) AS first_hired_employee,\n\n    -- Most recent hire before current employee\n    LAG(employee_id) OVER (\n        PARTITION BY department\n        ORDER BY hire_date\n    ) AS previous_hire,\n\n    -- Next hire after current employee\n    LEAD(employee_id) OVER (\n        PARTITION BY department\n        ORDER BY hire_date\n    ) AS next_hire,\n\n    -- Third most senior employee\n    NTH_VALUE(employee_id, 3) OVER (\n        PARTITION BY department\n        ORDER BY hire_date\n        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n    ) AS third_senior_employee\n\nFROM employees\nORDER BY department, hire_date;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#handling-null-values","title":"Handling NULL Values","text":"<pre><code>SELECT\n    id,\n    value,\n    -- Handle NULLs in navigation\n    COALESCE(\n        LAG(value) OVER (ORDER BY id),\n        0\n    ) AS prev_value_with_default,\n\n    -- Skip NULL values\n    LAG(value) IGNORE NULLS OVER (ORDER BY id) AS prev_non_null_value,\n\n    -- Count consecutive NULLs\n    CASE WHEN value IS NULL THEN\n        COUNT(*) FILTER (WHERE value IS NULL) OVER (\n            ORDER BY id\n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        )\n    END AS consecutive_nulls\nFROM nullable_data;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#troubleshooting-common-issues","title":"\ud83d\udd27 Troubleshooting Common Issues","text":""},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#issue-1-unexpected-null-values","title":"Issue 1 Unexpected NULL Values","text":"<pre><code>-- Problem: LAG returns NULL for first row\nSELECT id, value, LAG(value) OVER (ORDER BY id) FROM table1;\n\n-- Solution: Provide default value\nSELECT id, value, LAG(value, 1, 0) OVER (ORDER BY id) FROM table1;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#issue-2-wrong-frame-behavior","title":"Issue 2 Wrong Frame Behavior","text":"<pre><code>-- Problem: LAST_VALUE returns current row instead of last row\nSELECT id, value, LAST_VALUE(value) OVER (ORDER BY id) FROM table1;\n\n-- Solution: Specify explicit frame\nSELECT id, value, LAST_VALUE(value) OVER (\n    ORDER BY id\n    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n) FROM table1;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-navigation-functions/#related-topics","title":"\ud83d\udcda Related Topics","text":"<ul> <li><code>concepts/SQL/window-functions/window-functions-overview.md</code> - Complete window functions guide</li> <li><code>concepts/SQL/window-functions/postgresql-ranking-functions.md</code> - Ranking functions</li> </ul>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/","title":"PostgreSQL Ranking Functions - Complete Guide","text":""},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#overview","title":"\ud83d\udd39 Overview","text":"<p>PostgreSQL provides several ranking functions that assign sequential numbers or ranks to rows based on specified ordering. These are essential for analytical queries and interview problems.</p>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#available-ranking-functions","title":"\ud83d\udcca Available Ranking Functions","text":""},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#row_number","title":"ROW_NUMBER()","text":"<p>Returns: Unique sequential integer for each row (1, 2, 3, ...)</p> <pre><code>ROW_NUMBER() OVER (\n    [PARTITION BY partition_column]\n    ORDER BY sort_column [ASC|DESC]\n)\n</code></pre> <p>Key Characteristics::</p> <ul> <li>No gaps in numbering</li> <li>Different values for tied rows</li> <li>Deterministic within partition</li> </ul>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#rank","title":"RANK()","text":"<p>Returns: Rank with gaps for tied values</p> <pre><code>RANK() OVER (\n    [PARTITION BY partition_column]\n    ORDER BY sort_column [ASC|DESC]\n)\n</code></pre> <p>Key Characteristics::</p> <ul> <li>Same rank for tied values</li> <li>Gaps in ranking sequence (e.g., 1, 2, 2, 4, ...)</li> <li>Non-consecutive ranking</li> </ul>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#dense_rank","title":"DENSE_RANK()","text":"<p>Returns: Rank without gaps for tied values</p> <pre><code>DENSE_RANK() OVER (\n    [PARTITION BY partition_column]\n    ORDER BY sort_column [ASC|DESC]\n)\n</code></pre> <p>Key Characteristics::</p> <ul> <li>Same rank for tied values</li> <li>No gaps in ranking sequence (e.g., 1, 2, 2, 3, 4, ...)</li> <li>Consecutive ranking</li> </ul>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#percent_rank","title":"PERCENT_RANK()","text":"<p>Returns: Relative rank as decimal (0.0 to 1.0)</p> <pre><code>PERCENT_RANK() OVER (\n    [PARTITION BY partition_column]\n    ORDER BY sort_column [ASC|DESC]\n)\n</code></pre> <p>Formula: <code>(rank - 1) / (total_rows_in_partition - 1)</code></p>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#cume_dist","title":"CUME_DIST()","text":"<p>Returns: Cumulative distribution as decimal (0.0 to 1.0)</p> <pre><code>CUME_DIST() OVER (\n    [PARTITION BY partition_column]\n    ORDER BY sort_column [ASC|DESC]\n)\n</code></pre> <p>Formula: <code>number_of_rows_with_value &lt;= current_value / total_rows_in_partition</code></p>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#ntilen","title":"NTILE(n)","text":"<p>Returns: Bucket number (1 to n) dividing rows into equal groups</p> <pre><code>NTILE(num_buckets) OVER (\n    [PARTITION BY partition_column]\n    ORDER BY sort_column [ASC|DESC]\n)\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#interview-ready-examples","title":"\ud83c\udfaf Interview-Ready Examples","text":""},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#example-1-employee-salary-ranking","title":"Example 1 Employee Salary Ranking","text":"<pre><code>-- Sample data setup\nCREATE TABLE employees (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(50),\n    department VARCHAR(50),\n    salary DECIMAL(10,2),\n    hire_date DATE\n);\n\nINSERT INTO employees (name, department, salary, hire_date) VALUES\n('Alice', 'Engineering', 120000, '2020-01-15'),\n('Bob', 'Engineering', 110000, '2020-03-20'),\n('Charlie', 'Engineering', 110000, '2020-05-10'),\n('David', 'Sales', 90000, '2021-01-10'),\n('Eve', 'Sales', 95000, '2021-02-15'),\n('Frank', 'Marketing', 85000, '2021-03-01');\n\n-- Ranking query\nSELECT\n    name,\n    department,\n    salary,\n    ROW_NUMBER() OVER (ORDER BY salary DESC) AS overall_row_num,\n    RANK() OVER (ORDER BY salary DESC) AS overall_rank,\n    DENSE_RANK() OVER (ORDER BY salary DESC) AS overall_dense_rank,\n    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS dept_row_num,\n    RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS dept_rank,\n    PERCENT_RANK() OVER (ORDER BY salary DESC) AS overall_percentile,\n    NTILE(4) OVER (ORDER BY salary DESC) AS salary_quartile\nFROM employees\nORDER BY salary DESC;\n</code></pre> <p>Expected Results::</p> name department salary overall_row_num overall_rank overall_dense_rank dept_row_num dept_rank overall_percentile salary_quartile Alice Engineering 120000 1 1 1 1 1 0.0 1 Bob Engineering 110000 2 2 2 2 2 0.2 1 Charlie Engineering 110000 3 2 2 3 2 0.2 2 Eve Sales 95000 4 4 3 1 1 0.6 2 David Sales 90000 5 5 4 2 2 0.8 3 Frank Marketing 85000 6 6 5 1 1 1.0 4"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#example-2-top-3-performers-by-department","title":"Example 2 Top 3 Performers by Department","text":"<pre><code>WITH ranked_employees AS (\n    SELECT\n        name,\n        department,\n        salary,\n        ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS dept_row_num,\n        RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS dept_rank\n    FROM employees\n)\nSELECT\n    name,\n    department,\n    salary,\n    dept_rank\nFROM ranked_employees\nWHERE dept_rank &lt;= 3\nORDER BY department, dept_rank;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#example-3-cumulative-distribution-for-performance-analysis","title":"Example 3 Cumulative Distribution for Performance Analysis","text":"<pre><code>SELECT\n    department,\n    salary,\n    CUME_DIST() OVER (ORDER BY salary DESC) AS cumulative_dist,\n    PERCENT_RANK() OVER (ORDER BY salary DESC) AS percent_rank,\n    NTILE(5) OVER (ORDER BY salary DESC) AS performance_quintile\nFROM employees\nORDER BY salary DESC;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#key-differences-summary","title":"\ud83d\udd0d Key Differences Summary","text":"Function Handles Ties Gaps in Sequence Use Case ROW_NUMBER() Assigns unique numbers No gaps When you need unique row identifiers RANK() Same rank for ties Creates gaps When tied values should have same rank DENSE_RANK() Same rank for ties No gaps When you want consecutive ranking PERCENT_RANK() Works with ties N/A For percentile calculations CUME_DIST() Works with ties N/A For cumulative distribution analysis NTILE() Divides into buckets N/A For grouping into equal-sized buckets"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#performance-tips","title":"\u26a1 Performance Tips","text":"<ol> <li>Index Strategy: Create indexes on ORDER BY columns for better performance</li> <li>Partitioning: Use PARTITION BY wisely - smaller partitions are faster</li> <li>Materialization: Consider CTEs for complex ranking scenarios</li> <li>Memory: Large result sets may require significant memory for sorting</li> </ol>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#common-interview-patterns","title":"\ud83c\udfaf Common Interview Patterns","text":""},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#pattern-1-top-n-per-group","title":"Pattern 1 Top N per Group","text":"<pre><code>SELECT *\nFROM (\n    SELECT\n        *,\n        ROW_NUMBER() OVER (PARTITION BY group_column ORDER BY value_column DESC) AS rn\n    FROM your_table\n) ranked\nWHERE rn &lt;= N;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#pattern-2-remove-duplicates-keep-firstlast","title":"Pattern 2 Remove Duplicates (Keep First/Last)","text":"<pre><code>DELETE FROM your_table\nWHERE id NOT IN (\n    SELECT id\n    FROM (\n        SELECT id, ROW_NUMBER() OVER (PARTITION BY duplicate_column ORDER BY created_at) AS rn\n        FROM your_table\n    ) ranked\n    WHERE rn = 1\n);\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#pattern-3-running-totals-with-ranks","title":"Pattern 3 Running Totals with Ranks","text":"<pre><code>SELECT\n    date,\n    amount,\n    SUM(amount) OVER (ORDER BY date) AS running_total,\n    ROW_NUMBER() OVER (ORDER BY date) AS day_number\nFROM daily_sales;\n</code></pre>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#postgresql-specific-optimizations","title":"\ud83d\ude80 PostgreSQL-Specific Optimizations","text":"<ol> <li>Parallel Execution: PostgreSQL can parallelize ranking functions in some cases</li> <li>Index-Only Scans: Properly indexed ORDER BY columns enable fast ranking</li> <li>Memory-Efficient: PostgreSQL optimizes memory usage for ranking operations</li> <li>Stable Results: Ranking functions are deterministic within partitions</li> </ol>"},{"location":"concepts/SQL/window-functions/postgresql-ranking-functions/#related-topics","title":"\ud83d\udcda Related Topics","text":"<ul> <li><code>concepts/SQL/window-functions/window-functions-overview.md</code> - Complete window functions guide</li> <li><code>concepts/SQL/aggregation/aggregate-functions.md</code> - Aggregation concepts</li> </ul>"},{"location":"concepts/SQL/window-functions/window-avg-comparison/","title":"PostgreSQL Window Functions AVG with OVER() Clause","text":""},{"location":"concepts/SQL/window-functions/window-avg-comparison/#advanced-average-comparison-with-postgresql-window-functions","title":"\u2705 Advanced Average Comparison with PostgreSQL Window Functions","text":"<p>This comprehensive guide demonstrates how to use PostgreSQL window functions to compare individual values against group aggregates, including advanced PostgreSQL-specific techniques.</p>"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#problem-statement","title":"Problem Statement","text":"<p>Find customers whose total sales exceed the average of all customer totals, with additional performance metrics.</p>"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#postgresql-specific-solutions","title":"\ud83c\udfc6 PostgreSQL-Specific Solutions","text":""},{"location":"concepts/SQL/window-functions/window-avg-comparison/#solution-1-basic-window-function-approach","title":"Solution 1 Basic Window Function Approach","text":"<pre><code>WITH customer_totals AS (\n  SELECT customer_id,\n          SUM(amount) AS total_sales,\n          COUNT(*) AS order_count,\n          MAX(sale_date) AS last_purchase_date\n  FROM sales\n  GROUP BY customer_id\n),\ncustomer_analysis AS (\n  SELECT *,\n          AVG(total_sales) OVER () AS overall_avg_sales,\n          STDDEV(total_sales) OVER () AS sales_stddev,\n          MIN(total_sales) OVER () AS min_sales,\n          MAX(total_sales) OVER () AS max_sales,\n          COUNT(*) OVER () AS total_customers,\n          ROW_NUMBER() OVER (ORDER BY total_sales DESC) AS sales_rank,\n          PERCENT_RANK() OVER (ORDER BY total_sales DESC) AS sales_percentile\n  FROM customer_totals\n)\nSELECT\n    customer_id,\n    total_sales,\n    order_count,\n    last_purchase_date,\n    ROUND(overall_avg_sales, 2) AS overall_avg_sales,\n    ROUND(sales_stddev, 2) AS sales_stddev,\n    sales_rank,\n    ROUND(sales_percentile * 100, 1) AS sales_percentile,\n    CASE\n        WHEN total_sales &gt; overall_avg_sales THEN 'Above Average'\n        WHEN total_sales = overall_avg_sales THEN 'At Average'\n        ELSE 'Below Average'\n    END AS performance_category,\n    ROUND((total_sales - overall_avg_sales) / overall_avg_sales * 100, 2) AS pct_above_avg\nFROM customer_analysis\nWHERE total_sales &gt; overall_avg_sales\nORDER BY total_sales DESC;\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#solution-2-advanced-postgresql-windowed-aggregate","title":"Solution 2 Advanced PostgreSQL Windowed Aggregate","text":"<pre><code>SELECT\n    customer_id,\n    total_sales,\n    overall_avg_sales,\n    order_count,\n    -- PostgreSQL-specific windowed aggregates\n    ROUND(AVG(total_sales) OVER (ORDER BY total_sales ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING), 2) AS true_overall_avg,\n    -- Cumulative statistics\n    SUM(total_sales) OVER (ORDER BY total_sales DESC ROWS UNBOUNDED PRECEDING) AS cumulative_sales,\n    ROUND(SUM(total_sales) OVER (ORDER BY total_sales DESC ROWS UNBOUNDED PRECEDING) / SUM(total_sales) OVER () * 100, 2) AS cumulative_pct_of_total,\n    -- Performance bands\n    NTILE(5) OVER (ORDER BY total_sales DESC) AS performance_quintile\nFROM (\n    SELECT\n        customer_id,\n        SUM(amount) AS total_sales,\n        COUNT(*) AS order_count,\n        AVG(SUM(amount)) OVER () AS overall_avg_sales\n    FROM sales\n    GROUP BY customer_id\n) customer_totals\nWHERE total_sales &gt; overall_avg_sales\nORDER BY total_sales DESC;\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#solution-3-postgresql-named-windows","title":"Solution 3 PostgreSQL Named Windows","text":"<pre><code>SELECT\n    customer_id,\n    total_sales,\n    order_count,\n    overall_avg_sales,\n    dept_avg_sales,\n    -- Comparison metrics\n    ROUND((total_sales - overall_avg_sales) / overall_avg_sales * 100, 2) AS vs_overall_avg_pct,\n    ROUND((total_sales - dept_avg_sales) / dept_avg_sales * 100, 2) AS vs_dept_avg_pct,\n    -- Department ranking\n    dept_sales_rank,\n    dept_sales_percentile\nFROM (\n    SELECT\n        customer_id,\n        department,\n        SUM(amount) AS total_sales,\n        COUNT(*) AS order_count,\n        -- Named windows for clarity and performance\n        AVG(SUM(amount)) OVER global_window AS overall_avg_sales,\n        AVG(SUM(amount)) OVER dept_window AS dept_avg_sales,\n        ROW_NUMBER() OVER dept_window AS dept_sales_rank,\n        ROUND(PERCENT_RANK() OVER dept_window * 100, 1) AS dept_sales_percentile\n    FROM sales s\n    JOIN customers c ON s.customer_id = c.customer_id\n    GROUP BY customer_id, department\n    WINDOW\n        global_window AS (),\n        dept_window AS (PARTITION BY department)\n) analysis\nWHERE total_sales &gt; overall_avg_sales\nORDER BY total_sales DESC;\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#postgresql-window-function-deep-dive","title":"\ud83d\udd0d PostgreSQL Window Function Deep Dive","text":""},{"location":"concepts/SQL/window-functions/window-avg-comparison/#empty-over-clause-behavior","title":"Empty OVER() Clause Behavior","text":"<ul> <li><code>AVG(total_sales) OVER ()</code> computes the average across all rows in the result set</li> <li>The empty parentheses indicate a window with no partitioning or ordering</li> <li>PostgreSQL optimization: This is computed once, not per row</li> <li>Performance: O(1) after initial computation vs O(n) for traditional approaches</li> </ul>"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#advanced-frame-specifications","title":"Advanced Frame Specifications","text":"<pre><code>-- Running average (PostgreSQL optimized)\nAVG(sales) OVER (ORDER BY date ROWS UNBOUNDED PRECEDING)\n\n-- Moving average with PostgreSQL RANGE frames\nAVG(sales) OVER (ORDER BY date RANGE BETWEEN INTERVAL '30 days' PRECEDING AND CURRENT ROW)\n\n-- Group-based frames (PostgreSQL 11+)\nAVG(sales) OVER (ORDER BY category GROUPS 2 PRECEDING)\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#postgresql-specific-advantages","title":"\ud83e\udde0 PostgreSQL-Specific Advantages","text":"<ul> <li>Memory Efficiency: PostgreSQL optimizes window function memory usage</li> <li>Parallel Processing: Window functions can leverage PostgreSQL's parallel query execution</li> <li>Index Integration: Smart use of indexes on PARTITION BY and ORDER BY columns</li> <li>Advanced Frames: Support for RANGE, ROWS, and GROUPS frame types</li> <li>Named Windows: Improved readability and maintainability</li> </ul>"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#enhanced-sample-data","title":"\ud83d\udcca Enhanced Sample Data","text":"sale_id customer_id department amount sale_date 1 100 Electronics 500 2024-01-01 2 101 Books 700 2024-01-03 3 100 Electronics 200 2024-02-01 4 102 Electronics 300 2024-02-10 5 100 Electronics 1000 2024-03-01 6 103 Books 450 2024-01-15 7 101 Books 320 2024-02-20"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#advanced-expected-results","title":"\ud83d\udcc8 Advanced Expected Results","text":"customer_id total_sales overall_avg_sales performance_quintile vs_overall_avg_pct 100 1700 744.29 1 128.45% 101 1020 744.29 2 37.01% 102 300 744.29 4 -59.68% 103 450 744.29 3 -39.55%"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#postgresql-performance-optimizations","title":"\u26a1 PostgreSQL Performance Optimizations","text":""},{"location":"concepts/SQL/window-functions/window-avg-comparison/#index-strategy","title":"Index Strategy","text":"<pre><code>-- Optimal index for window functions\nCREATE INDEX idx_sales_customer_date ON sales (customer_id, sale_date);\nCREATE INDEX idx_sales_customer_amount ON sales (customer_id, amount DESC);\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#query-execution-plan-analysis","title":"Query Execution Plan Analysis","text":"<pre><code>EXPLAIN (ANALYZE, BUFFERS)\nSELECT\n    customer_id,\n    SUM(amount) AS total_sales,\n    AVG(SUM(amount)) OVER () AS overall_avg\nFROM sales\nGROUP BY customer_id;\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#memory-configuration","title":"Memory Configuration","text":"<pre><code>-- For large result sets\nSET work_mem = '256MB';\nSET temp_buffers = '128MB';\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#production-considerations","title":"\ud83d\ude80 Production Considerations","text":"<ul> <li>Large Datasets: Use appropriate frame specifications to limit memory usage</li> <li>Real-time Queries: Consider materialized views for frequently accessed window function results</li> <li>Partitioning: Leverage PostgreSQL table partitioning for date-based window functions</li> <li>Monitoring: Use <code>pg_stat_statements</code> to monitor window function query performance</li> </ul>"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#postgresql-window-function-best-practices","title":"\ud83d\udcda PostgreSQL Window Function Best Practices","text":"<ol> <li>Use Named Windows for complex queries with multiple window functions</li> <li>Be Explicit about frame specifications to avoid unexpected results</li> <li>Index Strategically on window partitioning and ordering columns</li> <li>Test Performance with <code>EXPLAIN ANALYZE</code> before production deployment</li> <li>Consider Alternatives like CTEs when window functions are overkill</li> </ol>"},{"location":"concepts/SQL/window-functions/window-avg-comparison/#related-postgresql-window-function-topics","title":"\ud83d\udd17 Related PostgreSQL Window Function Topics","text":"<ul> <li><code>concepts/SQL/window-functions/window-functions-overview.md</code> - Complete window functions guide</li> <li><code>concepts/SQL/window-functions/postgresql-ranking-functions.md</code> - Ranking functions</li> <li><code>concepts/SQL/window-functions/postgresql-navigation-functions.md</code> - Navigation functions</li> <li><code>concepts/SQL/window-functions/postgresql-advanced-concepts.md</code> - Advanced concepts</li> <li><code>concepts/SQL/window-functions/postgresql-interview-examples.md</code> - Interview examples</li> </ul>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/","title":"PostgreSQL Window Functions Advanced AVG with OVER()","text":""},{"location":"concepts/SQL/window-functions/window-avg-simplified/#postgresql-optimized-window-function-solution","title":"\u2705 PostgreSQL-Optimized Window Function Solution","text":"<p>This guide demonstrates advanced PostgreSQL techniques for windowed aggregate functions, including performance optimizations and complex analytical patterns.</p>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#problem-statement","title":"Problem Statement","text":"<p>Find customers whose total sales exceed the average of all customer totals, with advanced PostgreSQL analytics.</p>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#postgresql-optimized-solutions","title":"\ud83c\udfc6 PostgreSQL-Optimized Solutions","text":""},{"location":"concepts/SQL/window-functions/window-avg-simplified/#solution-1-single-query-windowed-aggregate","title":"Solution 1 Single-Query Windowed Aggregate","text":"<pre><code>SELECT\n    customer_id,\n    total_sales,\n    overall_avg_sales,\n    ROUND((total_sales - overall_avg_sales) / overall_avg_sales * 100, 2) AS pct_above_avg,\n    CASE\n        WHEN total_sales &gt; overall_avg_sales THEN 'Above Average'\n        ELSE 'Below Average'\n    END AS performance_category\nFROM (\n    SELECT\n        customer_id,\n        SUM(amount) AS total_sales,\n        -- PostgreSQL windowed aggregate function\n        AVG(SUM(amount)) OVER () AS overall_avg_sales,\n        -- Additional PostgreSQL window functions\n        ROW_NUMBER() OVER (ORDER BY SUM(amount) DESC) AS sales_rank,\n        PERCENT_RANK() OVER (ORDER BY SUM(amount) DESC) AS sales_percentile,\n        NTILE(4) OVER (ORDER BY SUM(amount) DESC) AS performance_quartile\n    FROM sales\n    GROUP BY customer_id\n) AS customer_analysis\nWHERE total_sales &gt; overall_avg_sales\nORDER BY total_sales DESC;\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#solution-2-postgresql-named-windows-with-multiple-metrics","title":"Solution 2 PostgreSQL Named Windows with Multiple Metrics","text":"<pre><code>SELECT\n    customer_id,\n    department,\n    total_sales,\n    overall_avg,\n    dept_avg,\n    -- Performance comparisons\n    ROUND((total_sales - overall_avg) / overall_avg * 100, 2) AS vs_global_avg_pct,\n    ROUND((total_sales - dept_avg) / dept_avg * 100, 2) AS vs_dept_avg_pct,\n    -- Department ranking\n    dept_rank,\n    dept_percentile,\n    -- Global ranking\n    global_rank,\n    global_percentile\nFROM (\n    SELECT\n        customer_id,\n        department,\n        SUM(amount) AS total_sales,\n        -- Named windows for PostgreSQL optimization\n        AVG(SUM(amount)) OVER global_window AS overall_avg,\n        AVG(SUM(amount)) OVER dept_window AS dept_avg,\n        ROW_NUMBER() OVER dept_window AS dept_rank,\n        ROUND(PERCENT_RANK() OVER dept_window * 100, 1) AS dept_percentile,\n        ROW_NUMBER() OVER global_window AS global_rank,\n        ROUND(PERCENT_RANK() OVER global_window * 100, 1) AS global_percentile\n    FROM sales s\n    JOIN customers c ON s.customer_id = c.customer_id\n    GROUP BY customer_id, department\n    WINDOW\n        global_window AS (),\n        dept_window AS (PARTITION BY department)\n) AS analysis\nWHERE total_sales &gt; overall_avg\nORDER BY total_sales DESC;\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#solution-3-postgresql-time-based-window-analysis","title":"Solution 3 PostgreSQL Time-Based Window Analysis","text":"<pre><code>SELECT\n    customer_id,\n    sale_month,\n    monthly_sales,\n    -- Moving averages\n    ROUND(AVG(monthly_sales) OVER (\n        PARTITION BY customer_id\n        ORDER BY sale_month\n        ROWS 2 PRECEDING\n    ), 2) AS moving_avg_3m,\n\n    -- Customer's overall average\n    ROUND(AVG(monthly_sales) OVER (\n        PARTITION BY customer_id\n    ), 2) AS customer_avg,\n\n    -- Global monthly average\n    ROUND(AVG(monthly_sales) OVER (\n        PARTITION BY sale_month\n    ), 2) AS global_month_avg,\n\n    -- Performance vs customer history\n    ROUND((monthly_sales - AVG(monthly_sales) OVER (\n        PARTITION BY customer_id\n    )) / AVG(monthly_sales) OVER (\n        PARTITION BY customer_id\n    ) * 100, 2) AS vs_customer_history_pct,\n\n    -- Month-over-month growth\n    ROUND(\n        (monthly_sales - LAG(monthly_sales) OVER (\n            PARTITION BY customer_id\n            ORDER BY sale_month\n        )) / LAG(monthly_sales) OVER (\n            PARTITION BY customer_id\n            ORDER BY sale_month\n        ) * 100, 2\n    ) AS mom_growth_pct\n\nFROM (\n    SELECT\n        customer_id,\n        DATE_TRUNC('month', sale_date) AS sale_month,\n        SUM(amount) AS monthly_sales\n    FROM sales\n    GROUP BY customer_id, DATE_TRUNC('month', sale_date)\n) AS monthly_data\nORDER BY customer_id, sale_month;\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#postgresql-windowed-aggregate-deep-dive","title":"\ud83d\udd0d PostgreSQL Windowed Aggregate Deep Dive","text":""},{"location":"concepts/SQL/window-functions/window-avg-simplified/#windowed-aggregate-function-syntax","title":"Windowed Aggregate Function Syntax","text":"<pre><code>aggregate_function(expression) OVER (\n    [PARTITION BY partition_expression]\n    [ORDER BY sort_expression]\n    [frame_clause]\n)\n</code></pre> <p>Key PostgreSQL Features::</p> <ul> <li>Any Aggregate Function: <code>SUM()</code>, <code>AVG()</code>, <code>COUNT()</code>, <code>MIN()</code>, <code>MAX()</code>, <code>STDDEV()</code>, etc.</li> <li>Nested Aggregates: <code>AVG(SUM(amount)) OVER ()</code> - aggregate of aggregate</li> <li>Frame Control: Precise control over calculation window</li> <li>Performance: PostgreSQL optimizes windowed aggregates efficiently</li> </ul>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#advanced-frame-specifications","title":"Advanced Frame Specifications","text":"<pre><code>-- Cumulative sum (PostgreSQL optimized)\nSUM(amount) OVER (ORDER BY date ROWS UNBOUNDED PRECEDING)\n\n-- Moving sum with explicit frame\nSUM(amount) OVER (ORDER BY date ROWS 2 PRECEDING)\n\n-- Range-based frames (PostgreSQL specific)\nSUM(amount) OVER (ORDER BY date RANGE BETWEEN INTERVAL '30 days' PRECEDING AND CURRENT ROW)\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#postgresql-specific-optimizations","title":"\ud83e\udde0 PostgreSQL-Specific Optimizations","text":""},{"location":"concepts/SQL/window-functions/window-avg-simplified/#memory-and-performance","title":"Memory and Performance","text":"<ul> <li>Single Pass: PostgreSQL computes windowed aggregates in a single pass when possible</li> <li>Parallel Execution: Leverages PostgreSQL's parallel query capabilities</li> <li>Index Utilization: Smart index usage for ORDER BY columns</li> <li>Memory Management: Efficient memory usage for large result sets</li> </ul>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#index-strategy-for-window-functions","title":"Index Strategy for Window Functions","text":"<pre><code>-- Optimal indexes for windowed aggregates\nCREATE INDEX idx_sales_customer_date ON sales (customer_id, sale_date);\nCREATE INDEX idx_sales_customer_amount ON sales (customer_id, amount DESC);\n\n-- For time-based windowed aggregates\nCREATE INDEX idx_sales_date_customer ON sales (sale_date, customer_id);\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#enhanced-sample-data-with-departments","title":"\ud83d\udcca Enhanced Sample Data with Departments","text":"sale_id customer_id department amount sale_date 1 100 Electronics 500 2024-01-01 2 101 Books 700 2024-01-03 3 100 Electronics 200 2024-02-01 4 102 Electronics 300 2024-02-10 5 100 Electronics 1000 2024-03-01 6 103 Books 450 2024-01-15 7 101 Books 320 2024-02-20"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#advanced-postgresql-results","title":"\ud83d\udcc8 Advanced PostgreSQL Results","text":"customer_id department total_sales overall_avg dept_avg vs_global_avg_pct vs_dept_avg_pct 100 Electronics 1700 744.29 1000.00 128.45% 70.00% 101 Books 1020 744.29 823.33 37.01% 23.88% 102 Electronics 300 744.29 1000.00 -59.68% -70.00% 103 Books 450 744.29 823.33 -39.55% -45.35%"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#postgresql-performance-analysis","title":"\u26a1 PostgreSQL Performance Analysis","text":""},{"location":"concepts/SQL/window-functions/window-avg-simplified/#query-execution-plan","title":"Query Execution Plan","text":"<pre><code>EXPLAIN (ANALYZE, BUFFERS)\nSELECT\n    customer_id,\n    SUM(amount) AS total_sales,\n    AVG(SUM(amount)) OVER () AS overall_avg\nFROM sales\nGROUP BY customer_id;\n</code></pre> <p>Expected PostgreSQL Optimizations::</p> <ul> <li>HashAggregate for initial grouping</li> <li>WindowAgg for window function computation</li> <li>Parallel processing if enabled</li> <li>Index scans if appropriate indexes exist</li> </ul>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#memory-configuration","title":"Memory Configuration","text":"<pre><code>-- For large datasets with windowed aggregates\nSET work_mem = '256MB';\nSET temp_buffers = '128MB';\nSET enable_parallel_append = on;\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#advanced-postgresql-techniques","title":"\ud83d\ude80 Advanced PostgreSQL Techniques","text":""},{"location":"concepts/SQL/window-functions/window-avg-simplified/#combining-window-functions-with-ctes","title":"Combining Window Functions with CTEs","text":"<pre><code>WITH customer_totals AS (\n    SELECT\n        customer_id,\n        department,\n        SUM(amount) AS total_sales,\n        COUNT(*) AS order_count\n    FROM sales\n    GROUP BY customer_id, department\n),\ncustomer_ranks AS (\n    SELECT *,\n        -- Multiple window functions in one pass\n        AVG(total_sales) OVER () AS global_avg,\n        AVG(total_sales) OVER (PARTITION BY department) AS dept_avg,\n        ROW_NUMBER() OVER (ORDER BY total_sales DESC) AS global_rank,\n        ROW_NUMBER() OVER (PARTITION BY department ORDER BY total_sales DESC) AS dept_rank,\n        PERCENT_RANK() OVER (ORDER BY total_sales DESC) AS global_percentile\n    FROM customer_totals\n)\nSELECT *\nFROM customer_ranks\nWHERE total_sales &gt; global_avg;\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#postgresql-specific-analytical-functions","title":"PostgreSQL-Specific Analytical Functions","text":"<pre><code>SELECT\n    customer_id,\n    total_sales,\n    -- Statistical functions\n    STDDEV(total_sales) OVER () AS global_stddev,\n    VARIANCE(total_sales) OVER () AS global_variance,\n    -- Percentile functions\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY total_sales) OVER () AS global_median,\n    -- Correlation with order count (if needed)\n    CORR(total_sales, order_count) OVER () AS sales_order_correlation\nFROM customer_totals;\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#postgresql-best-practices","title":"\ud83d\udcda PostgreSQL Best Practices","text":"<ol> <li>Use Windowed Aggregates when you need to combine aggregation with window functions</li> <li>Named Windows for complex queries with multiple window specifications</li> <li>Frame Control to optimize performance and memory usage</li> <li>Index Strategy based on PARTITION BY and ORDER BY columns</li> <li>Test Performance with realistic data volumes</li> </ol>"},{"location":"concepts/SQL/window-functions/window-avg-simplified/#related-postgresql-window-function-topics","title":"\ud83d\udd17 Related PostgreSQL Window Function Topics","text":"<ul> <li><code>concepts/SQL/window-functions/window-functions-overview.md</code> - Complete window functions guide</li> <li><code>concepts/SQL/window-functions/postgresql-ranking-functions.md</code> - Ranking functions</li> <li><code>concepts/SQL/window-functions/postgresql-navigation-functions.md</code> - Navigation functions</li> <li><code>concepts/SQL/window-functions/postgresql-advanced-concepts.md</code> - Advanced concepts</li> <li><code>concepts/SQL/window-functions/postgresql-interview-examples.md</code> - Interview examples</li> </ul>"},{"location":"concepts/SQL/window-functions/window-functions-overview/","title":"\ud83d\udd39 PostgreSQL Window Functions - Complete Guide","text":""},{"location":"concepts/SQL/window-functions/window-functions-overview/#definition","title":"\u2705 Definition","text":"<p>Window functions perform calculations across a set of table rows related to the current row. Unlike aggregate functions, window functions do not collapse rows and return a value for each row in the result set.</p> <p>They require an OVER() clause to define the window specification (partition, order, and frame).</p>"},{"location":"concepts/SQL/window-functions/window-functions-overview/#core-syntax","title":"\ud83e\udde9 Core Syntax","text":"<pre><code>function_name(expression) OVER (\n    [PARTITION BY partition_expression [, ...]]\n    [ORDER BY sort_expression [ASC | DESC] [NULLS FIRST | NULLS LAST] [, ...]]\n    [frame_clause]\n)\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-functions-overview/#postgresql-specific-features","title":"\ud83d\udcd8 PostgreSQL-Specific Features","text":""},{"location":"concepts/SQL/window-functions/window-functions-overview/#named-window-specifications","title":"Named Window Specifications","text":"<p>Use the <code>WINDOW</code> clause to define reusable window specifications:</p> <pre><code>SELECT\n    employee_id,\n    department_id,\n    salary,\n    AVG(salary) OVER dept_window AS avg_dept_salary,\n    ROW_NUMBER() OVER dept_window AS dept_rank\nFROM employees\nWINDOW dept_window AS (PARTITION BY department_id ORDER BY salary DESC);\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-functions-overview/#frame-specifications","title":"Frame Specifications","text":"<p>PostgreSQL supports three frame types:</p> <ul> <li><code>ROWS</code>: Physical row-based frames</li> <li><code>RANGE</code>: Logical value-based frames</li> <li><code>GROUPS</code>: Group-based frames</li> </ul> <pre><code>-- Running total of last 3 rows\nSUM(amount) OVER (ORDER BY date ROWS 2 PRECEDING)\n\n-- Running total within date range\nSUM(amount) OVER (ORDER BY date RANGE BETWEEN INTERVAL '7 days' PRECEDING AND CURRENT ROW)\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-functions-overview/#postgresql-window-function-categories","title":"\ud83d\udd22 PostgreSQL Window Function Categories","text":""},{"location":"concepts/SQL/window-functions/window-functions-overview/#1-ranking-functions","title":"1. Ranking Functions","text":"<ul> <li><code>ROW_NUMBER()</code> - Unique sequential number per partition</li> <li><code>RANK()</code> - Rank with gaps for ties</li> <li><code>DENSE_RANK()</code> - Rank without gaps for ties</li> <li><code>PERCENT_RANK()</code> - Relative rank (0-1)</li> <li><code>CUME_DIST()</code> - Cumulative distribution</li> <li><code>NTILE(n)</code> - Divide into n equal buckets</li> </ul>"},{"location":"concepts/SQL/window-functions/window-functions-overview/#2-navigation-functions","title":"2. Navigation Functions","text":"<ul> <li><code>LAG(value, offset, default)</code> - Value from previous row</li> <li><code>LEAD(value, offset, default)</code> - Value from next row</li> <li><code>FIRST_VALUE(value)</code> - First value in window frame</li> <li><code>LAST_VALUE(value)</code> - Last value in window frame</li> <li><code>NTH_VALUE(value, n)</code> - Nth value in window frame</li> </ul>"},{"location":"concepts/SQL/window-functions/window-functions-overview/#3-aggregate-functions-as-window-functions","title":"3. Aggregate Functions as Window Functions","text":"<ul> <li><code>SUM()</code>, <code>AVG()</code>, <code>COUNT()</code>, <code>MIN()</code>, <code>MAX()</code></li> <li><code>STDDEV()</code>, <code>VARIANCE()</code> (PostgreSQL-specific)</li> <li>All standard aggregates work as window functions</li> </ul>"},{"location":"concepts/SQL/window-functions/window-functions-overview/#practical-example-employee-analysis","title":"\ud83d\udcca Practical Example - Employee Analysis","text":"<pre><code>SELECT\n    employee_id,\n    department_id,\n    salary,\n    -- Department statistics\n    AVG(salary) OVER (PARTITION BY department_id) AS dept_avg_salary,\n    MIN(salary) OVER (PARTITION BY department_id) AS dept_min_salary,\n    MAX(salary) OVER (PARTITION BY department_id) AS dept_max_salary,\n\n    -- Employee ranking within department\n    ROW_NUMBER() OVER (PARTITION BY department_id ORDER BY salary DESC) AS dept_salary_rank,\n    RANK() OVER (PARTITION BY department_id ORDER BY salary DESC) AS dept_salary_rank_with_ties,\n    PERCENT_RANK() OVER (PARTITION BY department_id ORDER BY salary DESC) AS dept_percentile,\n\n    -- Compare with previous employee in department\n    LAG(salary) OVER (PARTITION BY department_id ORDER BY salary DESC) AS next_highest_salary,\n    LEAD(salary) OVER (PARTITION BY department_id ORDER BY salary DESC) AS next_lowest_salary,\n\n    -- Running totals and moving averages\n    SUM(salary) OVER (PARTITION BY department_id ORDER BY salary DESC ROWS UNBOUNDED PRECEDING) AS running_dept_total,\n    AVG(salary) OVER (ORDER BY salary DESC ROWS 2 PRECEDING) AS moving_avg_3_employees\n\nFROM employees\nORDER BY department_id, salary DESC;\n</code></pre>"},{"location":"concepts/SQL/window-functions/window-functions-overview/#performance-considerations","title":"\u26a1 Performance Considerations","text":"<ol> <li>Index Usage: Window functions benefit from indexes on PARTITION BY and ORDER BY columns</li> <li>Frame Optimization: PostgreSQL optimizes <code>ROWS UNBOUNDED PRECEDING</code> frames efficiently</li> <li>Memory: Large partitions may require significant memory for sorting</li> <li>Parallel Processing: PostgreSQL can parallelize window function execution in some cases</li> </ol>"},{"location":"concepts/SQL/window-functions/window-functions-overview/#comparison-with-other-databases","title":"\ud83d\udd04 Comparison with Other Databases","text":"Feature PostgreSQL SQL Server Oracle MySQL Named Windows \u2705 \u2705 \u2705 \u274c RANGE Frames \u2705 \u2705 \u2705 \u274c GROUPS Frames \u2705 (v11+) \u274c \u274c \u274c LAG/LEAD \u2705 \u2705 \u2705 \u2705 (8.0+) PERCENT_RANK \u2705 \u2705 \u2705 \u274c CUME_DIST \u2705 \u2705 \u2705 \u274c"},{"location":"concepts/SQL/window-functions/window-functions-overview/#best-practices","title":"\ud83d\ude80 Best Practices","text":"<ol> <li>Use Named Windows for complex queries with multiple window functions</li> <li>Index Strategically on window partitioning and ordering columns</li> <li>Be Careful with Frames - default frame can be expensive for large datasets</li> <li>Test Performance - window functions can be resource-intensive</li> <li>Consider Alternatives - sometimes CTEs or subqueries are clearer for simple cases</li> </ol>"},{"location":"concepts/SQL/window-functions/window-functions-overview/#related-topics","title":"\ud83d\udcda Related Topics","text":"<ul> <li><code>concepts/SQL/aggregation/aggregate-functions.md</code> - Basic aggregation concepts</li> <li><code>concepts/SQL/cte/cte-vs-window-comparison.md</code> - When to use CTEs vs window functions</li> </ul>"},{"location":"concepts/System-Design/","title":"System Design &amp; Advanced Concepts","text":"<p>This section covers advanced technical concepts and system design patterns commonly discussed in technical interviews, with real-world examples from Netflix and other large-scale systems.</p>"},{"location":"concepts/System-Design/#topics","title":"Topics","text":""},{"location":"concepts/System-Design/#system-architecture","title":"\ud83d\ude80 System Architecture","text":"<p>High-level system design patterns and architectural decisions for large-scale applications.</p>"},{"location":"concepts/System-Design/#performance-optimization","title":"\u26a1 Performance Optimization","text":"<p>Performance optimization techniques, caching strategies, and scalability solutions.</p>"},{"location":"concepts/System-Design/#experimentation","title":"\ud83e\uddea Experimentation","text":"<p>A/B testing frameworks, experimentation platforms, and statistical methodologies.</p>"},{"location":"concepts/System-Design/#case-studies","title":"\ud83d\udcda Case Studies","text":"<p>Real-world case studies and architectural decisions from engineering challenges at Netflix and other companies.</p>"},{"location":"concepts/System-Design/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand microservices and distributed system design</li> <li>Learn performance optimization and caching strategies</li> <li>Master A/B testing and experimentation frameworks</li> <li>Analyze real-world architectural challenges and solutions</li> <li>Apply system design patterns to large-scale applications</li> </ul>"},{"location":"concepts/System-Design/#navigation","title":"\ud83d\udd17 Navigation","text":"<ul> <li>\u2b05\ufe0f Back to Concepts Overview</li> <li>\ud83d\udcca SQL Concepts</li> <li>\ud83d\udccb Data Modeling</li> <li>\ud83c\udfe2 Company Interview Prep</li> </ul>"},{"location":"concepts/System-Design/case-studies/","title":"Case Studies","text":"<p>This section contains comprehensive real-world case studies and architectural decisions from large-scale streaming platforms, providing insights into how complex problems are solved at massive scale.</p>"},{"location":"concepts/System-Design/case-studies/#top-10-feature-implementation-case-study","title":"\ud83d\udcca Top 10 Feature Implementation Case Study","text":""},{"location":"concepts/System-Design/case-studies/#overview","title":"Overview","text":"<p>Complete end-to-end case study for building Netflix's \"Top 10 shows by country per day\" feature with data modeling, SQL queries, and data quality checks.</p>"},{"location":"concepts/System-Design/case-studies/#problem-statement","title":"Problem Statement","text":"<p>Build the backend for the Top 10 shows by country per day shown on the homepage that must be:</p> <ul> <li>Accurate (deduped, reconciled)</li> <li>Fresh (ready by 06:00 local time)</li> <li>Fast (sub-second read latency)</li> <li>Explainable (traceable to source events)</li> </ul>"},{"location":"concepts/System-Design/case-studies/#1-data-model-design","title":"1. Data Model Design","text":""},{"location":"concepts/System-Design/case-studies/#core-tables","title":"Core Tables","text":"<pre><code>-- Dimensions\nCREATE TABLE dim_users (\n  user_id BIGINT PRIMARY KEY,\n  signup_date DATE,\n  country STRING,\n  device_type STRING,\n  is_active BOOLEAN,\n  created_at TIMESTAMP,\n  updated_at TIMESTAMP\n);\n\nCREATE TABLE dim_shows (\n  show_id BIGINT PRIMARY KEY,\n  title STRING,\n  genre STRING,\n  is_original BOOLEAN,\n  release_date DATE,\n  maturity_rating STRING\n);\n\n-- Facts\nCREATE TABLE fact_watch_events (\n  event_id STRING,\n  user_id BIGINT,\n  show_id BIGINT,\n  country STRING,\n  watch_seconds INT,\n  event_ts TIMESTAMP,\n  src STRING,\n  ingestion_ts TIMESTAMP,\n  event_date DATE GENERATED ALWAYS AS (DATE(event_ts))\n) PARTITIONED BY (event_date);\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#serving-layer-tables","title":"Serving Layer Tables","text":"<pre><code>-- Daily aggregates for performance\nCREATE TABLE agg_watch_day_country_show (\n  event_date DATE,\n  country STRING,\n  show_id BIGINT,\n  total_watch_seconds BIGINT,\n  viewers BIGINT,\n  rank_in_country INT,\n  PRIMARY KEY (event_date, country, show_id)\n);\n\n-- Pre-computed Top 10 for fast reads\nCREATE TABLE top10_country_day (\n  event_date DATE,\n  country STRING,\n  rank_position INT,\n  show_id BIGINT,\n  total_watch_seconds BIGINT,\n  PRIMARY KEY (event_date, country, rank_position)\n);\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#2-etl-pipeline-architecture","title":"2. ETL Pipeline Architecture","text":""},{"location":"concepts/System-Design/case-studies/#ingestion-strategy","title":"Ingestion Strategy","text":"<pre><code>-- Raw event ingestion\nCREATE TABLE raw_watch_events (\n  event_id STRING,\n  user_id BIGINT,\n  show_id BIGINT,\n  country STRING,\n  watch_seconds INT,\n  event_ts TIMESTAMP,\n  src STRING,\n  ingestion_ts TIMESTAMP\n) PARTITIONED BY (dt DATE);\n\n-- Deduplication logic\nCREATE TABLE deduped_events AS\nSELECT *\nFROM (\n  SELECT *,\n    ROW_NUMBER() OVER (\n      PARTITION BY event_id\n      ORDER BY ingestion_ts DESC\n    ) AS rn\n  FROM raw_watch_events\n  WHERE dt = CURRENT_DATE - INTERVAL '1' DAY\n) t\nWHERE rn = 1;\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#daily-aggregation-job","title":"Daily Aggregation Job","text":"<pre><code>-- Build daily aggregates with late data handling\nCREATE OR REPLACE TABLE agg_watch_day_country_show AS\nWITH base AS (\n  SELECT\n    DATE(event_ts) AS event_date,\n    country,\n    show_id,\n    SUM(watch_seconds) AS total_watch_seconds,\n    COUNT(DISTINCT user_id) AS viewers\n  FROM fact_watch_events\n  WHERE event_ts &gt;= DATE_TRUNC('day', CURRENT_DATE - INTERVAL '2' DAY)\n    AND event_ts &lt; CURRENT_DATE\n  GROUP BY DATE(event_ts), country, show_id\n),\nranked AS (\n  SELECT *,\n    RANK() OVER (\n      PARTITION BY event_date, country\n      ORDER BY total_watch_seconds DESC, viewers DESC, show_id\n    ) AS rnk\n  FROM base\n)\nSELECT * FROM ranked;\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#top-10-materialization","title":"Top 10 Materialization","text":"<pre><code>CREATE OR REPLACE TABLE top10_country_day AS\nSELECT\n  event_date,\n  country,\n  rnk AS rank_position,\n  show_id,\n  total_watch_seconds\nFROM agg_watch_day_country_show\nWHERE rnk &lt;= 10\nORDER BY event_date, country, rnk;\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#3-core-sql-queries","title":"3. Core SQL Queries","text":""},{"location":"concepts/System-Design/case-studies/#homepage-api-query","title":"Homepage API Query","text":"<pre><code>-- Get today's Top 10 for user's country\nSELECT\n  rank_position,\n  show_id,\n  total_watch_seconds,\n  ds.title,\n  ds.genre,\n  ds.is_original\nFROM top10_country_day t\nJOIN dim_shows ds ON t.show_id = ds.show_id\nWHERE country = 'US'\n  AND event_date = CURRENT_DATE - INTERVAL '1' DAY\nORDER BY rank_position;\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#analytics-queries","title":"Analytics Queries","text":"<pre><code>-- Week-over-week trend analysis\nWITH this_week AS (\n  SELECT country, show_id, SUM(total_watch_seconds) AS w_watch\n  FROM agg_watch_day_country_show\n  WHERE event_date &gt;= DATE_TRUNC('week', CURRENT_DATE) - INTERVAL '7' DAY\n    AND event_date &lt; DATE_TRUNC('week', CURRENT_DATE)\n  GROUP BY country, show_id\n),\nprev_week AS (\n  SELECT country, show_id, SUM(total_watch_seconds) AS w_watch_prev\n  FROM agg_watch_day_country_show\n  WHERE event_date &gt;= DATE_TRUNC('week', CURRENT_DATE) - INTERVAL '14' DAY\n    AND event_date &lt; DATE_TRUNC('week', CURRENT_DATE) - INTERVAL '7' DAY\n  GROUP BY country, show_id\n)\nSELECT\n  t.country, t.show_id, ds.title,\n  t.w_watch,\n  p.w_watch_prev,\n  (t.w_watch - COALESCE(p.w_watch_prev,0)) AS delta,\n  CASE WHEN COALESCE(p.w_watch_prev,0) = 0 THEN NULL\n       ELSE (t.w_watch - p.w_watch_prev) * 100.0 / p.w_watch_prev END AS pct_change\nFROM this_week t\nLEFT JOIN prev_week p USING (country, show_id)\nJOIN dim_shows ds ON t.show_id = ds.show_id\nORDER BY pct_change DESC NULLS LAST\nLIMIT 20;\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#originals-vs-licensed-analysis","title":"Originals vs Licensed Analysis","text":"<pre><code>SELECT\n  a.event_date,\n  a.country,\n  SUM(CASE WHEN s.is_original THEN a.total_watch_seconds ELSE 0 END) AS originals_watch,\n  SUM(a.total_watch_seconds) AS total_watch,\n  CASE WHEN SUM(a.total_watch_seconds) = 0 THEN 0\n       ELSE SUM(CASE WHEN s.is_original THEN a.total_watch_seconds ELSE 0 END) * 100.0\n           / SUM(a.total_watch_seconds) END AS originals_share_pct\nFROM agg_watch_day_country_show a\nJOIN dim_shows s ON a.show_id = s.show_id\nGROUP BY a.event_date, a.country\nORDER BY a.event_date, a.country;\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#4-data-quality-framework","title":"4. Data Quality Framework","text":""},{"location":"concepts/System-Design/case-studies/#completeness-checks","title":"Completeness Checks","text":"<pre><code>-- Ensure partitions exist for all countries yesterday\nSELECT country\nFROM dim_geo\nWHERE country NOT IN (\n  SELECT DISTINCT country\n  FROM agg_watch_day_country_show\n  WHERE event_date = CURRENT_DATE - INTERVAL '1' DAY\n);\n\n-- Check freshness\nSELECT MAX(ingestion_ts) AS last_arrival\nFROM fact_watch_events\nWHERE DATE(event_ts) = CURRENT_DATE - INTERVAL '1' DAY;\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#accuracy-uniqueness-checks","title":"Accuracy &amp; Uniqueness Checks","text":"<pre><code>-- Deduplication verification\nSELECT event_id\nFROM fact_watch_events\nGROUP BY event_id\nHAVING COUNT(*) &gt; 1;\n\n-- Data validation\nSELECT COUNT(*) AS bad_rows\nFROM fact_watch_events\nWHERE watch_seconds &lt; 0 OR watch_seconds &gt; 24*60*60;\n\n-- Referential integrity\nSELECT COUNT(*) AS orphans\nFROM fact_watch_events e\nLEFT JOIN dim_users u ON e.user_id = u.user_id\nWHERE u.user_id IS NULL;\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#reconciliation-checks","title":"Reconciliation Checks","text":"<pre><code>-- Compare rollup to source with tolerance\nWITH src AS (\n  SELECT DATE(event_ts) d, country, show_id, SUM(watch_seconds) s\n  FROM fact_watch_events\n  WHERE DATE(event_ts) = CURRENT_DATE - INTERVAL '1' DAY\n  GROUP BY DATE(event_ts), country, show_id\n),\nagg AS (\n  SELECT event_date d, country, show_id, total_watch_seconds s\n  FROM agg_watch_day_country_show\n  WHERE event_date = CURRENT_DATE - INTERVAL '1' DAY\n)\nSELECT *\nFROM src s\nJOIN agg a USING (d, country, show_id)\nWHERE ABS(s.s - a.s) &gt; 5;\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#5-performance-optimizations","title":"5. Performance Optimizations","text":""},{"location":"concepts/System-Design/case-studies/#indexing-strategy","title":"Indexing Strategy","text":"<pre><code>-- Composite indexes for common query patterns\nCREATE INDEX idx_user_watch ON fact_watch_events (user_id, event_ts);\nCREATE INDEX idx_country_show ON fact_watch_events (country, show_id, event_ts);\nCREATE INDEX idx_show_country_date ON agg_watch_day_country_show (show_id, country, event_date);\n\n-- Covering index for Top 10 queries\nCREATE INDEX idx_top10_covering ON top10_country_day (country, event_date, rank_position)\nINCLUDE (show_id, total_watch_seconds);\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#partitioning-clustering","title":"Partitioning &amp; Clustering","text":"<pre><code>-- Optimize for time-based queries\nALTER TABLE fact_watch_events\nPARTITION BY (event_date)\nCLUSTER BY (country, show_id);\n\n-- Optimize for country-based queries\nALTER TABLE agg_watch_day_country_show\nCLUSTER BY (country, event_date);\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#caching-strategy","title":"Caching Strategy","text":"<pre><code>-- Cache Top 10 results for popular countries\nCREATE TABLE top10_cache (\n  country STRING,\n  event_date DATE,\n  results JSON,\n  last_updated TIMESTAMP,\n  ttl_minutes INT DEFAULT 60,\n  PRIMARY KEY (country, event_date)\n);\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#6-monitoring-alerting","title":"6. Monitoring &amp; Alerting","text":""},{"location":"concepts/System-Design/case-studies/#sla-monitoring","title":"SLA Monitoring","text":"<pre><code>-- Top 10 availability check\nSELECT COUNT(*) AS top10_ready\nFROM top10_country_day\nWHERE event_date = CURRENT_DATE - INTERVAL '1' DAY;\n\n-- Pipeline health monitoring\nCREATE TABLE pipeline_health (\n  pipeline_name STRING,\n  check_name STRING,\n  status STRING,\n  check_ts TIMESTAMP,\n  details JSON,\n  PRIMARY KEY (pipeline_name, check_name, check_ts)\n);\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#7-edge-cases-business-rules","title":"7. Edge Cases &amp; Business Rules","text":""},{"location":"concepts/System-Design/case-studies/#content-merges-splits","title":"Content Merges &amp; Splits","text":"<pre><code>-- Handle show ID changes due to content merges\nCREATE TABLE show_canonical_map (\n  old_show_id BIGINT,\n  new_show_id BIGINT,\n  merge_date DATE,\n  PRIMARY KEY (old_show_id)\n);\n\n-- Query with canonical mapping\nSELECT\n  COALESCE(scm.new_show_id, t.show_id) AS canonical_show_id,\n  SUM(t.total_watch_seconds) AS total_watch\nFROM agg_watch_day_country_show t\nLEFT JOIN show_canonical_map scm ON t.show_id = scm.old_show_id\nWHERE t.event_date = CURRENT_DATE - INTERVAL '1' DAY\nGROUP BY COALESCE(scm.new_show_id, t.show_id);\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#kids-content-regional-restrictions","title":"Kids Content &amp; Regional Restrictions","text":"<pre><code>-- Exclude kids content from Top 10 (business rule)\nCREATE TABLE content_restrictions (\n  show_id BIGINT,\n  restriction_type STRING,         -- 'kids_only', 'region_blocked'\n  region_code STRING,\n  PRIMARY KEY (show_id, restriction_type, region_code)\n);\n\n-- Apply restrictions in Top 10 calculation\nSELECT *\nFROM agg_watch_day_country_show a\nLEFT JOIN content_restrictions cr\n  ON a.show_id = cr.show_id\n  AND cr.restriction_type = 'region_blocked'\n  AND cr.region_code = a.country\nWHERE cr.show_id IS NULL;  -- Only include non-blocked content\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#key-success-metrics","title":"Key Success Metrics","text":"<ul> <li>Data Freshness: Top 10 available by 06:00 local time</li> <li>Query Performance: &lt; 100ms P95 for homepage queries</li> <li>Data Accuracy: &lt; 0.1% discrepancy between source and aggregates</li> <li>System Reliability: 99.9% uptime for Top 10 service</li> </ul>"},{"location":"concepts/System-Design/case-studies/#interview-talking-points","title":"Interview Talking Points","text":"<ul> <li>Start with serving question: \"Top 10 by country per day\"</li> <li>Propose star schema: fact_watch_events + dimensions</li> <li>Show precise SQL: CTE for aggregation, RANK() for ordering</li> <li>Call out data quality: completeness, deduplication, reconciliation</li> <li>Mention performance: pre-computation, partitioning, caching</li> <li>Address scale: partitioning, clustering, late data handling</li> </ul> <p>This case study demonstrates end-to-end data engineering thinking from modeling to production monitoring.</p>"},{"location":"concepts/System-Design/case-studies/#content-delivery-network-cdn-optimization","title":"\ud83c\udfac Content Delivery Network (CDN) Optimization","text":"<p>Challenge Streaming platforms handle massive traffic with varying content popularity and user locations.</p>"},{"location":"concepts/System-Design/case-studies/#architecture-evolution","title":"Architecture Evolution","text":"<p>Phase 1 - Traditional CDN::</p> <pre><code>// Simple geographic routing\nfunction routeContent(userLocation, contentId) {\n  const region = getRegionFromLocation(userLocation);\n  const edgeServer = cdnMap[region][contentId];\n\n  if (edgeServer.hasContent(contentId)) {\n    return edgeServer;\n  } else {\n    return originServer.pullContent(contentId);\n  }\n}\n</code></pre> <p>Problems::</p> <ul> <li>Cache misses for new/unpopular content</li> <li>Hot spots during premieres</li> <li>Inefficient for global distribution</li> </ul> <p>Phase 2 - Intelligent CDN with Predictive Caching::</p> <pre><code>class PredictiveCDNManager:\n    def __init__(self):\n        self.content_popularity_model = MLModel.load('popularity_predictor')\n        self.user_behavior_analyzer = StreamingAnalyzer()\n\n    def pre_cache_content(self, region, upcoming_content):\n        \"\"\"Pre-cache content based on predictions\"\"\"\n        popularity_score = self.content_popularity_model.predict(\n            upcoming_content, region\n        )\n\n        if popularity_score &gt; 0.8:  # High popularity threshold\n            self.prefetch_to_edge_servers(upcoming_content, region)\n\n    def optimize_bitrate(self, user_profile, network_conditions):\n        \"\"\"Dynamically adjust streaming quality\"\"\"\n        device_capability = user_profile['device_type']\n        bandwidth = network_conditions['measured_speed']\n\n        optimal_bitrate = self.select_optimal_bitrate(\n            device_capability, bandwidth\n        )\n\n        return self.generate_adaptive_playlist(\n            content_id, optimal_bitrate\n        )\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#machine-learning-integration","title":"Machine Learning Integration","text":"<p>Content Popularity Prediction::</p> <pre><code>-- Training data for ML model\nCREATE TABLE content_features (\n  title_id BIGINT PRIMARY KEY,\n  genre STRING,\n  cast_popularity_score DOUBLE,\n  marketing_spend DOUBLE,\n  social_media_mentions BIGINT,\n  trailer_views BIGINT,\n  release_date DATE,\n  target_audience_age INT\n);\n\n-- Real-time feature updates\nCREATE STREAM content_trending_metrics AS\nSELECT\n  title_id,\n  COUNT(*) as search_count,\n  HOP_START(event_ts, INTERVAL '1' HOUR, INTERVAL '24' HOUR) as window_start\nFROM search_events\nWHERE search_query LIKE CONCAT('%', title_name, '%')\nGROUP BY title_id, HOP(event_ts, INTERVAL '1' HOUR, INTERVAL '24' HOUR);\n</code></pre> <p>Results*</p> <ul> <li>40% reduction in CDN costs through intelligent caching</li> <li>Improved streaming quality with 50% fewer rebuffers</li> <li>90% cache hit rate for popular content</li> </ul>"},{"location":"concepts/System-Design/case-studies/#personalization-engine-at-scale","title":"\ud83d\udc65 Personalization Engine at Scale","text":"<p>Challenge</p> <p>Provide personalized recommendations for hundreds of millions of users across global regions with sub-200ms latency.</p>"},{"location":"concepts/System-Design/case-studies/#architecture-design","title":"Architecture Design","text":"<p>Multi-Stage Recommendation Pipeline::</p> <pre><code>-- Stage 1: Candidate Generation (Batch)\nCREATE TABLE candidate_sets AS\nSELECT\n  profile_id,\n  ARRAY_AGG(title_id ORDER BY score DESC) as candidates,\n  generation_timestamp\nFROM (\n  SELECT\n    profile_id,\n    title_id,\n    collaborative_filtering_score + content_based_score as score\n  FROM user_item_matrix uim\n  JOIN content_features cf ON uim.title_id = cf.title_id\n  WHERE uim.interaction_strength &gt; 0.1\n) t\nGROUP BY profile_id;\n\n-- Stage 2: Ranking (Real-time)\nCREATE TABLE personalized_recs AS\nSELECT\n  profile_id,\n  title_id,\n  ROW_NUMBER() OVER (PARTITION BY profile_id ORDER BY final_score DESC) as rank_position,\n  context_type,  -- 'home', 'because_you_watched', etc.\n  generation_ts\nFROM (\n  SELECT\n    profile_id,\n    title_id,\n    machine_learning_ranking_score +\n    diversity_penalty +\n    recency_boost +\n    device_compatibility_score as final_score\n  FROM candidate_sets cs\n  JOIN real_time_features rtf ON cs.profile_id = rtf.profile_id\n  CROSS JOIN user_context uc\n) ranked_candidates\nWHERE rank_position &lt;= 100;\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#key-components","title":"Key Components","text":"<p>Feature Store Architecture::</p> <pre><code>class FeatureStore:\n    def __init__(self):\n        self.redis_cache = RedisCluster()\n        self.historical_store = Cassandra()\n\n    def get_user_features(self, profile_id):\n        \"\"\"Multi-level feature retrieval\"\"\"\n        # Hot features from Redis\n        hot_features = self.redis_cache.hgetall(f\"user:{profile_id}:hot\")\n\n        # Warm features from Cassandra\n        warm_features = self.historical_store.get_recent_features(\n            profile_id, days=30\n        )\n\n        return self.merge_features(hot_features, warm_features)\n\n    def update_feature(self, profile_id, feature_name, value):\n        \"\"\"Real-time feature updates\"\"\"\n        self.redis_cache.hset(f\"user:{profile_id}:hot\", feature_name, value)\n\n        # Async write to historical store\n        self.historical_store.insert_async(\n            profile_id, feature_name, value, timestamp\n        )\n</code></pre> <p>Model Serving Infrastructure::</p> <pre><code>-- Online model serving with A/B testing\nCREATE TABLE model_serving_log (\n  profile_id BIGINT,\n  model_version STRING,\n  experiment_id STRING,\n  recommendation_context STRING,\n  response_time_ms INT,\n  cache_hit BOOLEAN,\n  timestamp TIMESTAMP\n) PARTITIONED BY (dt DATE);\n\n-- Model performance monitoring\nSELECT\n  model_version,\n  AVG(response_time_ms) as avg_latency,\n  PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY response_time_ms) as p95_latency,\n  COUNT(CASE WHEN cache_hit THEN 1 END) / COUNT(*) as cache_hit_rate\nFROM model_serving_log\nWHERE dt &gt;= CURRENT_DATE - INTERVAL '7' DAY\nGROUP BY model_version;\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#scaling-challenges-solved","title":"Scaling Challenges Solved","text":"<p>Cold Start Problem::</p> <ul> <li>Solution: Content-based recommendations using user demographics</li> <li>Implementation: Hybrid approach combining collaborative and content-based filtering</li> </ul> <p>Data Freshness::</p> <ul> <li>Solution: Streaming feature updates with Lambda architecture</li> <li>Implementation: Kafka streams for real-time updates, batch for comprehensive features</li> </ul> <p>Global Consistency::</p> <ul> <li>Solution: Distributed model training with model versioning</li> <li>Implementation: MLflow for model lifecycle management</li> </ul>"},{"location":"concepts/System-Design/case-studies/#performance-metrics","title":"Performance Metrics","text":"<ul> <li>200ms P95 latency for recommendation serving</li> <li>Handles 1M+ requests/second during peak hours</li> <li>99.5% recommendation accuracy vs. random selection</li> <li>&lt;1% model serving errors with comprehensive monitoring</li> </ul>"},{"location":"concepts/System-Design/case-studies/#security-and-privacy-at-scale","title":"\ud83d\udd10 Security and Privacy at Scale","text":"<p>Challenge</p> <p>Protect user privacy while enabling personalization across hundreds of millions of users globally.</p>"},{"location":"concepts/System-Design/case-studies/#privacy-first-architecture","title":"Privacy-First Architecture","text":"<p>Data Minimization::</p> <pre><code>-- Only store necessary data for recommendations\nCREATE TABLE privacy_safe_user_profile (\n  profile_id BIGINT PRIMARY KEY,\n  hashed_user_id STRING,  -- SHA-256 hash, not actual ID\n  age_group STRING,       -- 18-24, 25-34, etc., not exact age\n  content_preferences ARRAY&lt;STRING&gt;,  -- anonymized categories\n  device_type STRING,\n  country_code STRING,\n  language_code STRING,\n  timezone_group STRING,  -- Eastern, Pacific, etc.\n  created_at DATE         -- not exact timestamp\n);\n\n-- No PII in recommendation features\nCREATE TABLE recommendation_features (\n  profile_hash STRING,\n  content_category STRING,\n  interaction_type STRING,  -- 'viewed', 'liked', 'disliked'\n  interaction_timestamp DATE,\n  device_category STRING\n);\n</code></pre> <p>Differential Privacy Implementation::</p> <pre><code>class DifferentialPrivacyEngine:\n    def __init__(self, epsilon=1.0):\n        self.epsilon = epsilon  # Privacy budget\n        self.noise_generator = LaplaceNoise(sigma=1/epsilon)\n\n    def add_noise_to_aggregate(self, true_count, sensitivity=1):\n        \"\"\"Add calibrated noise to prevent re-identification\"\"\"\n        noise = self.noise_generator.sample()\n        return true_count + noise\n\n    def privatize_histogram(self, user_histogram):\n        \"\"\"Apply differential privacy to user behavior histograms\"\"\"\n        privatized = {}\n        for bucket, count in user_histogram.items():\n            privatized[bucket] = self.add_noise_to_aggregate(count)\n        return privatized\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#compliance-and-governance","title":"Compliance and Governance","text":"<p>GDPR Compliance Architecture::</p> <pre><code>-- User consent and data control\nCREATE TABLE user_consent (\n  profile_id BIGINT PRIMARY KEY,\n  consent_version INT,\n  marketing_consent BOOLEAN,\n  personalization_consent BOOLEAN,\n  analytics_consent BOOLEAN,\n  data_retention_days INT,\n  consent_updated_at TIMESTAMP\n);\n\n-- Automatic data deletion\nCREATE PROCEDURE purge_expired_data()\nBEGIN\n  DELETE FROM user_activity\n  WHERE profile_id IN (\n    SELECT profile_id\n    FROM user_consent\n    WHERE data_retention_days &gt; 0\n      AND created_at &lt; CURRENT_DATE - INTERVAL data_retention_days DAY\n  );\nEND;\n</code></pre>"},{"location":"concepts/System-Design/case-studies/#security-measures","title":"Security Measures","text":"<ul> <li>End-to-end encryption for all user data in transit</li> <li>Column-level encryption for sensitive fields at rest</li> <li>Zero-trust architecture with continuous authentication</li> <li>Automated security scanning and penetration testing</li> </ul> <p>Results</p> <ul> <li>100% GDPR compliance across all regions</li> <li>Zero data breaches in 5+ years</li> <li>Maintained personalization quality while protecting privacy</li> <li>User trust scores consistently above 90%</li> </ul>"},{"location":"concepts/System-Design/case-studies/#cross-platform-content-discovery","title":"\ud83d\udcf1 Cross-Platform Content Discovery","text":"<p>Challenge</p> <p>Provide consistent content discovery experience across 1000+ device types and platforms.</p>"},{"location":"concepts/System-Design/case-studies/#device-aware-recommendation-engine","title":"Device-Aware Recommendation Engine","text":"<p>Device Capability Detection::</p> <pre><code>// Client-side device capability detection\nclass DeviceProfiler {\n  static getCapabilities() {\n    return {\n      screenResolution: `${screen.width}x${screen.height}`,\n      colorDepth: screen.colorDepth,\n      touchSupport: 'ontouchstart' in window,\n      gpuAccelerated: this.detectGPUAcceleration(),\n      networkSpeed: navigator.connection?.effectiveType,\n      memorySize: navigator.deviceMemory,\n      supportedCodecs: this.detectVideoCodecs()\n    };\n  }\n}\n</code></pre> <p>Adaptive Content Delivery::</p> <pre><code>-- Device-aware content selection\nCREATE TABLE device_optimized_content (\n  title_id BIGINT,\n  device_category STRING,  -- 'mobile', 'tv', 'web', 'gaming_console'\n  optimal_bitrate INT,\n  supported_resolutions ARRAY&lt;STRING&gt;,\n  recommended_encoding STRING,\n  special_features JSON,   -- HDR, Dolby Atmos, etc.\n  PRIMARY KEY (title_id, device_category)\n);\n\n-- Runtime content adaptation\nSELECT\n  dc.title_id,\n  dc.optimal_bitrate,\n  dc.supported_resolutions,\n  dc.recommended_encoding\nFROM device_optimized_content dc\nJOIN user_device_profile udp ON dc.device_category = udp.device_category\nWHERE udp.profile_id = :profile_id\n  AND dc.title_id IN (SELECT title_id FROM user_recommendations);\n</code></pre> <p>Results</p> <ul> <li>Consistent experience across all device types</li> <li>Optimized streaming quality based on device capabilities</li> <li>Reduced support tickets by 60% through device-aware recommendations</li> <li>Improved engagement with platform-specific content discovery</li> </ul>"},{"location":"concepts/System-Design/case-studies/#key-architectural-patterns","title":"\ud83d\udcda Key Architectural Patterns","text":""},{"location":"concepts/System-Design/case-studies/#1-event-driven-architecture","title":"1. Event-Driven Architecture","text":"<ul> <li>Loose coupling between services</li> <li>Scalable for unpredictable workloads</li> <li>Auditable with complete event trails</li> </ul>"},{"location":"concepts/System-Design/case-studies/#2-data-mesh-architecture","title":"2. Data Mesh Architecture","text":"<ul> <li>Domain ownership of data products</li> <li>Federated governance with central standards</li> <li>Self-service analytics for teams</li> </ul>"},{"location":"concepts/System-Design/case-studies/#3-streaming-first-processing","title":"3. Streaming-First Processing","text":"<ul> <li>Real-time insights with low latency</li> <li>Backpressure handling for traffic spikes</li> <li>Exactly-once processing for data consistency</li> </ul>"},{"location":"concepts/System-Design/case-studies/#4-privacy-by-design","title":"4. Privacy-by-Design","text":"<ul> <li>Data minimization principles</li> <li>Consent management integration</li> <li>Automated compliance checking</li> </ul>"},{"location":"concepts/System-Design/case-studies/#5-multi-region-active-active","title":"5. Multi-Region Active-Active","text":"<ul> <li>Global availability with local performance</li> <li>Data residency compliance</li> <li>Disaster recovery capabilities</li> </ul>"},{"location":"concepts/System-Design/case-studies/#decision-making-framework","title":"\ud83d\udee0\ufe0f Decision-Making Framework","text":"<p>When faced with architectural challenges, consider this framework:</p> <ol> <li>Define Success Metrics - What are we optimizing for?</li> <li>Consider Scale - How will this work at large scale?</li> <li>Evaluate Trade-offs - What's the cost of each option?</li> <li>Prototype and Test - Validate assumptions with data</li> <li>Monitor and Iterate - Continuous improvement based on metrics</li> <li>Document Decisions - Share learnings with the organization</li> </ol>"},{"location":"concepts/System-Design/case-studies/#example-decision-matrix","title":"Example Decision Matrix","text":"Option Latency Cost Complexity Scalability Result Monolithic DB \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50 Chose different approach Microservices \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50 \u2b50\u2b50\u2b50 Selected for flexibility Event Sourcing \u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 Implemented with CQRS <p>\ud83d\udd17 Cross-References::</p> <ul> <li>System Architecture - Technical implementation details</li> <li>Performance Optimization - Optimization techniques used</li> <li>Data Modeling Patterns - Schema design decisions</li> </ul> <p>Navigate back to System Design &amp; Advanced Concepts</p>"},{"location":"concepts/System-Design/experimentation/","title":"Experimentation and A/B Testing","text":"<p>This section covers experimentation platforms, A/B testing frameworks, and statistical methodologies for measuring the impact of product changes at large-scale streaming platforms.</p>"},{"location":"concepts/System-Design/experimentation/#ab-testing-framework","title":"\ud83e\uddea A/B Testing Framework","text":""},{"location":"concepts/System-Design/experimentation/#core-components","title":"Core Components","text":"<p>Experiment Design::</p> <pre><code>-- Experiment metadata table\nCREATE TABLE experiments (\n  exp_id STRING PRIMARY KEY,\n  name STRING,\n  hypothesis STRING,\n  owner_team STRING,\n  start_time TIMESTAMP,\n  end_time TIMESTAMP,\n  status STRING,  -- DRAFT, RUNNING, COMPLETED, CANCELLED\n  unit_type STRING,  -- PROFILE, ACCOUNT, DEVICE\n  primary_kpi STRING,  -- watch_hours, retention_rate, etc.\n  secondary_kpis ARRAY&lt;STRING&gt;,\n  target_population STRING,  -- ALL, NEW_USERS, PREMIUM, etc.\n  randomization_method STRING,  -- SIMPLE, STRATIFIED, CUPED\n  created_at TIMESTAMP\n);\n\n-- Variant definitions\nCREATE TABLE experiment_variants (\n  exp_id STRING REFERENCES experiments(exp_id),\n  variant_id STRING,  -- control, treatment_a, treatment_b\n  variant_name STRING,\n  traffic_percentage DOUBLE,\n  description STRING,\n  PRIMARY KEY (exp_id, variant_id)\n);\n</code></pre>"},{"location":"concepts/System-Design/experimentation/#user-assignment-and-randomization","title":"User Assignment and Randomization","text":"<p>Consistent Hashing for User Assignment::</p> <pre><code>def assign_variant(user_id: str, experiment_id: str, traffic_pct: float) -&gt; str:\n    \"\"\"\n    Deterministic user assignment using consistent hashing.\n    Ensures same user always gets same variant for same experiment.\n    \"\"\"\n    hash_input = f\"{experiment_id}:{user_id}\"\n    hash_value = hash_function(hash_input)\n    normalized_hash = hash_value / max_hash_value\n\n    if normalized_hash &lt; traffic_pct / 100:\n        return \"treatment\"\n    else:\n        return \"control\"\n</code></pre>"},{"location":"concepts/System-Design/experimentation/#stratified-randomization","title":"Stratified Randomization","text":"<p>By User Segments::</p> <pre><code>-- Stratified assignment by subscription tier\nWITH user_segments AS (\n  SELECT\n    profile_id,\n    CASE\n      WHEN subscription_tier = 'BASIC' THEN 'segment_a'\n      WHEN subscription_tier = 'STANDARD' THEN 'segment_b'\n      WHEN subscription_tier = 'PREMIUM' THEN 'segment_c'\n    END as segment\n  FROM user_profiles\n),\nsegmented_assignment AS (\n  SELECT\n    profile_id,\n    segment,\n    -- Each segment gets independent randomization\n    CASE\n      WHEN hash_function(profile_id, exp_id) % 100 &lt; 50 THEN 'treatment'\n      ELSE 'control'\n    END as variant\n  FROM user_segments\n)\nSELECT * FROM segmented_assignment;\n</code></pre>"},{"location":"concepts/System-Design/experimentation/#statistical-analysis","title":"\ud83d\udcca Statistical Analysis","text":""},{"location":"concepts/System-Design/experimentation/#power-analysis-and-sample-size","title":"Power Analysis and Sample Size","text":"<p>Sample Size Calculation::</p> <pre><code>def calculate_sample_size(baseline_rate: float, mde: float,\n                         power: float = 0.8, alpha: float = 0.05) -&gt; int:\n    \"\"\"\n    Calculate required sample size for A/B test.\n\n    Args:\n        baseline_rate: Current conversion rate (e.g., 0.05 for 5%)\n        mde: Minimum detectable effect (e.g., 0.01 for 1% absolute increase)\n        power: Statistical power (1 - beta)\n        alpha: Significance level\n\n    Returns:\n        Required sample size per variant\n    \"\"\"\n    import statsmodels.stats.power as power\n\n    effect_size = mde / (baseline_rate * (1 - baseline_rate))**0.5\n    sample_size = power.tt_ind_solve_power(\n        effect_size=effect_size,\n        alpha=alpha,\n        power=power,\n        nobs1=None,\n        ratio=1\n    )\n\n    return int(sample_size)\n</code></pre>"},{"location":"concepts/System-Design/experimentation/#guardrail-metrics-and-early-stopping","title":"Guardrail Metrics and Early Stopping","text":"<p>Automated Experiment Monitoring::</p> <pre><code>-- Daily guardrail checks\nCREATE TABLE experiment_guardrails (\n  exp_id STRING,\n  metric_date DATE,\n  metric_name STRING,\n  control_value DOUBLE,\n  treatment_value DOUBLE,\n  relative_change DOUBLE,\n  statistical_significance DOUBLE,\n  PRIMARY KEY (exp_id, metric_date, metric_name)\n);\n\n-- Alert on significant degradation\nSELECT\n  exp_id,\n  metric_name,\n  relative_change,\n  statistical_significance\nFROM experiment_guardrails\nWHERE metric_date = CURRENT_DATE - INTERVAL '1' DAY\n  AND ABS(relative_change) &gt; 0.05  -- 5% change threshold\n  AND statistical_significance &lt; 0.01;  -- p-value threshold\n</code></pre>"},{"location":"concepts/System-Design/experimentation/#advanced-statistical-methods","title":"\ud83d\udcc8 Advanced Statistical Methods","text":""},{"location":"concepts/System-Design/experimentation/#cuped-controlled-experiment-using-pre-experiment-data","title":"CUPED (Controlled-experiment Using Pre-experiment Data)","text":"<p>Variance Reduction::</p> <pre><code>-- CUPED implementation for more sensitive A/B tests\nWITH pre_experiment_data AS (\n  SELECT\n    profile_id,\n    AVG(daily_watch_hours) as pre_avg_watch_hours\n  FROM watch_history\n  WHERE watch_date BETWEEN exp_start_date - INTERVAL '30' DAY\n                       AND exp_start_date - INTERVAL '1' DAY\n  GROUP BY profile_id\n),\nexperiment_results AS (\n  SELECT\n    profile_id,\n    variant,\n    AVG(daily_watch_hours) as post_avg_watch_hours\n  FROM watch_history w\n  JOIN experiment_assignments e ON w.profile_id = e.profile_id\n  WHERE watch_date BETWEEN exp_start_date AND exp_end_date\n  GROUP BY profile_id, variant\n),\ncuped_adjusted AS (\n  SELECT\n    e.variant,\n    e.post_avg_watch_hours - 0.5 * (p.pre_avg_watch_hours - overall_pre_avg) as adjusted_metric\n  FROM experiment_results e\n  JOIN pre_experiment_data p ON e.profile_id = p.profile_id\n  CROSS JOIN (SELECT AVG(pre_avg_watch_hours) as overall_pre_avg\n              FROM pre_experiment_data) overall\n)\nSELECT\n  variant,\n  AVG(adjusted_metric) as mean_adjusted_watch_hours,\n  STDDEV(adjusted_metric) as std_adjusted_watch_hours,\n  COUNT(*) as sample_size\nFROM cuped_adjusted\nGROUP BY variant;\n</code></pre>"},{"location":"concepts/System-Design/experimentation/#sequential-testing","title":"Sequential Testing","text":"<p>P-value Monitoring Over Time::</p> <pre><code>def sequential_test(control_data: List[float], treatment_data: List[float],\n                   alpha: float = 0.05) -&gt; str:\n    \"\"\"\n    Sequential testing for early stopping decisions.\n\n    Returns:\n        'continue', 'stop_early_positive', 'stop_early_negative'\n    \"\"\"\n    # Use cumulative data for sequential analysis\n    cumulative_control = np.cumsum(control_data)\n    cumulative_treatment = np.cumsum(treatment_data)\n\n    # Calculate cumulative statistics\n    n_control = len(control_data)\n    n_treatment = len(treatment_data)\n\n    if n_control == 0 or n_treatment == 0:\n        return 'continue'\n\n    # Sequential probability ratio test\n    likelihood_ratio = calculate_likelihood_ratio(cumulative_control, cumulative_treatment)\n\n    # Decision boundaries\n    upper_bound = math.log(1/alpha)\n    lower_bound = math.log(alpha)\n\n    if likelihood_ratio &gt; upper_bound:\n        return 'stop_early_positive'\n    elif likelihood_ratio &lt; lower_bound:\n        return 'stop_early_negative'\n    else:\n        return 'continue'\n</code></pre>"},{"location":"concepts/System-Design/experimentation/#experimentation-platform-architecture","title":"\ud83c\udfaf Experimentation Platform Architecture","text":""},{"location":"concepts/System-Design/experimentation/#event-tracking-and-collection","title":"Event Tracking and Collection","text":"<p>Client-Side Event Tracking::</p> <pre><code>// Modern event tracking implementation\nclass ExperimentTracker {\n  static trackExperimentEvent(experimentId, variant, eventName, metadata) {\n    const event = {\n      experiment_id: experimentId,\n      variant_id: variant,\n      event_name: eventName,\n      user_id: this.getUserId(),\n      session_id: this.getSessionId(),\n      timestamp: Date.now(),\n      metadata: metadata,\n      platform: 'web', // or mobile, tv, etc.\n      user_agent: navigator.userAgent\n    };\n\n    // Send to data collection service\n    this.sendEvent(event);\n  }\n}\n</code></pre>"},{"location":"concepts/System-Design/experimentation/#real-time-experiment-results","title":"Real-Time Experiment Results","text":"<p>Streaming Analytics::</p> <pre><code>-- Real-time experiment metrics with Kafka streams\nCREATE STREAM experiment_events (\n  exp_id STRING,\n  profile_id BIGINT,\n  variant STRING,\n  event_name STRING,\n  event_value DOUBLE,\n  event_timestamp TIMESTAMP\n) WITH (\n  KAFKA_TOPIC = 'experiment_events',\n  KEY_FORMAT = 'JSON',\n  VALUE_FORMAT = 'JSON'\n);\n\n-- Streaming aggregation for real-time dashboard\nCREATE TABLE experiment_metrics AS\nSELECT\n  exp_id,\n  variant,\n  event_name,\n  COUNT(*) as event_count,\n  AVG(event_value) as avg_value,\n  TUMBLE_START(event_timestamp, INTERVAL '5' MINUTE) as window_start\nFROM experiment_events\nGROUP BY exp_id, variant, event_name, TUMBLE(event_timestamp, INTERVAL '5' MINUTE);\n</code></pre>"},{"location":"concepts/System-Design/experimentation/#common-pitfalls-and-solutions","title":"\ud83d\udeab Common Pitfalls and Solutions","text":""},{"location":"concepts/System-Design/experimentation/#selection-bias","title":"Selection Bias","text":"<p>Problem: Non-random assignment leads to biased results.</p> <p>Solutions::</p> <ul> <li>Use proper randomization methods</li> <li>Stratify by key user characteristics</li> <li>Monitor for sample ratio mismatch (SRM)</li> </ul>"},{"location":"concepts/System-Design/experimentation/#multiple-testing-problem","title":"Multiple Testing Problem","text":"<p>Problem: Running many statistical tests increases false positive rate.</p> <p>Solutions::</p> <ul> <li>Bonferroni correction for multiple comparisons</li> <li>False Discovery Rate (FDR) control</li> <li>Pre-register hypothesis and analysis plan</li> </ul>"},{"location":"concepts/System-Design/experimentation/#simpsons-paradox","title":"Simpson's Paradox","text":"<p>Problem: Aggregate results differ from subgroup results.</p> <p>Solutions::</p> <ul> <li>Always analyze by key segments</li> <li>Include interaction terms in statistical models</li> <li>Use stratified analysis</li> </ul>"},{"location":"concepts/System-Design/experimentation/#platform-specific-experimentation-challenges","title":"\ud83d\udcca Platform-Specific Experimentation Challenges","text":""},{"location":"concepts/System-Design/experimentation/#global-experimentation","title":"Global Experimentation","text":"<ul> <li>Content Localization: Different content libraries per region</li> <li>Device Diversity: Experiments across web, mobile, smart TVs</li> <li>Time Zones: Coordinated experiment timing globally</li> </ul>"},{"location":"concepts/System-Design/experimentation/#content-discovery-experiments","title":"Content Discovery Experiments","text":"<ul> <li>Algorithm Changes: Testing new recommendation algorithms</li> <li>UI Changes: Homepage layout and personalization</li> <li>Content Promotion: Featured content and editorial curation</li> </ul>"},{"location":"concepts/System-Design/experimentation/#performance-impact-experiments","title":"Performance Impact Experiments","text":"<ul> <li>Loading Speed: Impact on engagement metrics</li> <li>Streaming Quality: Adaptive bitrate algorithms</li> <li>Caching Strategies: Content delivery optimization</li> </ul>"},{"location":"concepts/System-Design/experimentation/#experimentation-best-practices","title":"\ud83d\udee0\ufe0f Experimentation Best Practices","text":""},{"location":"concepts/System-Design/experimentation/#experiment-design-checklist","title":"Experiment Design Checklist","text":"<ul> <li>[ ] Clear hypothesis statement</li> <li>[ ] Defined primary and secondary metrics</li> <li>[ ] Power analysis for sample size</li> <li>[ ] Proper randomization method</li> <li>[ ] Guardrail metrics defined</li> <li>[ ] Analysis plan documented</li> </ul>"},{"location":"concepts/System-Design/experimentation/#during-experiment","title":"During Experiment","text":"<ul> <li>[ ] Monitor for data quality issues</li> <li>[ ] Check for unexpected side effects</li> <li>[ ] Communicate with stakeholders</li> <li>[ ] Prepare contingency plans</li> </ul>"},{"location":"concepts/System-Design/experimentation/#after-experiment","title":"After Experiment","text":"<ul> <li>[ ] Comprehensive statistical analysis</li> <li>[ ] Segment analysis for insights</li> <li>[ ] Document learnings and decisions</li> <li>[ ] Plan follow-up experiments</li> </ul>"},{"location":"concepts/System-Design/experimentation/#key-takeaways","title":"\ud83d\udcda Key Takeaways","text":"<ul> <li>Statistical Rigor: Always use proper statistical methods and power analysis</li> <li>Monitoring: Continuous monitoring prevents bad experiments from running too long</li> <li>Segmentation: Analyze results by key user segments for deeper insights</li> <li>Platform Integration: Seamless integration with product development workflow</li> <li>Ethical Considerations: Consider user experience and potential harm</li> </ul> <p>\ud83d\udd17 Cross-References::</p> <ul> <li>System Architecture - Platform architecture for experimentation</li> <li>Performance Optimization - Measuring performance impact</li> <li>Data Modeling Patterns - Event tracking schemas</li> </ul>"},{"location":"concepts/System-Design/performance-optimization/","title":"Performance Optimization","text":"<p>This section explores performance optimization techniques, caching strategies, and scalability solutions used at large-scale streaming platforms to handle massive scale while maintaining low latency.</p>"},{"location":"concepts/System-Design/performance-optimization/#database-performance-optimization","title":"\u26a1 Database Performance Optimization","text":""},{"location":"concepts/System-Design/performance-optimization/#indexing-strategies","title":"Indexing Strategies","text":"<p>Composite Indexes for Common Query Patterns::</p> <pre><code>-- Index for user watch history queries\nCREATE INDEX idx_watch_history_user_date\nON fact_watch_events (profile_id, event_ts, title_id);\n\n-- Index for content popularity queries\nCREATE INDEX idx_watch_content_region\nON fact_watch_events (title_id, region, event_ts);\n\n-- Covering index for dashboard queries\nCREATE INDEX idx_metrics_composite\nON fact_watch_events (region, device_type, event_ts)\nINCLUDE (watch_seconds);\n</code></pre>"},{"location":"concepts/System-Design/performance-optimization/#query-optimization-techniques","title":"Query Optimization Techniques","text":"<p>Partition Pruning::</p> <pre><code>-- Efficient date range queries with partition pruning\nSELECT profile_id, COUNT(*) as watch_count\nFROM fact_watch_events\nWHERE event_ts &gt;= '2025-01-01' AND event_ts &lt; '2025-02-01'\n  AND region = 'US'\nGROUP BY profile_id;\n-- Only scans January 2025 partitions\n</code></pre> <p>Materialized Views for Complex Aggregations::</p> <pre><code>-- Pre-computed daily metrics\nCREATE MATERIALIZED VIEW daily_user_metrics AS\nSELECT\n  profile_id,\n  DATE(event_ts) as activity_date,\n  COUNT(DISTINCT session_id) as sessions,\n  SUM(watch_seconds) as total_watch_time,\n  COUNT(DISTINCT title_id) as unique_titles\nFROM fact_watch_events\nWHERE event_ts &gt;= CURRENT_DATE - INTERVAL '90' DAY\nGROUP BY profile_id, DATE(event_ts);\n\n-- Refresh daily\nREFRESH MATERIALIZED VIEW daily_user_metrics;\n</code></pre>"},{"location":"concepts/System-Design/performance-optimization/#caching-strategies","title":"\ud83d\ude80 Caching Strategies","text":""},{"location":"concepts/System-Design/performance-optimization/#multi-level-caching-architecture","title":"Multi-Level Caching Architecture","text":"<p>Level 1 - Client-Side Cache::</p> <ul> <li>Browser localStorage for user preferences</li> <li>Service Worker for offline content metadata</li> <li>CDN for static assets</li> </ul> <p>Level 2 - Application Cache::</p> <ul> <li>Redis for session data and user state</li> <li>In-memory cache for frequently accessed content metadata</li> <li>Distributed cache for recommendation results</li> </ul> <p>Level 3 - Database Cache::</p> <ul> <li>Query result caching</li> <li>Prepared statement caching</li> <li>Connection pooling</li> </ul>"},{"location":"concepts/System-Design/performance-optimization/#cache-implementation-example","title":"Cache Implementation Example","text":"<pre><code>// Redis caching strategy for user recommendations\nconst cacheKey = `recs:${profileId}:${context}`;\n\nconst recommendations = await redis.get(cacheKey);\nif (!recommendations) {\n  // Fetch from database/model\n  recommendations = await fetchRecommendations(profileId, context);\n\n  // Cache with TTL\n  await redis.setex(cacheKey, 1800, JSON.stringify(recommendations)); // 30 minutes\n}\n\nreturn recommendations;\n</code></pre>"},{"location":"concepts/System-Design/performance-optimization/#load-balancing-and-traffic-management","title":"\ud83d\udcca Load Balancing and Traffic Management","text":""},{"location":"concepts/System-Design/performance-optimization/#global-load-balancing","title":"Global Load Balancing","text":"<ul> <li>GeoDNS: Route users to nearest region</li> <li>Latency-Based Routing: Direct to lowest latency endpoint</li> <li>Health Checks: Automatic failover for unhealthy instances</li> </ul>"},{"location":"concepts/System-Design/performance-optimization/#application-load-balancing","title":"Application Load Balancing","text":"<pre><code># NGINX upstream configuration\n\nupstream playback_service {\n  least_conn;\n  server playback-01:8080 max_fails=3 fail_timeout=30s;\n  server playback-02:8080 max_fails=3 fail_timeout=30s;\n  server playback-03:8080 max_fails=3 fail_timeout=30s;\n}\n\n# Rate limiting\n\nlimit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;\n\nserver {\n  listen 80;\n  location /api/playback {\n    limit_req zone=api burst=20 nodelay;\n    proxy_pass http://playback_service;\n  }\n}\n</code></pre>"},{"location":"concepts/System-Design/performance-optimization/#auto-scaling-strategies","title":"\ud83c\udfd7\ufe0f Auto-Scaling Strategies","text":""},{"location":"concepts/System-Design/performance-optimization/#horizontal-pod-autoscaling-hpa","title":"Horizontal Pod Autoscaling (HPA)","text":"<pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: playback-service-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: playback-service\n  minReplicas: 10\n  maxReplicas: 100\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n</code></pre>"},{"location":"concepts/System-Design/performance-optimization/#predictive-auto-scaling","title":"Predictive Auto-Scaling","text":"<ul> <li>Machine Learning Models: Predict traffic based on historical patterns</li> <li>Event-Driven Scaling: Scale up for new content releases</li> <li>Time-Based Scaling: Scheduled scaling for known traffic patterns</li> </ul>"},{"location":"concepts/System-Design/performance-optimization/#storage-optimization","title":"\ud83d\udcbe Storage Optimization","text":""},{"location":"concepts/System-Design/performance-optimization/#data-compression-and-encoding","title":"Data Compression and Encoding","text":"<pre><code>-- Choose optimal compression based on data type\nCREATE TABLE playback_events (\n  -- Use ZSTD for good compression ratio and speed\n  event_data STRING ENCODE ZSTD,\n\n  -- Use DELTA encoding for timestamps\n  event_ts TIMESTAMP ENCODE DELTA,\n\n  -- Use dictionary encoding for low-cardinality columns\n  event_type STRING ENCODE DICTIONARY,\n\n  -- Use run-length encoding for sorted data\n  region STRING ENCODE RUNLENGTH\n)\nPARTITIONED BY (dt DATE);\n</code></pre>"},{"location":"concepts/System-Design/performance-optimization/#file-size-optimization","title":"File Size Optimization","text":"<ul> <li>Target File Size: 128-512 MB per file</li> <li>Compaction Jobs: Merge small files during off-peak hours</li> <li>Z-Ordering: Co-locate related data for efficient queries</li> </ul>"},{"location":"concepts/System-Design/performance-optimization/#performance-monitoring","title":"\ud83d\udd0d Performance Monitoring","text":""},{"location":"concepts/System-Design/performance-optimization/#key-metrics-to-track","title":"Key Metrics to Track","text":"<ul> <li>Latency Percentiles: P50, P95, P99 response times</li> <li>Throughput: Requests per second, data processed per minute</li> <li>Error Rates: 4xx, 5xx error percentages</li> <li>Resource Utilization: CPU, memory, disk, network</li> </ul>"},{"location":"concepts/System-Design/performance-optimization/#real-time-monitoring-dashboard","title":"Real-Time Monitoring Dashboard","text":"<pre><code>-- Real-time performance metrics query\nSELECT\n  service_name,\n  endpoint,\n  COUNT(*) as request_count,\n  AVG(response_time_ms) as avg_response_time,\n  PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY response_time_ms) as p95_response_time,\n  COUNT(CASE WHEN status_code &gt;= 500 THEN 1 END) as error_count\nFROM api_request_logs\nWHERE timestamp &gt;= NOW() - INTERVAL '5 minutes'\nGROUP BY service_name, endpoint\nORDER BY p95_response_time DESC;\n</code></pre>"},{"location":"concepts/System-Design/performance-optimization/#content-delivery-optimization","title":"\ud83c\udf10 Content Delivery Optimization","text":""},{"location":"concepts/System-Design/performance-optimization/#cdn-configuration","title":"CDN Configuration","text":"<ul> <li>Edge Caching: Cache content at edge locations</li> <li>Cache Invalidation: Smart invalidation strategies</li> <li>Dynamic Content: Real-time content personalization</li> </ul>"},{"location":"concepts/System-Design/performance-optimization/#streaming-quality-optimization","title":"Streaming Quality Optimization","text":"<pre><code>-- Adaptive bitrate selection based on network conditions\nCREATE TABLE streaming_quality_rules (\n  network_speed_mbps INT,\n  recommended_bitrate_kbps INT,\n  cdn_endpoint STRING,\n  region STRING\n);\n\n-- Real-time quality adjustments\nSELECT\n  session_id,\n  CASE\n    WHEN network_speed &lt; 5 THEN 480\n    WHEN network_speed &lt; 10 THEN 720\n    WHEN network_speed &lt; 25 THEN 1080\n    ELSE 2160\n  END as optimal_bitrate\nFROM streaming_sessions\nWHERE status = 'active';\n</code></pre>"},{"location":"concepts/System-Design/performance-optimization/#capacity-planning","title":"\ud83d\udcc8 Capacity Planning","text":""},{"location":"concepts/System-Design/performance-optimization/#resource-forecasting","title":"Resource Forecasting","text":"<ul> <li>Historical Analysis: Analyze past 12-24 months of data</li> <li>Growth Projections: Account for user growth and content expansion</li> <li>Seasonal Patterns: Plan for holiday peaks and content releases</li> </ul>"},{"location":"concepts/System-Design/performance-optimization/#stress-testing","title":"Stress Testing","text":"<ul> <li>Load Testing: Simulate peak traffic scenarios</li> <li>Chaos Engineering: Inject failures to test resilience</li> <li>Performance Benchmarks: Regular performance regression testing</li> </ul>"},{"location":"concepts/System-Design/performance-optimization/#optimization-best-practices","title":"\ud83d\udee0\ufe0f Optimization Best Practices","text":""},{"location":"concepts/System-Design/performance-optimization/#query-optimization-checklist","title":"Query Optimization Checklist","text":"<ul> <li>[ ] Use EXPLAIN to analyze query plans</li> <li>[ ] Ensure proper indexing on WHERE and JOIN columns</li> <li>[ ] Avoid SELECT * in production queries</li> <li>[ ] Use appropriate data types for columns</li> <li>[ ] Consider query result caching for expensive operations</li> </ul>"},{"location":"concepts/System-Design/performance-optimization/#system-design-principles","title":"System Design Principles","text":"<ul> <li>Stateless Services: Easier to scale horizontally</li> <li>Asynchronous Processing: Use queues for non-critical operations</li> <li>Circuit Breakers: Fail fast and recover gracefully</li> <li>Graceful Degradation: Maintain core functionality during failures</li> </ul>"},{"location":"concepts/System-Design/performance-optimization/#key-takeaways","title":"\ud83d\udcda Key Takeaways","text":"<ul> <li>Measure Everything: Comprehensive monitoring is crucial for optimization</li> <li>Optimize for 95th Percentile: Focus on typical user experience, not just averages</li> <li>Automate Scaling: Manual scaling doesn't work at large scale</li> <li>Cache Strategically: Different data types need different caching strategies</li> <li>Test at Scale: Always test optimizations under realistic load</li> </ul> <p>\ud83d\udd17 Cross-References::</p> <ul> <li>System Architecture - Overall system design patterns</li> <li>Data Modeling Patterns - Schema design for performance</li> <li>SQL Optimization - Query-level optimizations</li> </ul>"},{"location":"concepts/System-Design/system-architecture/","title":"System Architecture","text":""},{"location":"concepts/System-Design/system-architecture/#overview","title":"Overview","text":"<p>Netflix's system architecture handles massive scale with billions of events daily, supporting real-time recommendations, global content delivery, and analytics.</p>"},{"location":"concepts/System-Design/system-architecture/#core-architecture-components","title":"Core Architecture Components","text":""},{"location":"concepts/System-Design/system-architecture/#1-data-ingestion-layer","title":"1. Data Ingestion Layer","text":"<pre><code>-- Kafka topic structure for different event types\nCREATE TABLE event_topics (\n  topic_name STRING,\n  event_type STRING,                -- 'playback', 'recommendation', 'billing'\n  partition_key STRING,             -- 'user_id', 'session_id', 'title_id'\n  retention_hours INT,\n  replication_factor INT\n);\n\n-- Raw event storage in data lake\nCREATE TABLE raw_events (\n  event_id STRING,\n  event_type STRING,\n  event_data JSON,\n  ingestion_ts TIMESTAMP,\n  source STRING,                    -- 'client', 'server', 'partner'\n  partition_date DATE\n) PARTITIONED BY (partition_date);\n</code></pre>"},{"location":"concepts/System-Design/system-architecture/#2-stream-processing-architecture","title":"2. Stream Processing Architecture","text":"<pre><code>-- Real-time stream processing jobs\nCREATE TABLE stream_jobs (\n  job_id STRING PRIMARY KEY,\n  job_name STRING,\n  source_topic STRING,\n  sink_table STRING,\n  processing_logic STRING,         -- SQL or code reference\n  window_type STRING,              -- tumbling, sliding, session\n  window_size_seconds INT,\n  watermark_delay_seconds INT\n);\n\n-- State stores for streaming analytics\nCREATE TABLE stream_state (\n  state_key STRING,\n  state_value JSON,\n  last_updated TIMESTAMP,\n  ttl_seconds INT,\n  PRIMARY KEY (state_key)\n);\n</code></pre>"},{"location":"concepts/System-Design/system-architecture/#3-lambda-architecture-pattern","title":"3. Lambda Architecture Pattern","text":"<pre><code>-- Speed layer (real-time)\nCREATE TABLE realtime_metrics (\n  metric_name STRING,\n  metric_value DOUBLE,\n  window_start TIMESTAMP,\n  window_end TIMESTAMP,\n  computed_at TIMESTAMP,\n  PRIMARY KEY (metric_name, window_start)\n);\n\n-- Batch layer (historical accuracy)\nCREATE TABLE batch_metrics (\n  metric_name STRING,\n  date_partition DATE,\n  metric_value DOUBLE,\n  computed_at TIMESTAMP,\n  PRIMARY KEY (metric_name, date_partition)\n);\n\n-- Serving layer (merged view)\nCREATE VIEW unified_metrics AS\nSELECT\n  m.metric_name,\n  m.window_start,\n  m.metric_value,\n  'realtime' AS source\nFROM realtime_metrics m\nWHERE m.window_end &gt; CURRENT_TIMESTAMP - INTERVAL '1' HOUR\n\nUNION ALL\n\nSELECT\n  m.metric_name,\n  m.date_partition AS window_start,\n  m.metric_value,\n  'batch' AS source\nFROM batch_metrics m\nWHERE m.date_partition &lt; CURRENT_DATE - INTERVAL '1' DAY;\n</code></pre>"},{"location":"concepts/System-Design/system-architecture/#scalability-patterns","title":"Scalability Patterns","text":""},{"location":"concepts/System-Design/system-architecture/#1-database-sharding-strategy","title":"1. Database Sharding Strategy","text":"<pre><code>-- User data sharding by consistent hashing\nCREATE TABLE user_shard_map (\n  user_id BIGINT,\n  shard_id INT,\n  shard_host STRING,\n  created_at TIMESTAMP,\n  PRIMARY KEY (user_id)\n);\n\n-- Content metadata sharding\nCREATE TABLE content_shard_map (\n  title_id BIGINT,\n  shard_id INT,\n  shard_host STRING,\n  PRIMARY KEY (title_id)\n);\n\n-- Cross-shard queries using scatter-gather\nCREATE TABLE distributed_query_cache (\n  query_hash STRING,\n  results JSON,\n  completed_shards INT,\n  total_shards INT,\n  created_at TIMESTAMP,\n  PRIMARY KEY (query_hash)\n);\n</code></pre>"},{"location":"concepts/System-Design/system-architecture/#2-caching-layer-architecture","title":"2. Caching Layer Architecture","text":"<pre><code>-- Multi-level caching strategy\nCREATE TABLE cache_layers (\n  layer_name STRING PRIMARY KEY,\n  cache_type STRING,               -- 'local', 'distributed', 'edge'\n  ttl_seconds INT,\n  max_size_mb INT,\n  hit_rate_target DOUBLE\n);\n\n-- Cache invalidation events\nCREATE TABLE cache_invalidation_events (\n  event_id STRING PRIMARY KEY,\n  cache_key STRING,\n  invalidation_reason STRING,\n  triggered_at TIMESTAMP,\n  processed_at TIMESTAMP\n);\n</code></pre>"},{"location":"concepts/System-Design/system-architecture/#content-delivery-network-cdn-integration","title":"Content Delivery Network (CDN) Integration","text":"<pre><code>-- CDN log analysis for performance monitoring\nCREATE TABLE cdn_logs (\n  request_id STRING,\n  user_id BIGINT,\n  title_id BIGINT,\n  cdn_edge_location STRING,\n  response_time_ms INT,\n  bytes_transferred BIGINT,\n  http_status INT,\n  request_ts TIMESTAMP,\n  user_country STRING\n) PARTITIONED BY (dt DATE);\n\n-- Adaptive bitrate decisioning\nCREATE TABLE playback_quality_decisions (\n  session_id STRING,\n  title_id BIGINT,\n  user_id BIGINT,\n  device_type STRING,\n  connection_speed_mbps DOUBLE,\n  selected_bitrate_kbps INT,\n  buffer_health_seconds INT,\n  decision_ts TIMESTAMP\n);\n</code></pre>"},{"location":"concepts/System-Design/system-architecture/#microservices-architecture","title":"Microservices Architecture","text":"<pre><code>-- Service registry for microservices discovery\nCREATE TABLE service_registry (\n  service_name STRING,\n  service_instance STRING,\n  host STRING,\n  port INT,\n  health_status STRING,\n  last_heartbeat TIMESTAMP,\n  PRIMARY KEY (service_name, service_instance)\n);\n\n-- Distributed tracing for request correlation\nCREATE TABLE trace_events (\n  trace_id STRING,\n  span_id STRING,\n  parent_span_id STRING,\n  service_name STRING,\n  operation_name STRING,\n  start_time TIMESTAMP,\n  duration_ms INT,\n  tags JSON\n);\n</code></pre>"},{"location":"concepts/System-Design/system-architecture/#global-data-replication","title":"Global Data Replication","text":"<pre><code>-- Cross-region replication status\nCREATE TABLE replication_status (\n  table_name STRING,\n  source_region STRING,\n  target_region STRING,\n  last_replication_ts TIMESTAMP,\n  replication_lag_seconds INT,\n  status STRING,\n  PRIMARY KEY (table_name, source_region, target_region)\n);\n\n-- Regional data sovereignty compliance\nCREATE TABLE data_residency_rules (\n  data_type STRING,\n  allowed_regions ARRAY&lt;STRING&gt;,\n  retention_days INT,\n  encryption_required BOOLEAN,\n  PRIMARY KEY (data_type)\n);\n</code></pre>"},{"location":"concepts/System-Design/system-architecture/#design-principles","title":"Design Principles","text":""},{"location":"concepts/System-Design/system-architecture/#1-failure-handling","title":"1. Failure Handling","text":"<ul> <li>Circuit Breakers: Automatic failure detection and recovery</li> <li>Graceful Degradation: Core features remain available during outages</li> <li>Idempotent Operations: Safe retry of failed operations</li> </ul>"},{"location":"concepts/System-Design/system-architecture/#2-observability","title":"2. Observability","text":"<ul> <li>Metrics Collection: Comprehensive monitoring of all systems</li> <li>Distributed Tracing: End-to-end request visibility</li> <li>Log Aggregation: Centralized logging for debugging</li> </ul>"},{"location":"concepts/System-Design/system-architecture/#3-security-architecture","title":"3. Security Architecture","text":"<ul> <li>Zero Trust: Authentication for all service-to-service communication</li> <li>Data Encryption: At rest and in transit</li> <li>Access Control: Fine-grained permissions and audit trails</li> </ul>"},{"location":"concepts/System-Design/system-architecture/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Hot Data Path: Optimize for 99th percentile performance</li> <li>Cold Data Path: Cost-effective storage for historical data</li> <li>Data Locality: Process data close to where it's generated</li> <li>Auto-scaling: Automatic resource adjustment based on load</li> </ul>"},{"location":"concepts/System-Design/system-architecture/#follow-up-questions","title":"Follow-up Questions","text":"<ul> <li>How would you design for 99.99% uptime across global regions?</li> <li>How would you handle data consistency in a multi-region architecture?</li> <li>How would you implement canary deployments for microservices?</li> <li>How would you design the system to handle 10x traffic growth?</li> </ul>"},{"location":"interviews/","title":"Interview Problems Collection","text":"<p>This directory contains a curated collection of company-specific interview problems and solutions, organized by company.</p>"},{"location":"interviews/#anthropic","title":"Anthropic","text":"<ul> <li>Stack Trace Reconstruction</li> <li>Sorting &amp; Data Mutation</li> <li>Web Crawler</li> <li>Multi-Threaded Chat System</li> <li>File System</li> <li>Banking System</li> <li>CRUD Systems</li> <li>Ethics AI</li> <li>Web Crawler Challenge</li> <li>Banking System Design Challenge</li> <li>File System Design Challenge</li> <li>Employee Management Challenge</li> <li>Algorithmic Implementation Challenge</li> <li>AI System Design Challenge</li> </ul>"},{"location":"interviews/#netflix","title":"Netflix","text":"<ul> <li>SQL Problems</li> <li>Data Modeling</li> <li>Top 10 Feature Case Study</li> <li>Personalization Engine</li> </ul>"},{"location":"interviews/#nextdoor","title":"Nextdoor","text":"<ul> <li>Comment Threading</li> </ul>"},{"location":"interviews/#okareo","title":"Okareo","text":"<ul> <li>Dice Consistency Check</li> </ul>"},{"location":"interviews/Anthropic/","title":"Anthropic Interview Questions","text":""},{"location":"interviews/Anthropic/#from-anthropic-chatgptmd","title":"From anthropic-chatgpt.md","text":"<p>This file contains a comprehensive interview prep sheet with 8 coding and design problems covering system design, algorithms, and AI ethics.</p> <ul> <li>Stack Trace Reconstruction: Reconstruct valid stack traces from function call lists, handling mismatches and malformed sequences.</li> <li>Sorting &amp; Data Mutation Challenges: Implement sorting algorithms with data mutations, focusing on performance, stability, and edge cases.</li> <li>Web Crawler (Concurrency &amp; Async Evolution): Build a web crawler evolving from synchronous to asynchronous with concurrency, including rate limiting and fault tolerance.</li> <li>GPT / Claude Multi-Threaded Chat System: Design a system for handling multiple user questions in a single chat thread, managing state and concurrency.</li> <li>File System / In-Memory File Store: Design an in-memory data store with CRUD operations, backup/restore, and querying capabilities.</li> <li>Multi-Tiered Banking System: Build a banking system with transaction processing, analytics, scheduled transactions, and account merging.</li> <li>In-Memory CRUD / OOP Systems: Implement in-memory CRUD for applications like employee timesheets, handling data validation and constraints.</li> <li>Open-Ended Ethics AI Emotion: Discuss ethical implications if AI develops emotions, exploring anthropomorphism and societal impacts.</li> </ul>"},{"location":"interviews/Anthropic/#from-anthropic-claudemd","title":"From anthropic-claude.md","text":"<p>This file provides consolidated challenges focusing on system design and algorithmic implementations.</p> <ul> <li>Web Crawler Challenge: Design synchronous and asynchronous web crawlers with concurrency and performance considerations.</li> <li>Banking System Design Challenge: Multi-tiered banking app with account management, analytics, scheduled transactions, and account merging.</li> <li>File System Design Challenge: In-memory file system with CRUD, filtering, backup/restore, and potential distributed aspects.</li> <li>Employee Management Challenge: In-memory database for factory worker timesheet management with clock-in/out tracking.</li> <li>Algorithmic Implementation Challenge: Implement stack trace generation, sorting algorithms, or data mutations with optimizations.</li> <li> <p>AI System Design Challenge: Architecture for AI chat service handling multiple questions in a thread, including emotional aspects.</p> </li> <li> <p>anthropic-chatgpt.md</p> </li> <li>anthropic-claude.md</li> </ul>"},{"location":"interviews/Anthropic/anthropic-chatgpt/","title":"\ud83e\udde0 Anthropic Interview Prep Sheet","text":""},{"location":"interviews/Anthropic/anthropic-chatgpt/#stack-trace-reconstruction","title":"Stack Trace Reconstruction","text":"<p>Prompt: Given a list of function calls (enter/exit), reconstruct a valid stack trace.</p> <p>Practice::</p> <ul> <li>Stack data structure (push/pop logic)</li> <li>Matching calls with exit order</li> <li>Error handling (mismatched calls, early returns)</li> </ul> <p>Review: Recursive traces, call stack mechanics, handling malformed sequences</p>"},{"location":"interviews/Anthropic/anthropic-chatgpt/#sorting--data-mutation-challenges","title":"Sorting &amp; Data Mutation Challenges","text":"<p>Prompt: Implement sorting and data mutation logic across multiple steps. Handle edge cases, stability, and performance tradeoffs.</p> <p>Practice::</p> <ul> <li>Sorting algorithms (quicksort, mergesort, etc.)</li> <li>Chained mutations (transform \u2192 filter \u2192 sort)</li> <li>Time/space complexity reasoning</li> </ul> <p>Review: In-place vs out-of-place, Python\u2019s <code>sorted()</code>, stability in sorting</p>"},{"location":"interviews/Anthropic/anthropic-chatgpt/#web-crawler-concurrency--async-evolution","title":"Web Crawler (Concurrency &amp; Async Evolution)","text":"<p>Prompt: Build a web crawler. Start with synchronous crawling, evolve to asynchronous with concurrency.</p> <p>Practice::</p> <ul> <li>Sync: <code>requests</code> + <code>BeautifulSoup</code></li> <li>Async: <code>aiohttp</code> + <code>asyncio</code> + rate limiting</li> <li>Queueing, deduplication, fault tolerance</li> </ul> <p>Review: Async patterns in Python, producer-consumer, BFS/DFS crawling</p>"},{"location":"interviews/Anthropic/anthropic-chatgpt/#gpt--claude-multi-threaded-chat-system","title":"GPT / Claude Multi-Threaded Chat System","text":"<p>Prompt: Design a system to handle multiple user questions in a single GPT/Claude chat thread.</p> <p>Practice::</p> <ul> <li>Thread/session state management</li> <li>Concurrent request handling</li> <li>Context switching and isolation of responses</li> </ul> <p>Review: Message queues, chat memory, stateless vs stateful architecture</p>"},{"location":"interviews/Anthropic/anthropic-chatgpt/#file-system--in-memory-file-store","title":"File System / In-Memory File Store","text":"<p>Prompt: Design a file system or in-memory data store with operations like set, get, filter, backup, and restore.</p> <p>Practice::</p> <ul> <li>In-memory structure (dict-based)</li> <li>Backup/versioning logic</li> <li>Filtering and query capabilities</li> </ul> <p>Review: Serialization, snapshotting, CLI-command parsing</p>"},{"location":"interviews/Anthropic/anthropic-chatgpt/#multi-tiered-banking-system","title":"Multi-Tiered Banking System","text":"<p>Prompt: Design a banking system that evolves in stages:</p> <ul> <li>Basic transaction recording (deposits, transfers)</li> <li>Top-K analytics</li> <li>Scheduled/cancelable transactions</li> <li>Account merging while preserving history</li> </ul> <p>Practice::</p> <ul> <li>Ledger and transaction classes</li> <li>Priority queues for ranking</li> <li>Scheduling logic (time-based simulation)</li> <li>Performance/memory budget constraints</li> </ul> <p>Review: OOP principles, transaction integrity, scheduled tasks</p>"},{"location":"interviews/Anthropic/anthropic-chatgpt/#in-memory-crud--oop-systems","title":"In-Memory CRUD / OOP Systems","text":"<p>Prompt:</p> <p>Implement a small in-memory CRUD application:</p> <ul> <li>Employee timesheets or academic management</li> <li>Track time-in/out or grades</li> <li>Support unusual/complex operations</li> </ul> <p>Practice::</p> <ul> <li>OOP class models and relationships</li> <li>Data validation and constraints</li> <li>Handling corrupted or poorly-structured data</li> </ul> <p>Review: Data modeling, edge case design, object state transitions</p>"},{"location":"interviews/Anthropic/anthropic-chatgpt/#open-ended-ethics-ai-emotion","title":"Open-Ended Ethics AI Emotion","text":"<p>Prompt: What would you do if AI started to feel sad?</p> <p>Practice::</p> <ul> <li>Reason through emotional simulation</li> <li>Address anthropomorphism</li> <li>Explore ethical and societal implications</li> </ul> <p>Review: AI ethics, cognitive modeling, affective computing</p>"},{"location":"interviews/Anthropic/anthropic-chatgpt/#prep-tips","title":"\u2705 Prep Tips","text":"<ul> <li>Use the structure: problem \u2192 approach \u2192 tradeoffs \u2192 constraints:</li> <li>Practice verbal walkthroughs for design</li> <li>Clarify assumptions early</li> <li>Prepare for multi-stage iterations, especially on banking and file systems</li> <li>Stay grounded in real-world constraints (scale, latency, memory)</li> </ul>"},{"location":"interviews/Anthropic/anthropic-claude/","title":"Consolidated Anthropic Interview Questions","text":""},{"location":"interviews/Anthropic/anthropic-claude/#web-crawler-challenge","title":"Web Crawler Challenge","text":"<p>Design and implement a web crawler with both synchronous and asynchronous versions, focusing on concurrency handling, performance trade-offs, and Python implementation.</p>"},{"location":"interviews/Anthropic/anthropic-claude/#banking-system-design-challenge","title":"Banking System Design Challenge","text":"<p>Design a multi-tiered banking application with:</p> <ul> <li>Core tier: Account management and transaction processing (deposits/transfers)</li> <li>Analytics tier: Data metrics and ranking (e.g., top K accounts by transaction volume)</li> <li>Advanced tier: Scheduled transactions with cancellation functionality</li> <li>Extension tier: Account merging while preserving history</li> </ul>"},{"location":"interviews/Anthropic/anthropic-claude/#file-system-design-challenge","title":"File System Design Challenge","text":"<p>Implement an in-memory file/storage system with CRUD operations, filtering capabilities, and backup/restore functionality. Consider distributed aspects if time permits.</p>"},{"location":"interviews/Anthropic/anthropic-claude/#employee-management-challenge","title":"Employee Management Challenge","text":"<p>Design and implement an in-memory database for factory worker timesheet management, including clock-in/clock-out tracking and reporting.</p>"},{"location":"interviews/Anthropic/anthropic-claude/#algorithmic-implementation-challenge","title":"Algorithmic Implementation Challenge","text":"<p>Implement one of:</p> <ul> <li>A stack trace generator from function call lists</li> <li>A specified sorting algorithm with optimization considerations</li> <li>Data mutation operations with specific constraints</li> </ul>"},{"location":"interviews/Anthropic/anthropic-claude/#ai-system-design-challenge","title":"AI System Design Challenge","text":"<p>Design an AI chat service architecture that can handle multiple questions in a single thread. Extension question: How would you address emotional aspects of an AI system?</p>"},{"location":"interviews/netflix/","title":"Netflix Interview Preparation","text":"<p>Welcome to the reorganized Netflix interview preparation guide. This repository contains comprehensive content covering SQL problems, data modeling concepts, and advanced technical topics commonly asked in Netflix interviews.</p>"},{"location":"interviews/netflix/#directory-structure","title":"Directory Structure","text":""},{"location":"interviews/netflix/#sql-problems","title":"\ud83d\udcca sql-problems","text":"<p>SQL-focused interview problems organized by complexity and topic.</p> <ul> <li>01-basic-analytics - Fundamental SQL analytics problems</li> <li>02-ranking-window-functions - Advanced ranking and window function problems</li> <li>Advanced Patterns - Complex SQL patterns and recursive queries</li> <li>Optimization Challenges - Query optimization and performance tuning</li> </ul>"},{"location":"interviews/netflix/#data-modeling","title":"\ud83c\udfd7\ufe0f data-modeling","text":"<p>Data architecture and modeling concepts for large-scale systems.</p> <ul> <li>01-core-entities - Fundamental data entities and basic modeling</li> <li>02-relationships-patterns - Complex relationships and design patterns</li> <li>03-event-streaming - Event-driven architecture and streaming data</li> <li>04-scalability-patterns - Distributed systems and scalability patterns</li> </ul>"},{"location":"interviews/netflix/#advanced-concepts","title":"\ud83d\ude80 advanced-concepts","text":"<p>Advanced technical concepts and system design patterns.</p> <ul> <li>System Architecture - Microservices and distributed system design</li> <li>Performance Optimization - Caching, load balancing, and performance</li> <li>Experimentation - A/B testing and experimentation platforms</li> <li>Case Studies - Real-world architectural case studies</li> </ul>"},{"location":"interviews/netflix/#content-migration-status","title":"Content Migration Status","text":"<ul> <li>[x] Directory structure created</li> <li>[x] Index files created for all subdirectories</li> <li>[x] Structure simplification completed</li> <li>[x] Nested READMEs removed and navigation consolidated</li> <li>[x] Single-file directories converted to simplified naming</li> <li>[x] Content migration completed - using original files as authoritative source</li> <li>[x] Data modeling section - consolidated from original/netflix.md</li> <li>[x] SQL problems section - aligned with original/sql.md</li> <li>[x] Advanced concepts section - updated with original/case-study.md</li> <li>[x] Cross-references and navigation updated throughout</li> </ul>"},{"location":"interviews/netflix/#quick-start","title":"Quick Start","text":"<ol> <li>SQL Focus: Start with basic analytics if you're new to SQL</li> <li>Data Architecture: Explore core entities for data modeling fundamentals</li> <li>System Design: Check system architecture for senior-level topics</li> <li>Real-world Application: Study the Top 10 case study for end-to-end understanding</li> </ol>"},{"location":"interviews/netflix/#key-content-areas","title":"Key Content Areas","text":""},{"location":"interviews/netflix/#sql-problems_1","title":"\ud83d\udcca SQL Problems","text":"<ul> <li>Basic Analytics - DAU, retention, most-watched content</li> <li>Ranking &amp; Window Functions - Top-N queries, consecutive days, power users</li> <li>Advanced Patterns - Complex CTEs and recursive queries</li> <li>Optimization Challenges - Query performance and indexing</li> </ul>"},{"location":"interviews/netflix/#data-modeling_1","title":"\ud83c\udfd7\ufe0f Data Modeling","text":"<ul> <li>Core Entities - Content catalog, accounts/profiles, subscriptions</li> <li>Relationships &amp; Patterns - User history, recommendations, many-to-many relationships</li> <li>Event Streaming - Real-time events, engagement metrics, A/B testing</li> <li>Scalability Patterns - Partitioning, sharding, event streaming at scale</li> </ul>"},{"location":"interviews/netflix/#unified-data-modeling-framework","title":"\ud83c\udfd7\ufe0f Unified Data Modeling Framework","text":"<p>For comprehensive data modeling concepts beyond Netflix-specific implementations:</p> <ul> <li>Complete Data Modeling Guide - Unified framework with streaming platform patterns</li> <li>Core Entities - Universal content, user, and billing models</li> <li>Advanced Patterns - Relationship modeling and behavioral analytics</li> <li>Event Streaming - Real-time data patterns and experimentation</li> <li>Scalability - Distributed systems and performance</li> </ul>"},{"location":"interviews/netflix/#advanced-concepts_1","title":"\ud83d\ude80 Advanced Concepts","text":"<ul> <li>System Architecture - Microservices and distributed systems</li> <li>Performance Optimization - Caching, load balancing, optimization techniques</li> <li>Experimentation - A/B testing and experimentation platforms</li> <li>Case Studies - Real-world architectural challenges and solutions</li> </ul>"},{"location":"interviews/netflix/#cross-references","title":"Cross-References","text":""},{"location":"interviews/netflix/#from-sql-to-data-modeling","title":"From SQL to Data Modeling","text":"<ul> <li>Top 3 Shows per Region \u2192 Content Catalog Modeling</li> <li>Recommendation Acceptance \u2192 Recommendations Storage</li> </ul>"},{"location":"interviews/netflix/#from-data-modeling-to-advanced-concepts","title":"From Data Modeling to Advanced Concepts","text":"<ul> <li>Unified Data Modeling \u2192 Complete data modeling framework</li> <li> <p>Event Streaming \u2192 Top 10 Feature Implementation</p> </li> <li> <p>User Viewing History \u2192 Personalization Engine</p> </li> </ul>"},{"location":"interviews/netflix/#from-general-concepts-to-netflix-problems","title":"From General Concepts to Netflix Problems","text":"<ul> <li>SQL Learning Hub \u2192 SQL fundamentals and Netflix examples</li> <li>Aggregate Functions \u2192 Daily Active Users</li> <li>Window Functions \u2192 Ranking Problems</li> </ul>"},{"location":"interviews/netflix/#end-to-end-case-studies","title":"End-to-End Case Studies","text":"<ul> <li> <p>Top 10 Feature - Complete data pipeline from ingestion to serving</p> </li> <li> <p>Personalization Engine - ML-driven recommendation systems</p> </li> </ul>"},{"location":"interviews/netflix/#resources","title":"Resources","text":"<ul> <li>Data Modeling Concepts - Comprehensive data modeling and architecture source</li> <li>SQL Interview Problems - SQL interview questions and solutions source</li> <li>System Design Case Studies - \"Top 10\" feature comprehensive implementation source</li> </ul>"},{"location":"interviews/netflix/#interview-preparation-tips","title":"Interview Preparation Tips","text":"<ol> <li>Start with SQL: Master the basic analytics and window functions</li> <li>Learn Data Modeling: Understand the core entities and relationships</li> <li>Study Real Cases: Dive deep into the Top 10 case study for end-to-end understanding</li> <li>Practice Scale: Focus on scalability patterns and performance optimization</li> </ol>"},{"location":"interviews/nextdoor/","title":"Nextdoor Interview Questions","text":"<ul> <li>comments.py</li> </ul>"},{"location":"interviews/okareo/","title":"Okareo Interview - Dice Consistency Check","text":"<p>A six-sided dice is thrown repeatedly with results recorded as int[]. Each time we can only see three sides - top, front, right (bottom, left, back are hidden).</p> <p>The visible sides are recorded in the order top, front, right. So for example [1,2,3] is top = 1, front = 2, and right = 3. Each side could be a number 1 to 50.</p> <p>Multiple throws are recorded as [ [1,2,3], [1,3,6], [1,25,45]...]</p> <p>We need to check that these historical throws data is consistent with respect to a single six-sided dice.</p> <p>Write a method consistent(throws list[list[int]]) \u2192 int. The method returns 1 based index of the first entry in throws history that is inconsistent with respect to previous ones. If all the historical data is consistent the method should return 0.</p>"},{"location":"interviews/okareo/#solution","title":"Solution","text":"<p>See dice.py for the implementation and test cases.</p>"}]}